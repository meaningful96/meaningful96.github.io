---
title: "[Deep Learning]Artificial Neural Network(인공 신경망)"

categories: 
  - DeepLearning
tags:
  - [DL, NLP]

toc: true
toc_sticky: true

date: 2023-08-11
last_modified_at: 2023-07-11
---

# Aritifical Neural Network(인공 신경망, ANN)
## 1. Neural Network
사람의 뇌는 뉴런(신경세포)로 이루어져있다. 뉴런들은 신경전달물질들을 만들어 자극을 전달한다. 여러 뉴런간의 상호작용으로 신호가 전달되어 인간은 그 자극에 대해 반응을 하게된다. 이러한 뉴런을 모델링해 만들 개념이다. 
인공 신경망은 <span style="color:gold">**노드(Node)**</span>와 <span style="color:gold">**엣지(Edge)**</span>로 이루어진다. **노드**는 하나의 뉴런을 말하며, **엣지**는 뉴런간의 연결을 말하며 이 정도를 **가중치(weight)**라고 한다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/1e4edab8-bb19-4af0-b62b-0f25297d3b31">
</p>

Bias는 일종의 기준선을 주어 민감도를 조정한다. ANN의 목적은 <u><b>주어진 입력</b>에 대해 <b>원하는 출력</b>이 나오도록 weight와 bias를 알아내는 것</u>이다. 이를 모델이 학습한다고 한다.

1. Weight를 Signal에 곱한다.
2. Bias(민감도)를 더한다.
3. Activation function을 먹인다.

## 2. Activation Funciton(활성 함수)
입력 신호의 총합을 출력 신호로 변환하는 함수이다. 입력 신호들의 Weighted Sum을 활성함수에 넣어주어 출력된 신호가 다음 층의 신경층의 입력이 된다. Activation function은 **비선형 함수(Nonlinear function)**여야 한다. 신경망에서 선형 함수를 
이용하면 신경망의 층을 깊게하는 의미가 없이진다. 다음의 예로 확인할 수 있다.

```python
Activation_function = (3x + 2) 
Input = a

Output1 = 3a + 2
Output2 = 3(3a + 2) 2 # The seconde layer's output is still linear
```

인공 신경망의 장점은 선형 함수의 경우 Binary하게 classification하는 반면(2차원에서), 다중 분류를 할 수 있게 만들어주며 그 역할을 하는 것이 활성화 함수이다.(가장 쉬운 예는 XOR classification) 따라서, 활성화 함수를 거친 출력 값은 선형이면 안된다.
또한, 선형함수가 출력으로 나올 경우, Hidden layer가 없어도 표현이 가능하기 때문에 그 의미가 퇴색된다.

### 1) Sigmoid(시그모이드) function

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/5edfb23e-be9a-4640-ab4f-78545b50da67">
</p>

<p align="center">
<img width="800" alt="1" src="">
</p>

<br/>
<br/>

# Example with Pytorch
