---
title: "[Deep Learning]Artificial Neural Network(인공 신경망)"

categories: 
  - DeepLearning
tags:
  - [DL, NLP]

toc: true
toc_sticky: true

date: 2023-08-11
last_modified_at: 2023-07-11
---

# Aritifical Neural Network(인공 신경망, ANN)
## 1. Neural Network
사람의 뇌는 뉴런(신경세포)로 이루어져있다. 뉴런들은 신경전달물질들을 만들어 자극을 전달한다. 여러 뉴런간의 상호작용으로 신호가 전달되어 인간은 그 자극에 대해 반응을 하게된다. 이러한 뉴런을 모델링해 만들 개념이다. 
인공 신경망은 <span style="color:gold">**노드(Node)**</span>와 <span style="color:gold">**엣지(Edge)**</span>로 이루어진다. **노드**는 하나의 뉴런을 말하며, **엣지**는 뉴런간의 연결을 말하며 이 정도를 **가중치(weight)**라고 한다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/1e4edab8-bb19-4af0-b62b-0f25297d3b31">
</p>

Bias는 일종의 기준선을 주어 민감도를 조정한다. ANN의 목적은 <u><b>주어진 입력</b>에 대해 <b>원하는 출력</b>이 나오도록 weight와 bias를 알아내는 것</u>이다. 이를 모델이 학습한다고 한다.

1. Weight를 Signal에 곱한다.
2. Bias(민감도)를 더한다.
3. Activation function을 먹인다.

## 2. Activation Funciton(활성 함수)
입력 신호의 총합을 출력 신호로 변환하는 함수이다. 입력 신호들의 Weighted Sum을 활성함수에 넣어주어 출력된 신호가 다음 층의 신경층의 입력이 된다. Activation function은 **비선형 함수(Nonlinear function)**여야 한다. 신경망에서 선형 함수를 
이용하면 신경망의 층을 깊게하는 의미가 없이진다. 다음의 예로 확인할 수 있다.

```python
Activation_function = (3x + 2) 
Input = a

Output1 = 3a + 2
Output2 = 3(3a + 2) 2 # The second layer's output is still linear
```

인공 신경망의 장점은 선형 함수의 경우 Binary하게 classification하는 반면(2차원에서), 다중 분류를 할 수 있게 만들어주며 그 역할을 하는 것이 활성화 함수이다.(가장 쉬운 예는 XOR classification) 따라서, 활성화 함수를 거친 출력 값은 선형이면 안된다.
또한, 선형함수가 출력으로 나올 경우, Hidden layer가 없어도 표현이 가능하기 때문에 그 의미가 퇴색된다.

### 1) Sigmoid(시그모이드) 함수

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/5edfb23e-be9a-4640-ab4f-78545b50da67">
</p>

Sigmoid란 'S'모양을 말한다. 입력을 받아 그 값을 \[0, 1\]범위로 압축하여 출력한다. 물론 마냥 좋은 것만은 아니다. 단점도 역시 존재한다.

- 단점
  1. 기울기 소면(Vanishing Gradient): 미분시 미분값이 0으로 수렴
  2.   2. Sigmoid 함숫값의 중심이 0(Zero-Centered)이 아니다.

<br/>

### 2) tanh 

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/d300c11d-ca26-49ce-94b5-d109dcd4b1f1">
</p>

생긴 것은 Sigmoid와 비슷하지만, 그 출력값은 -1 ~ 1 사이의 값의 범위를 가진다. 사실 tanh함수는 sigmoid를 두배해 -1한 값과 같다.   
<center><span style = "font-size:110%">$$tanh(x) \; = \; 2sigmoid(2x) - 1$$</span></center>

- 단점
  1. 기울기 소면(Vanishing Gradient): 미분시 미분값이 0으로 수렴

<br/>

### 3) ReLU(Rectified Linear Unit) 함수 

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/e1e74965-17f9-436a-a293-c9ec5e7ca378">
</p>

가장 많이 사용하는 함수이다. 입력이 음수이면 출력값이 0으로 되는데, 아주 작은 값으로 출력되게끔 기울기를 아주 작은 값으로 설정한 Leaky ReLU를 일반적으로 더 많이 사용한다. 또한 값이 아무리 커져도 Saturation이 일어나지 않으며 exponetial이 수식에 포함되지 않아, 연산이 더 빠르다.

- 단점
  1. Zero-Centered가 아니기 때문에 지그재그 문제가 생할 수 있다.
 
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/c55433da-a3e6-4630-819d-b224058178d8">
</p>

## 3. Back Propagation(오차 역전파법)

신경망은 순환하지 않는 그래프로 연결된 뉴런의 집합이다. 이는 크게 세 개의 부분으로 나눠진다. **입력층**(Input Layer), **은닉층**(Hidden Layer), **출력층**(Output Layer)이다. 입력층에서 signal을 받으면 기존에 미리 부여된 가중치와 Weighted Sum을 거치고, 활성화 함수를 거쳐 은닉층으로 전달된다. 은닉층의 깊이에 따라 모델의 계산이 복잡해지며, 층이 많아질수록 Deep neural network라고 한다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/e3a3ccb8-a527-4371-91b7-a6a4df74b506">
</p>

오차 역전파법을 이용해 각 계층에 전달된 **오차(Error)**를 바탕으로 가중치를 갱신하는 방법은 Gradient Descent를 오차함수에 적용해 최소화하는 문제와 같다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/6668de3a-5e00-4b5b-967c-4565c8986780">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/ca0b234a-4908-4b13-87b9-c39829ffc1cf">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/6e2eefe2-890e-4f7f-a718-be926d4e6b6b">
</p>



<br/>
<br/>

# Example with Pytorch
# Reference
- [CS7015 Lecture 9]("http://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture9.pdf")      
- Reconstruction of porous media from extremely limited information using conditional generative adversarial networks - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/Commonly-used-activation-functions-a-Sigmoid-b-Tanh-c-ReLU-and-d-LReLU_fig3_335845675 [accessed 13 Aug, 2023]
