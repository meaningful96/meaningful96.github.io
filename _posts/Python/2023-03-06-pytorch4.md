---

title: Pytorch 문법 (4) - Fianl

categories: 
  - py
  
tags:
  - [DL, Pytorch]
  
toc: true
toc_sticky: true

date: 2023-03-06
last_modified_at: 2023-03-06
---

## 1. Autograd(자동미분) 복습
### 1) 정의

- `torch.autograd` 패키지는 Tensor의 모든 연산에 대해 **자동 미분** 제공
- 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻
- **역전파**를 위해 미분값을 자동으로 계산
- `requires_grad` 속성을 `True`로 설정하면, 해당 텐서에서 이루어지는 모든 연산들을 추적하기 시작
- 기록을 추적하는 것을 중단하게 하려면, `.detach()`를 호출하여 연산기록으로부터 분리

```python
import torch
a = torch.rand(3,3)
a = a * 3

print(a)
print(a.requires_grad)
```
```
tensor([[0.7729, 2.8212, 1.8116],
        [1.5490, 1.3230, 0.7801],
        [2.7811, 2.3541, 2.1352]])
        
False
```

--------------------------------------------------------------------------------

### 2) 기울기(Gradient)

```python
x = torch.ones(3,3,requires_grad = True)
print(x)

y = x + 5
print(y)
print(y.requires_grad)

z = y*y
out = z.mean()
print(z)
print(out)
```
```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], requires_grad=True)
-------------------------------------------------  
tensor([[6., 6., 6.],
        [6., 6., 6.],
        [6., 6., 6.]], grad_fn=<AddBackward0>)
True
-------------------------------------------------
        [36., 36., 36.],
        [36., 36., 36.]], grad_fn=<MulBackward0>)
tensor(36., grad_fn=<MeanBackward0>)
-------------------------------------------------
```

계산이 완료된 후, `backward()`를 호출하면 자동으로 역전파 계산이 가능하고, `.grad` 속성에 누적됨

```python
print(out)
out.backward()
```
```
tensor(36., grad_fn=<MeanBackward0>)
```

--------------------------------------------------------------------------------

`grad`: data가 거쳐온 layer에 대한 미분값 저장

```python
print(x)
print(x.grad)

print(y)
print(y.grad)
```
```
## x, x.grad
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], requires_grad=True)
tensor([[1.3333, 1.3333, 1.3333],
        [1.3333, 1.3333, 1.3333],
        [1.3333, 1.3333, 1.3333]])
        
## y, y.grad
tensor([[6., 6., 6.],
        [6., 6., 6.],
        [6., 6., 6.]], grad_fn=<AddBackward0>)
None
```

--------------------------------------------------------------------------------

```python
x = torch.randn(3, requires_grad = True)
y = x*2
while y.data.norm() < 1000:
    y = y*2

print(y)

v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)
y.backward(v)

print(x.grad)
```
```
tensor([-1502.9976,  -581.3363,   240.9863], grad_fn=<MulBackward0>)
tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])
```

--------------------------------------------------------------------------------

- `with torch.no_grad()`를 사용하여 기울기의 업데이트를 하지 않음
- 기록을 추적하는 것을 방지하기 위해 코드 블럭을 `with torch.no_grad()`로 감싸면 기울기 계산은 필요없지만,`requires_grad=True`로 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용
- 모델을 평가할 때는 모델 자체를 업데이트 하면 안되니 기울기 계산하지 않고, 현재 있는 상태에서 평가할 때는 no_grad()를 넣어줘야 함

```python
import torch

print(x.requires_grad)
print((x**2).requires_grad)

with torch.no_grad():
    print((x**2).requires_grad)  ## 여기서는 False나옴  
```
```
True
True
False
```

--------------------------------------------------------------------------------

`detach()`: 내용물(content)은 같지만 `require_grad`가 다른 새로운 Tensor를 가져올 때 사용

```python
print(x.requires_grad)
y = x.detach()
print(y.requires_grad)
print(x.eq(y).all()) ## eq(): equal, x와 y가 같니?
```
```
True
False
tensor(True)
```

### 3) 자동 미분 흐름 예제

