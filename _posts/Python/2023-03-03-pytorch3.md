---

title: Pytorch 문법 (3)

categories: 
  - py
  
tags:
  - [DL, Pytorch]
  
toc: true
toc_sticky: true

date: 2023-03-03
last_modified_at: 2023-03-03
---

## 1. 자동미분(Autograd)

### 1) AUTOGRAD (자동 미분)
- autograd 패키지는 Tensor의 모든 연산에 대해 **자동 미분** 제공
- 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻
- backprop를 위한 미분값을 자동으로 계산

### 2) Tensor
- data: **tensor**형태의 데이터
- grad: **data가 겨쳐온 layer에 대한 미분값 저장**
- grad_fn: 미분값을 계산한 **함수에 대한 정보** 저장 (어떤 함수에 대해서 backprop 했는지)
- `requires_grad` 속성을 `True`로 설정하면, 해당 텐서에서 이루어지는 모든 연산들을 추적하기 시작
- 계산이 완료된 후, `.backward()`를 호출하면 자동으로 `gradient`를 계산할 수 있으며, `.grad` 속성에 누적됨
- 기록을 추적하는 것을 중단하게 하려면, `.detach()`를 호출하여 연산기록으로부터 분리
- 기록을 추적하는 것을 방지하기 위해 코드 블럭을 `with torch.no_grad():`로 감싸면 `gradient`는 필요없지만, `requires_grad=True`로 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용
- Autograd 구현에서 매우 중요한 클래스 : `Function` 클래스

#### Autograd 예시 1)

```python
import torch

# requires_grad를 설정할 때만 기울기 추적
x = torch.tensor([3.0,4.0], requires_grad = True)
y = torch.tensor([1.0,2.0], requires_grad = True)
z = x + y

print(z) 
print(z.grad_fn)
```
```
tensor([4., 6.], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x7f6f4369b6d0>
```

---------------------------------------------------------------------------------------

```python
out = z.mean()
print(out)
print(out.grad_fn)
```
```
tensor(5., grad_fn=<MeanBackward0>)
<MeanBackward0 object at 0x7f6f43654790>
```

---------------------------------------------------------------------------------------

```python
out.backward()
print(x.grad)
print(y.grad)
print(z.grad)
```
```
tensor([0.5000, 0.5000])
tensor([0.5000, 0.5000])
None
```

---------------------------------------------------------------------------------------

```python
import torch

# requires_grad를 설정할 때만 기울기 추적
x = torch.tensor([3.0, 4.0], requires_grad=True)
y = torch.tensor([1.0, 2.0], requires_grad=True)
z = x + y

print(z) # [4.0, 6.0]
print(z.grad_fn) # 더하기(add)

out = z.mean()
print(out) # 5.0
print(out.grad_fn) # 평균(mean)

out.backward() # scalar에 대하여 가능
print(x.grad)
print(y.grad)
print(z.grad) # leaf variable에 대해서만 gradient 추적이 가능하다. 따라서 None. 즉, x,y 에 대해서만 gradient 값 추적 가능
```
```
tensor([4., 6.], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x7f6f43654640>

tensor(5., grad_fn=<MeanBackward0>)
<MeanBackward0 object at 0x7f6f43654790>

tensor([0.5000, 0.5000])
tensor([0.5000, 0.5000])
None
```

---------------------------------------------------------------------------------------

- 일반적으로 모델을 학습할 때는 **기울기(gradient)를 추적**한다.
- 하지만, 학습된 모델을 사용할 때는 파라미터를 업데이트하지 않으므로, 기울기를 추적하지 않는 것이 일반적이다.

```python
temp = torch.tensor([3.0, 4.0], requires_grad = True)
print(temp.requires_grad)
print((temp**2).requires_grad)
```
```
True
True
```

---------------------------------------------------------------------------------------

```python
with torch.no_grad():
    temp = torch.tensor([3.0,4.0], requires_grad = True)
    print(temp.requires_grad)
    print((temp**2).requires_grad)
```
```
True
False
```

---------------------------------------------------------------------------------------

```python
temp = torch.tensor([3.0, 4.0], requires_grad=True)
print(temp.requires_grad)
print((temp ** 2).requires_grad)

# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠르다.
with torch.no_grad():
    temp = torch.tensor([3.0, 4.0], requires_grad=True)
    print(temp.requires_grad)
    print((temp ** 2).requires_grad)
```
```
True
True
True
False
```

---------------------------------------------------------------------------------------

#### Autograd 예시 2)

```python
import torch

x = torch.ones(3,3, requires_grad = True)
print(x)

y = x + 5
print(y)
print(y.grad_fn) ## +5 연산에 대해서 back ward할 수 있는 오브젝트가 있다: AddBackward0

z = y*y*2
out = z.mean()

print(z, out) ## z는 더하기 연산에 대한 addbackward에 더하여 Mulbackward가 붙었고,
              ## out은 평균을 구해줬으므로 72, out에도 gradient fucntion이 붙었는데 meanbackward가 붙음)
```
```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], requires_grad=True)
        
tensor([[6., 6., 6.],
        [6., 6., 6.],
        [6., 6., 6.]], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x7f6f4366f4f0>

tensor([[72., 72., 72.],
        [72., 72., 72.],
        [72., 72., 72.]], grad_fn=<MulBackward0>) tensor(72., grad_fn=<MeanBackward0>)
```

---------------------------------------------------------------------------------------

- `requires_grad_(...)`는 기존 텐서의 `requires_grad`값을 바꿔치기(`in-place`)하여 변경

```python
a = torch.randn(3,3)
a = ((a * 3)/(a - 1))
print(a.requires_grad)

a.requires_grad_(True)
print(a.requires_grad)

b = (a * a).sum()
print(b.grad_fn) ## b가 a*ad의 sum연산이니 SumBackward가 리턴됨
```
```
False
True
<SumBackward0 object at 0x7f6f43671400>
```

---------------------------------------------------------------------------------------

이제 기울기 즉, Gradient 값을 계산해야한다. requires_grad = True 로 설정되어 있으니 `.backward()`를 통해 역전파 계산이 가능하다.(**Backpropagation**)
```python
out.backward()
print(x.grad)
```
```
tensor([[2.6667, 2.6667, 2.6667],
        [2.6667, 2.6667, 2.6667],
        [2.6667, 2.6667, 2.6667]])
```
이처럼. 역전파를 진행하는 명령어를 통해서 실행했고, $$\frac{\part{(out)}}{\part{x}}$$값이 바로 `x.grad`이다.

```python
x = torch.rand(3, requires_grad = True)

y = x * 2
while y.data.norm() < 1000:  ## x값에서 2를 곱한 y값에 2를 계속 곱해서 norm이 1000미만일때까지 곱함함
    y = y * 2

print(y)
```
```
tensor([ 589.7665,  299.5266, 1531.2843], grad_fn=<MulBackward0>)
```

---------------------------------------------------------------------------------------

```python
v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)
y.backward(v)

print(x.grad)
```
```
tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])
```

---------------------------------------------------------------------------------------

Training을 하는 것이 아닌 **Evaluation(평가)**를 할 때는, gradient값이 업데이트되면 안된다. 즉, Test set을 이용하는데 있어서 미분 값은 **업데이트하지 않고 유지**시켜야 한다. `with torch.no_grad()` 메서드를 사용하여 gradient의 업데이트를 하지 않는다.

```python
print(x.requires_grad)          ## True
print((x**2).requires_grad)     ## True

with torch.no_grad():
    print((x**2).requires_grad) ## False
```
```
True
True
False
```

---------------------------------------------------------------------------------------

`detach()`: 내용물(contant)은 같지만, reuquire_grad가 다른 새로운 Tensor를 가져올 때 사용하는 메서드이다.
```python
print(x.requires_grad)
y = x.detach()
print(y.requires_grad)
print(x.eq(y).all())
```
```
True
False
tensor(True)
```

#### Autograd 흐름 다시 보기(1)

- 계산 흐름 
	1. forward방향은 $$a \rightarrow b \rightarrow c \rightarrow out $$ 같다. 
	2. $$\quad \frac{\partial out}{\partial a} = ?$$

- `backward()`를 통해 
	1. $$a \leftarrow b \leftarrow c \leftarrow out $$을 계산하면 
	2. $$\frac{\partial out}{\partial a}$$값이 `a.grad`에 채워짐

```python
import torch

a = torch.ones(2,2)
print(a)
```
```
tensor([[1., 1.],
        [1., 1.]])
```

---------------------------------------------------------------------------------------

```python
a = torch.ones(2,2, requires_grad = True)
print(a)

print("a:data:", a.data)
print("a.grad:", a.grad)
print("a.grad_fn", a.grad_fn)
```
```
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
a:data: tensor([[1., 1.],
        [1., 1.]])
a.grad: None
a.grad_fn None
```

---------------------------------------------------------------------------------------

- $$b = a + 2$$ 계산

```python
b = a + 2
print(b)
```
```
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
```

---------------------------------------------------------------------------------------

- $$c = b^2$$ 계산

```python
c = b**2
print(c)
```
```
tensor([[9., 9.],
        [9., 9.]], grad_fn=<PowBackward0>)
```

---------------------------------------------------------------------------------------

- 마지막으로 out을 계산

```python
out = c.sum()
print(out)
```
```
tensor(36., grad_fn=<SumBackward0>)
```

---------------------------------------------------------------------------------------

역전파를 위해 `.backward()`진행

```python
print(out)
out.backward()
```
```
tensor(36., grad_fn=<SumBackward0>)
```

---------------------------------------------------------------------------------------

- 미분값들 확인
	- a의 `grad_fn이 None인 이유: 직접적으로 계산한 부분이 없었기 때문

a 확인

```python
print("a:data:", a.data)
print("a.grad:", a.grad)  ## 바뀐걸 볼 수 있음음
print("a.grad_fn", a.grad_fn)
```
```
a:data: tensor([[1., 1.],
        [1., 1.]])
a.grad: tensor([[6., 6.],
        [6., 6.]])
a.grad_fn None
```

b 확인

```python
print("b:data:", b.data)
print("b.grad:", b.grad)
print("b.grad_fn", b.grad_fn)
```
```
b:data: tensor([[3., 3.],
        [3., 3.]])
b.grad: None
b.grad_fn <AddBackward0 object at 0x7f6f4366fb80>
```

c 확인

```python
print("c:data:", c.data)
print("c.grad:", c.grad)
print("c.grad_fn", c.grad_fn)
```

```python
print("c:data:", c.data)
print("c.grad:", c.grad)
print("c.grad_fn", c.grad_fn)
```
```
c:data: tensor([[9., 9.],
        [9., 9.]])
c.grad: None
c.grad_fn <PowBackward0 object at 0x7f6f43642e50>
```

out 확인

```python
print("out:data:", out.data)
print("out.grad:", out.grad)
print("out.grad_fn", out.grad_fn)
```
```
out:data: tensor(36.)
out.grad: None
out.grad_fn <SumBackward0 object at 0x7f6f43698ee0>
```

---------------------------------------------------------------------------------------

#### Autograd 흐름 다시 보기(2)
- `grad`값을 넣어서 `backward`
- 아래의 코드에서 `.grad`값이 None은 gradient값이 필요하지 않기 때문

```python
x = torch.ones(3, requires_grad = True)
y = (x**2)
z = y**2 + x
out = z.sum()
print(out)
```
```
tensor(6., grad_fn=<SumBackward0>)
```

---------------------------------------------------------------------------------------

```python
grad = torch.Tensor([0.1,1,100])
z.backward(grad)

print("x:data:", x.data)
print("x.grad:", x.grad)
print("x.grad_fn", x.grad_fn)
```
```
x:data: tensor([1., 1., 1.])
x.grad: tensor([  0.5000,   5.0000, 500.0000])
x.grad_fn None
```

```python
print("y:data:", y.data)
print("y.grad:", y.grad)
print("y.grad_fn", y.grad_fn)
```
```
y:data: tensor([1., 1., 1.])
y.grad: None
y.grad_fn <PowBackward0 object at 0x7f6f436b35b0>
```

```python
print("z:data:", z.data)
print("z.grad:", z.grad)
print("z.grad_fn", z.grad_fn)
```
```
z:data: tensor([2., 2., 2.])
z.grad: None
z.grad_fn <AddBackward0 object at 0x7f6f43642c10>
```

### 3) nn & nn.functional

- 두 패키지가 같은 기능이지만 방식이 조금 다름
- 위의 `autograd` 관련 작업들을 두 패키지를 통해 진행할 수 있음
- 텐서를 직접 다룰 때 `requires_grad`와 같은 방식으로 진행할 수 있음
- 결론적으로, `torch.nn`은 attribute를 활용해 state를 저장하고 활용하고,  
  `torch.nn.functional`로 구현한 함수의 경우에는 인스턴스화 시킬 필요 없이 사용이 가능

#### nn패키치
- 주로 가중치(weights), 편향(bias)값들이 내부에서 자동으로 생성되는 레이어들을 사용할 때  
	- 따라서, `weight`값들을 직접 선언 안함


- 예시
  - Containers
  - Convolution Layers
  - Pooling layers
  - Padding Layers
  - Non-linear Activations (weighted sum, nonlinearity)
  - Non-linear Activations (other)
  - Normalization Layers
  - Recurrent Layers
  - Transformer Layers
  - Linear Layers
  - Dropout Layers
  - Sparse Layers
  - Distance Functions
  - Loss Functions
  - ..
- https://pytorch.org/docs/stable/nn.html

```python
import torch
import torch.nn as nn

# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
m1 = nn.Conv2d(16, 33, 3, stride = 2) ## in channel수, out channel수, kernel사이즈, stride
m2 = nn.Conv2d(in_channels = 16, out_channels = 33, kernel_size = 3, stride = 2)
m3 = nn.Conv2d(in_channels = 16, out_channels = 33, kernel_size = (3,5), stride = (2,1), padding = (4,2))
m4 = nn.Conv2d(in_channels = 16, out_channels = 33, kernel_size = (3,5), stride = (2,1), padding = (4,2), dilation = (3,1)) # 딜레이션. 간격(3,1)사이즈로 간격을 벌린상태로 진행 -> 다소 spars함

inp = torch.randn(20,16,50,100)

out1 = m1(inp)
out2 = m2(inp)
out3 = m3(inp)
out4 = m4(inp)

print(out1.size())
print(out2.size())
print(out3.size())
print(out4.size())
```
```
torch.Size([20, 33, 24, 49])
torch.Size([20, 33, 24, 49])
torch.Size([20, 33, 28, 100])
torch.Size([20, 33, 26, 100])
```

---------------------------------------------------------------------------------------

```python
# With square kernels and equal stride
m = nn.Conv2d(16, 33, 3, stride=2)
# non-square kernels and unequal stride and with padding
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
# non-square kernels and unequal stride and with padding and dilation
m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
input = torch.randn(20, 16, 50, 100)
output = m(input)
```

---------------------------------------------------------------------------------------

#### nn.functional 패키지

- 가중치를 직접 선언하여 인자로 넣어줘야 함.

- 예시)
    - Convolution functions
    - Pooling functions
    - Non-linear activation functions
    - Normalization functions
    - Linear functions
    - Dropout functions
    - Sparse functions
    - Distance functions
    - Loss functions
    - ..

- https://pytorch.org/docs/stable/nn.functional.html

```python
import torch
import torch.nn.functional as F

filters = torch.randn(8,4,3,3)

inputs = torch.randn(1,4,5,5)
conv = F.conv2d(inputs, filters, padding = 1)
print(conv.shape) ## filter 통과후 1,4,5,5 -> 1,8,5,5가 됨
```
```
torch.Size([1, 8, 5, 5])
```

---------------------------------------------------------------------------------------

#### Torchvision

- `transforms`: 전처리할 때 사용하는 메소드
- `transforms`에서 제공하는 클래스 이외에 일반적으로 클래스를 따로 만들어 전처리 단계를 진행
    - 아래의 코드에서 다양한 전처리 기술 확인  
    - https://pytorch.org/docs/stable/torchvision/transforms.html

```python
import torch
import torchvision.transforms as transforms
```

- 예시)
    - `DataLoader`의 인자로 들어갈 `transform`을 미리 정의할 수 있음
    - `Compose`를 통해 리스트 안에 순서대로 전처리 진행
    - 대표적인 예로, `ToTensor`()를 하는 이유는  **<u>torchvision이 PIL Image형태로만 입력을 받기 때문에</u> **데이터 처리를 위해서 Tensor형으로 변환해야함

```python
transforms = transforms.Compose([transforms.ToTensor(),
                                 transforms.Normalize(mean = (0.5,), std = (0.5,))])
```

#### utils.data


