---

title: Pytorch 문법 (3)

categories: 
  - py
  
tags:
  - [DL, Pytorch]
  
toc: true
toc_sticky: true

date: 2023-03-03
last_modified_at: 2023-03-03
---

## 1. 자동미분(Autograd)

### 1) AUTOGRAD (자동 미분)
- autograd 패키지는 Tensor의 모든 연산에 대해 **자동 미분** 제공
- 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻
- backprop를 위한 미분값을 자동으로 계산

### 2) Tensor
- data: tensor형태의 데이터
- grad: data가 겨쳐온 layer에 대한 미분값 저장
- grad_fn: 미분값을 계산한 함수에 대한 정보 저장 (어떤 함수에 대해서 backprop 했는지)
- `requires_grad` 속성을 `True`로 설정하면, 해당 텐서에서 이루어지는 모든 연산들을 추적하기 시작
- 계산이 완료된 후, `.backward()`를 호출하면 자동으로 `gradient`를 계산할 수 있으며, `.grad` 속성에 누적됨
- 기록을 추적하는 것을 중단하게 하려면, `.detach()`를 호출하여 연산기록으로부터 분리
- 기록을 추적하는 것을 방지하기 위해 코드 블럭을 `with torch.no_grad():`로 감싸면 `gradient`는 필요없지만, `requires_grad=True`로 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용
- Autograd 구현에서 매우 중요한 클래스 : `Function` 클래스

#### Autograd 예시 1)

```python
import torch

# requires_grad를 설정할 때만 기울기 추적
x = torch.tensor([3.0,4.0], requires_grad = True)
y = torch.tensor([1.0,2.0], requires_grad = True)
z = x + y

print(z) 
print(z.grad_fn)
```
```
tensor([4., 6.], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x7f6f4369b6d0>
```
---------------------------------------------------------------------------------------
```python
out = z.mean()
print(out)
print(out.grad_fn)
```
```
tensor(5., grad_fn=<MeanBackward0>)
<MeanBackward0 object at 0x7f6f43654790>
```
---------------------------------------------------------------------------------------
```python
out.backward()
print(x.grad)
print(y.grad)
print(z.grad)
```
```
tensor([0.5000, 0.5000])
tensor([0.5000, 0.5000])
None
```
---------------------------------------------------------------------------------------
```python
import torch

# requires_grad를 설정할 때만 기울기 추적
x = torch.tensor([3.0, 4.0], requires_grad=True)
y = torch.tensor([1.0, 2.0], requires_grad=True)
z = x + y

print(z) # [4.0, 6.0]
print(z.grad_fn) # 더하기(add)

out = z.mean()
print(out) # 5.0
print(out.grad_fn) # 평균(mean)

out.backward() # scalar에 대하여 가능
print(x.grad)
print(y.grad)
print(z.grad) # leaf variable에 대해서만 gradient 추적이 가능하다. 따라서 None. 즉, x,y 에 대해서만 gradient 값 추적 가능
```
```
tensor([4., 6.], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x7f6f43654640>

tensor(5., grad_fn=<MeanBackward0>)
<MeanBackward0 object at 0x7f6f43654790>

tensor([0.5000, 0.5000])
tensor([0.5000, 0.5000])
None
```
---------------------------------------------------------------------------------------

- 일반적으로 모델을 학습할 때는 **기울기(gradient)를 추적**한다.
- 하지만, 학습된 모델을 사용할 때는 파라미터를 업데이트하지 않으므로, 기울기를 추적하지 않는 것이 일반적이다.

```python
temp = torch.tensor([3.0, 4.0], requires_grad = True)
print(temp.requires_grad)
print((temp**2).requires_grad)
```
```
True
True
```
---------------------------------------------------------------------------------------
```python
with torch.no_grad():
    temp = torch.tensor([3.0,4.0], requires_grad = True)
    print(temp.requires_grad)
    print((temp**2).requires_grad)
```
```
True
False
```
---------------------------------------------------------------------------------------
```python
temp = torch.tensor([3.0, 4.0], requires_grad=True)
print(temp.requires_grad)
print((temp ** 2).requires_grad)

# 기울기 추적을 하지 않기 때문에 계산 속도가 더 빠르다.
with torch.no_grad():
    temp = torch.tensor([3.0, 4.0], requires_grad=True)
    print(temp.requires_grad)
    print((temp ** 2).requires_grad)
```
```
True
True
True
False
```
---------------------------------------------------------------------------------------

#### Autograd 예시 2)

```python
import torch

x = torch.ones(3,3, requires_grad = True)
print(x)

y = x + 5
print(y)
print(y.grad_fn) ## +5 연산에 대해서 back ward할 수 있는 오브젝트가 있다: AddBackward0

z = y*y*2
out = z.mean()

print(z, out) ## z는 더하기 연산에 대한 addbackward에 더하여 Mulbackward가 붙었고,
              ## out은 평균을 구해줬으므로 72, out에도 gradient fucntion이 붙었는데 meanbackward가 붙음)
```
```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], requires_grad=True)
        
tensor([[6., 6., 6.],
        [6., 6., 6.],
        [6., 6., 6.]], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x7f6f4366f4f0>

tensor([[72., 72., 72.],
        [72., 72., 72.],
        [72., 72., 72.]], grad_fn=<MulBackward0>) tensor(72., grad_fn=<MeanBackward0>)
```
---------------------------------------------------------------------------------------

- `requires_grad_(...)`는 기존 텐서의 `requires_grad`값을 바꿔치기(`in-place`)하여 변경

```python
a = torch.randn(3,3)
a = ((a * 3)/(a - 1))
print(a.requires_grad)

a.requires_grad_(True)
print(a.requires_grad)

b = (a * a).sum()
print(b.grad_fn) ## b가 a*ad의 sum연산이니 SumBackward가 리턴됨
```
```
False
True
<SumBackward0 object at 0x7f6f43671400>
```
