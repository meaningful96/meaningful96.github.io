---
title: (cs224w) Chapter 10. 
categories: 
  - Graph
  
tags:
  - [Graph, KG, cs224w]
  
toc: true
toc_sticky: true

date: 2023-02-10
last_modified_at: 2023-02-10
---

cs224w 10주차

- 10.1 Heterogeneous & Knowledge Graph Embedding
- 10.2 Knowledge Graph Completion
- 10.3 Knowledge Graph Completion Algorithm



## 1. Heterogeneous Graph and Relational GCN(RGCN)

### 1) Recap of Heterogeneous Graph 

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218294842-4ec07765-f78e-4bf0-8173-e95ccbbb970e.png">
</p> 

Heterogeneous Graph는 쉽게 말해 <span style = "color:gold">**Edge(Relation)이나 Node(Entity)의 종류 혹은 타입이 여러 개**</span>인 그래프이다.  오른쪽의 예시 그래프를 보면 이해하기 쉽다.

- 총 노드 수 8개
    - $$a_1, a_2, a_3$$ 는 Actor Type Node이다.
    - $$m_1, m_2, m_3$$는 Movie Type Node이다.
    - $$d_1, d_2$$는 Director Type Node이다.
- 즉, Node의 Type이 여러개인 Node이다.



[Heterogeneous Graph with multiple node & edge type]

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218295010-bc0c9820-4a96-4b32-9383-81239351aab3.png">
</p>

- Biomedical Knowledge Graph
    - Multiple Node Type
    - Multiple Edge Type
- Event Graph
    - Multiple Node Type
    - Multiple Edge Type





### 2) Recap of Original GCN

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218295119-2531d5b9-524a-4770-bb45-b2cdb516ace8.png">
</p>

RGCN에 앞서 Original GCN(Traditional GCN)의 임베딩 방법을 살펴보면, 두 가지 단계로 구성된다.

- Message Transformation
- Message Aggregation


먼저 **Message Transformation**부분을 보면, 이전 Layer의 임베딩 결과값을 Indegree값으로 Normalized 한 값을 나눈 값에 Weight를 곱해준 형태이다. 
- Message Transformation
	1. Normalized $$h_u^{l-1}$$ by parent's indegree (Ex. 모든 노드의 평균 degree로 Normalized 한 값을 이전 레이어 임베딩에 곱함)
	2. Weighted by $$W^{(l)}$$ (곱해진 값을 가중치를 곱해 Importance를 구함)

다음으로 **Message Aggregation**이다. 모든 노드들에 대해 Transformation된 값들을 취합해주면 되므로, $$\sum$$처리를 하면된다.


### 3)  Relational GCN(RGCN)

#### History

RGCN이라는 모델이 나온 배경은, '어떻게하면 GCN으로 Heterogeneous Graph를 다룰수 있는가?'에서 시작한다. 실제로 이 세상에 존재하는 대부분의 그래프는 Heterogeneous Graph이고 그렇기에 그에 걸맞는 모델의 필요성이 증가함에 따라 연구가 진행되었다.

이전에 리뷰했던 '[Identity-Aware Graph Neural Network]("https://meaningful96.github.io/paperreview/IDGNN/")'라는 논문에서와 비슷한 방식으로 Heterogeneous Graph를 표현할 수 있다. 이 논문에서는 Isomorphic한 Graph의 특징으로 노드들의 위치 정보를 구별하지 못하기에 '**Coloring**'을 통해서 노드들의 위치 정보를 주입해주는 방식을 제안했다. 이 Coloring을 하기 위해서는 서로 다른 Neural Network를 이용해 노드들을 임베딩한다.

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218296327-6804895d-628c-4d59-8f36-32d35c70214d.png">
</p>

마찬가지로, Relation이 여러개인 Heterogeneous Graph를 표현해주기 위해 <span style = "color:aqua">**서로 다른 신경망을 이용해 Weight Matrix**</span>를 만들어낸다. ID-GNN과 비슷하게 RGCN은 **Relation에 Coloring**을 하는 것이다.





### 4) RGCN 구조
#### Embedding
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218321912-1376dbd3-df30-45b9-a170-0db0697877e4.png">
</p>
RGCN의 임베딩의 기본적인 형태는 Original GCN과 비슷하다. 하지만, 기존의 GCN과 크게 차이가나는 부분이 세 가지 있다.

- Normalized Term
- Total Message Transformation
- Self-Loop

먼저 <span style = "color:aqua">**Normalized 부분**</span>을 보면, 기존의 GCN은 Homogeneous Graph를 다루기 때문에 relation에 Independent하다. 하지만, RGCN은 이름에서도 알 수 있듯이 **relation에 dependent하기 때문에 Normalized부분도 relation을 고려**해주어야한다. 

물론,  <span style = "color:aqua">**Message Transformation**</span>되는 부분도 **relation에 dependent**하기 때문에, Weight Matrix를 $$\bf{W}_r^{(l)}$$ 로 표기가 된다.

마지막으로 <span style = "color:aqua">**Self-Loop**</span>이다. 서로 다른 relation으로 기준 노드에서 이웃 노드들의 정보를 Aggregation하게 된다. 이 때, 만약 relation의 출발과 끝 모두 자기 자신이고, 이러한 Self-Loop가 relation마다 달라 여러개가 존재한다면, 이 정보 역시 취합해줘야한다. Original GCN에서는 Homogeneous Graph이기에 Self-Loop를 따로 고려하지 않아도 된다. 반면, RGCN은 relation에 dependent하기 때문에 **relation마다 Self-Loop가 다를 수도 있어서 따로 취합해주어야 하는 것**이다.





#### Scalability - (1)

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218842088-8b7cdc39-03d4-40b9-9b07-d5da48b924e8.png">
</p>
RGCN에는 **Scalability**라는 단점이 있다. 서로 다른 Weight Matrix를 만들기위해 다양한 Neural Network를 만들기 때문에 계산해야하는 Parameter수가 매우 많다. 이는 결국 Overfitting으로 이어질 수 있다.

이를 해결하기 위해서는 두 가지 방법이 존재한다.
- Block Diagonal Matrix
- Basis Leaning(Basis Transformation)


#### Scalability - (2)
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218843795-9dffd5e9-7302-4e40-aaab-d69e7dca09ac.png">
</p>

먼저 **Block Diagonal Matrix**이다. Block Diagonal Matrix는 임베딩을 이루는 <span style = "color:aqua">**벡터열의 위치를 조작하여 임의로 대각선 방향으로 Submatrix**</span>를 만드는 방법이다. 이렇게 하면 Computational Complexity는 기존의 $$d^{(l+1)}\; \times\; d^{(l)}$$ 에서 $$B \; \times \; \frac{d^{(l+1)}}{B} \; \times \frac{d^{(l)}}{B}$$ 로 Parameter수가 줄어든다. 하지만 여기에는 치명적인 단점이 존재하는데, 바로 Embedding Matrix에 벡터열의 위치를 맘대로 조작했기 때문에 기준 노드의 1-hop 내지 2-hop의 정보들만 Aggregation되고, 나머지 정보들은 모두 유실이된다.

다음으로는 **Basis Learning**이다. Basis Learning은 Basis Transformation이라고도 하는데, 간단하게 말하면 relation별로 나눠진 Weight들을 다시 어떤 공유할 수 있는 Weight들의 $$\sum$$  꼴로 변환하는 것이다. 즉, Weight Matrix를 Basis의 Linear Combination을 하는 것을 말한다. 이렇게 되면 Basis Matrix의 계수만 학습하면 가능하기에 현저하게 계산량이 줄어들게 된다.

### 5) Link Prediction  with RGCN 
#### Overview
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218875006-809f8111-4d5c-4368-afa7-2a6588b41e8a.png">
</p>

RGCN은 relation에 dependent한 모델이다. 따라서, RGCN이 풀고자 하는 Task는 유실된 Edge를 찾는, **Link Prediction**이다.  Heterogeneous Graph를  RGCN으로 Link Prediction Task를 풀려면 먼저 Edge들의 카테고리를 분리해야 한다. 이 때, RGCN에서 일부 relation들은 데이터 수가 적어 랜덤 샘플링을 할 경우 그 relation들의 validation edge나 test edge가 없을수도 있다.

따라서, 모든 관계가 비슷한 분포로 Split 되도록 만들어주어야하며, 이는 **층화 샘플링(Stratified Sampling)**을 통해 할 수 있다.



#### Training (1)

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218877612-6eb7fc97-95cc-470e-a8c3-b16f7a83bc1b.png">
</p>

- Training Supervision Edge: 예측하려는 Edge
- Training Message Edge: 예측을 위해 사용되는 Edge(Solid Line, 진한 선)
- Negative Edge: Supervision/Message 에 속하지 않는 Edge 

만약 예측하려는 Training Supervision Edge가 ($$E, r_3, A$$) 이라면 그 의미는 'E 엔티티에서 출발해 A 엔티티로 가는데, 그 때의 Edge 타입은 $$r_3$$'이다. Negative Edge의 예시로는 ($$E, r_3, B$$)같이 그래프 상에서 보이지 않는 Triple이다.

Score function이라는 것은 예측하려는 Triple이 얼마나 좋은지 아닌지 평가할 지표를 만들어주는 함수이다. Relation-Specific Score Function은 임베딩 차원을 상수로 만들어 버린다. 즉, 하나의 실수값으로 나오기 때문에 Triple마다 적합한 정도를 평가하게 해주는 함수이다. 한가지 예를 들면 $$f_{r_1}(h_E,h_A) = h_E^TW_{r_1}h_A$$이다. $$h_E$$와 $$h_A$$의 경우 임베딩 벡터이고, $$W_{r_1}$$은 Weight Matrix이므로 $$f_{r_1}$$은 ($$1 \times 1$$)의 크기가 된다. 이를 relation  $$r_1$$ 의 Score function이라고 한다.



#### Training (2)

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218880787-8ff4dab7-51da-4910-82a0-12144fa7a978.png">
</p>

Training을 하는 과정은 크게 세 단계로 구성된다.
1. **Training Supervision Edge을 Scoring**을 한다.
2. **Negative Edge를 만든 후, Scoring**을 한다.
3. 이 Score function을 이용해 **Cross Entropy를 Loss**로 이용하여 Optimization한다.

이 때, Training Supervision Edge Score function은 상수가 되고, 이걸 Sigmoid 같은 Nonlinear function에 대입하면 Positive Sample의 Objective function이 되고[  $$log\sigma(f_{r_3}(h_E, h_A))$$  ] , Negative Score function에 Nonlinearity를 적용하면 Negative Sample의 Objective function이 된다.[  $$log(1-\sigma(f_{r_3}(h_E, h_B)))$$  ]

결론적으로, Training Supervision Edge의 Scoring function을 Maximization하는 방향으로, 그리고 Negative Edge의 Scoring function은 minimization하는 방향으로 Loss의 최적화를 진행하면 된다.



#### Prediction

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218881989-bc885a60-757e-4f30-9cc0-0fc33cdeede6.png">
</p>

Training을 진행했으면, 학습된 모델을 가지고 Prediction을 진행하며 이 모델이 좋은지 나쁜지 성능평가도 해야한다. 여기서, 성능평가라는 것은 Model이 어떤 특정 relation에 대한 validation edge를 어떻게 예측하는지를 평가하는 것이다.(제대로 예측하는지를 평가)

먼저, Prediction 과정에서 예측하려는 Edge를 Predict Validation Edge라 한다. Prediction과정은 4 단계로 구성된다.

1. Prediction 과정에서 예측하려는 Edge의 Score를 계산한다.
2. 그 Edge의 모든 Negative Edge 생성하고 Scoring한다.
3. 예측하려는 Edge의 랭킹을 구한다.
4. Evaluation Metric(**Hits@k** 와 **Reciprocal Rank**)에 적용하여 평가한다.

이 때, 두 Evaluation Metric 모두 그 값이 클수록 결과가 좋은 것이다.


## 2. Knowledge Graph(KG): Completion with Embedding
### 1) Knowledge Graph(KG)
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218883112-89cf5e22-3e14-40e4-a6ad-7bf8800a6767.png">
</p>
Knowledge Graph는 Heterogeneous Graph의 한 종류이다. 다만, Knowledge Graph에서는 Node를 **Entity**로, Edge를 **Relation**으로 부르며, 이 relation은 Entity와 Entity사이의 Domain Information으로 구성된다.

위의 그래프를 보면 이해하기 쉽다. 예를들어 (Paper-pupYear-Year)로 되어있는 Triple을 생각해볼때, Transformer 논문('Attention is all you need')를 대입해보면 이 경우 Paper = 'Attention is all you need', Year = 2017이라는 정보가 저장되어있고 이 둘 사이의 관계인 relation이 출판 연도인 pupYear가 되는 것이다.

### 2) Link Prediction VS Knowledge Graph Completion
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/218897256-1bbe6df4-a55d-493a-a1ab-576ca645d6db.png">
</p>
그래프를 분석하는 Task 중에서, Link Prediction과 Knowledge Graph Completion은 비슷하지만 조금 다르다.

- Link Prediction
    - 주어진 정보(조건) X
    - 전체 그래프에서 유실된 Edge를 찾는 것을 목표
    
- Knowledge Graph Completion
    - head Entity와 relation Edge가 주어짐
    - ($$h, r$$)가 주어졌을 때, 이 상관 관계에 알맞는 Tail Entity를 찾는 것을 목표 
    - Knowledge Graph Completion은 다시 Link prediction in KGs, Triple Classification, Relation Prediction으로 나눠진다.
        1. Link Prediction in KGs: ($$h,r, ?$$) , ($$?, r, t$$) 에서 ?를 찾아내는 문제이다.
        2. Triple Classification: ($$h,r,t$$)의 참/거짓을 알아내는 문제이다. 
        3. Relation Prediction: ($$h,?,t$$)가 주어졌을 때 Head와 Tail의 관계성을 나타내는 Relation Edge를 찾는 문제이다.

위의 예시에서 만약 Triple 중 $$(J.K. Rowling, genre, ?)$$인 경우 Link Prediction Task로 Tail Entity인 Science Fiction을 찾는 것이 되고, 만약 $$(J.K. Roling, ?, Science Fiction)$$일 경우, Relation Prediction Task가 되며 ?는 genre가 된다.


## 3. KG Completion: TransE, TransR, DistMul, ComplEx
### 1) KG Representation
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/219945528-490f133e-383c-46a4-a91b-1a80da138096.png">
</p>

Knowledge Graph에서는 문제 정의를 위해서 Triple이라는 개념을 사용한다. Triple은 (Entity(Node), Relation(Edge), Entity(Node))의 형태를 말하며 KG에서는 특별히 <span style = "color:aqua">**Relation에서 시작 Entity를 Head, 끝 부분을 Tail**</span>이라고 지칭한다. 따라서 ($$h,r,t$$)로 표기한다.

Knowledge Graph Completion의 Key Idea는 <span style ="color:gold">Triple이 주어졌을 때, head 와 relation의 임베딩($e_{(h,r)}$)이 t의 임베딩($e_t$)과 가까워지게 하는 것</span>이다. 따라서, 여러가지 KG Completion 모델들을 공부할 때, 각각의 모델들이 어떻게 ($$h,r$$)을 임베딩하고, 거리가 가까워지는 것을 어떻게 수학적으로 표현하는지에 주목해야 한다.
- How to Embed ($$h,r$$)
- How to define closeness



Connectivity Pattern이라는 것은 가지고 있는 Data Set의 feature들이 어떠한 패턴을 나타내는지이다. 
- Symmetric relation: <$$h,r$$> = <$$t$$>에서 head와 tail을 바꿔도 참인 경우

- Antisymmetric relation: <$$h,r$$> = <$$t$$>에서 head와 tail을 바꾼 <$$t,r$$> = <$$h$$>가 서로 다른 값인 경우. 
    - 즉, head와 tail을 바꾸면 완전히 다른 값이 나오는 경우이다.
    
- Inverse relation: head와 tail을 바꿨을때 기존의 Triple관계와 같은 값이 나오는 어떤 relation이 존재하는 경우
    - 즉, $$<h,r_1,t> \;= \;<t,r_2,h>$$를 만족하고, $$r_2 \; = \; -r_1$$ 이다.
    
- Compositional relation: <$$h_1, r_1, t_1$$> + <$$t_1, r_2, t_2$$> 의 합이(벡터 합) <$$h_2, r_2, t_2$$>인 경우이다.
    - 즉, ($$h_1, t_1$$)가 vector $$r_1$$에의해 ($$t_1, t_2$$)로 가고 여기에 $$r_2$$를 더해준 값이 ($$h_2, t_2$$)이다.
    - 또한 ($$h_1, t_1$$) 에 어떤 relation vector인 $$r_3$$를 더해주면 (h_2, t_2)가 된다.
    
- 1-to-N relation: 연구실 하나에 교수님이 한분이고, 여러 명의 대학원생이 있는 것처럼 하나의 h와 r이 여러 개의 tail을 만드는 경우
    - 즉, <$$h,r,t$$> = <교수님, h에 소속됨, 학생1> = <교수님, h에 소속됨, 학생2> =$$\cdots$$ 로 예시를 들 수 있음.

이 Connectivity Pattern이 중요한 이유는, 만약 가지고 있는 Data set이 Symmetric과 1-to-N relation을 만족할 경우 이 두 가지 패턴을 만족하는 모델을 사용해 문제를 해결하는 것이 가장 성능적으로 우수하기 때문이다. 따라서, 모델이 무슨 Pattern에 대해서 참인지도 파악해야 한다.

### 2) TransE 
#### Scoring function
<span style = "font-size:80%">(Scoring function과 Connectivity Pattern 중심으로 설명)</span>
<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220021831-4c9cc414-f73c-4f68-83a1-5d795f3aa121.png">
</p>

TransE 모델의 Scoring function은 $$||h+r-t||$$정의된다. 즉, head 임베딩과 relation 임베딩을 더한값에서 tail 임베딩을 뺀 형태이다. 또한 Scoring function은  L1-norm 또는 L2-norm이다.

Scoring function의 목적은 관계가 성립하는 tail entity를 찾기 위함이다. <span style = "color:aqua">관계가 성립하는 $t$에 대해서는 그 거리를 최소화하고, 관계가 성립하지 않는 $t^{'}$ 에 대해서는 거리를 최대화</span> 해야한다. 따라서 이를 잘 반영해주는 Loss가 Cross-Entropy를 Positive Sample의 거리와 Negative Sample의 거리의 차로 표현하며, Loss를 최소화하는 방향으로 학습하면 된다.

#### Connectivity Pattern

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220023925-f5108ae4-10ad-4c1c-8a48-ecf8ddf1fed8.png">
</p>

TransE 모델에서 적용할 수 있는 Connectivity Pattern은 **'Antisymmetric', 'Inverse', 'Composition'** 으로 총 3가지이다. 

- Antisymmetric relation pattern은 $$h$$와 $$t$$의 위치가 바뀌면 그 결과가 거짓이 되는(논리적 부정) 관계성이다.  
- Inverse relation pattern은 $$h$$와 $$t$$의 자리는 고정하고, relation이 서로 inverse 관계이고 그 결과가 참인 관계성이다.
- Compositional relation pattern은 relation이 서로 다른 벡터합으로 표현될 수 있는 경우를 말한다.



반면, TransE 모델에서 **'Symmetric'과 '1-to-N'** relation pattern은 부적합하다. 

- Symmetric은 r=0일때만 만족하므로 부적합하다.
- 1-to-N의 경우 $$t_1$$과 $$t_2$$가 그림처럼 서로 다른 경우가 있을 수 있으므로 부적합하다.





### 3) TransR

#### Scoring function 

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220026217-dd159a9a-6b09-421f-8919-e7d48d2477f2.png">
</p>

TransE 모델과 달리 TransR 모델은 <span style = "color:aqua">Entity 임베딩과 Relation 임베딩이 같은 공간에 있지 않고 **분리**</span>되어 있다. 즉, Projection을 기본 아이디어로 삼고 있는 모델이다. Entity Space에 있는 Head Entity와 Tail Entity를 Relation Space로 mapping을 하는 선형변환(linear transformation) 관계이다.

Scoring function은 TransE와 형태는 비슷하지만, Entity들이 Projection된 이후 선형 결합된 형태이다.

#### Connectivity Pattern

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220026281-510d292e-4ebb-418f-871f-5cbf14f183a0.png">
</p>

TransR 모델에서 적합한 Connectivity Pattern은 'Symmetric'과 'Antisymmetric', '1-to-N', 'Inverse' 로 총 4가지이다. 

- Symmetric relation pattern: Entity 공간에서 다르더라도 같은 점으로 mapping될 수 있기 때문에 symmetric relation을 만족한다.
- Antisymmetric relation pattern
- 1-to-N relation pattern: Symmetric과 같은 이유
- Inverse relation pattern

반면, Compositional relation pattern은 TrnasR 모델에서는 부적합한데, 그 이유는, 직관적으로 relation들이 각각의 relation에 대해 서로 다른 공간을 가지므로, 하나의 선형변환으로 표현하기 힘들기 때문이다.

### DistMult

#### Scoring function

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220032203-ff3b70b4-5b5c-4a14-b22b-0f3481ecbc96.png">
</p>

DIstMult 모델의 Scoring function은 $$h,r,t$$의 내적이다. 하지만, 여기서 중요한 것은 그 의미이다. DistMult의 Scoring function이 가지는 의미는 **cos 유사도**를 나타낸다는 것이다. 이로인해 Bilinear Modeling이라고도 한다. Scoring function의 정확한 정의는 <span style = "color:aqua">head 임베딩을 relation 임베딩과 Element-wise product한 후 이 값을 tail 임베딩과 내적한 것</span>이다. 



#### Connectivity Pattern

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220032254-9ac34ac6-4dc4-4b5a-9a70-8df2af6067a2.png">
</p>

- 만족하는 Connectivity Pattern
    - Symmetric relation pattern
    - 1-to-N relation pattern

- 불만족하는 Connectivity Pattern

    - Antisymmetric relation pattern
    - Compositional relation pattern
    - Inverse relation pattern

    

### ComplEx

#### Scoring function

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220032287-e90d15c8-5ee6-436d-8714-73b7983b0269.png">
</p>

ComplEx 모델은 DistMult 모델을 복소 평면으로 확장한 것이다. 따라서 여기서도 마찬가지로 cos 유사도를 Scoring function으로 사용하며, 실수부만 고려한다.



#### Connectivity Pattern

<p align="center">
<img width="900" alt="1" src="https://user-images.githubusercontent.com/111734605/220032340-9a8b5d46-80f7-4366-9779-ab856e9285ca.png">
</p>

복소수를 고려하기 때문에 켤레 복소수나 Complex Conjugate등을 이용하면 저갑한 것과 부적합한 relation pattern을 증명할 수 있다.

