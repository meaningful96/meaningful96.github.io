---
title: "[그래프 이론]Graph Convolution Network"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-08-05
last_modified_at: 2024-08-05
---

# GNN의 학습 메커니즘
## 1. CNN vs GNN
<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/39c98d0e-dc97-47b8-95f9-a73c4638aca3">
</p>

GNN의 개념을 이해하기 위해서는 CNN의 개념을 제대로 알고 있어야한다. CNN은 정규적이고 유클리드(Euclidean) 구조의 데이터들을 임베딩하는 반면, **GNN은 CNN을 보다 형식적으로 일반화한 버전**이다. CNN은 2D 이미지와 같은 구조화된 격자 데이터에서 효과적으로 작동한다. 예를 들어, 위의 그림은 3x3 필터를 사용한 단일 CNN 계층을 보여준다. 여기서 각 픽셀은 주변 픽셀의 정보를 통합하여 업데이트된다. 구체적으로, 각 픽셀은 **이웃 픽셀로부터 메시지를 개별적으로 변환**하고, 이 **메시지들을 합산**하여 업데이트된다. 이 과정은 다음 수식으로 표현된다:

<center>$$
\mathbf{h}_4^{(l+1)} = \sigma \left( \mathbf{W}_0^{(l)} \mathbf{h}_0^{(l)} + \mathbf{W}_1^{(l)} \mathbf{h}_1^{(l)} + \cdots + \mathbf{W}_8^{(l)} \mathbf{h}_8^{(l)} \right)
$$</center>

여기서 ($$\mathbf{h}_i$$)는 픽셀 혹은 노드의 은닉층 활성화값을 나타내고, ($$\mathbf{W}_i$$)는 각 메시지에 대한 가중치 행렬을 나타낸다.

반면, **GNN**은 <span style="color:red">**정규적이지 않고 비유클리드(Non-Euclidean) 구조의 데이터를 처리**</span>한다. 이는 그래프 구조의 데이터를 다루는 데 특화되어 있으며, 소셜 네트워크, 화합물 구조, 지식 그래프 등과 같은 복잡한 데이터 간의 관계를 분석하고 예측하는 데 사용된다. GNN은 각 정점이 이웃 정점의 정보를 집계하여 자신의 상태를 업데이트하는 방식으로 작동하며, CNN과 유사하게 **이웃 정점으로부터 메시지를 수집(Message Passing)하고 이를 통합(Aggregation)**하여 업데이트한다. 따라서 GNN은 CNN의 개념을 보다 일반화하여 다양한 데이터 구조에 적용할 수 있게 하며, 이는 다양한 도메인에서 복잡한 데이터 간의 관계를 효과적으로 학습할 수 있게 한다.

## 2. GNN의 기본 구조
<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/453738b0-e5a8-4d4a-9789-c01d635f833e">
</p>

GNN은 입력으로 **특성 행렬(Feature Matrix)**과 **인접 행렬(Adjacency matrix)**을 받는다. 구조 정보와 노드들의 특징을 활용하여 그래프 내의 노드(=정점)들의 임베딩을 학습하게 된다.

GNN의 주요 아이디어는 **노드 쌍 간에 메시지를 전달하고 이를 집계**하는 것이다. 이를 통해 노드(그리고 필요 시 간선)의 표현을 세밀하게 다듬는다. GNN의 구조는 여러 개의 은닉층을 포함하며, 각 은닉층에서는 노드 간의 정보를 전달하여 임베딩을 반복적으로 계산한다. 구체적으로, 각 단계에서 **타겟 노드의 임베딩을 1-hop 단위로 이웃 노드들의 정보를 취합하여 업데이트**한다. 각 스텝에서 이웃 노드의 정보를 통합하고, 이를 통해 타겟 노드의 임베딩을 점진적으로 구체화한다.

이 과정은 다음과 같이 요약할 수 있다:
- 인접 행렬(Adjacency matrix) =  $$ \mathbf{A} \in \mathbb{R}^{N \times N} $$
- 특성 행렬(Feature matrix) =  $$ \mathbf{X} \in \mathbb{R}^{N \times F} $$

이 예시에서, 입력은 특성 행렬과 인접행렬로 구성되며, 각 노드는 이웃 노드와의 메시지 교환을 통해 자신의 임베딩을 갱신한다. 그림에서처럼 은닉층을 거칠 때마다 ReLU와 같은 활성화 함수가 적용되어 각 단계의 임베딩이 더욱 정교해진다. 이와 같은 과정을 통해 GNN은 그래프 내의 구조적 패턴과 노드 간의 복잡한 관계를 효과적으로 학습할 수 있다.

<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/2eb31959-39bc-47f2-beb7-43866b505ec8">
</p>

이 때, 이웃 노드들의 정보(=메세지)를 타겟 노드로 전달해주는 것을 **메세지 패싱(Message Passing)**, 타겟 노드가 이 정보들을 취합하는 것을 **집계(Aggregation)**이라고 한다. 그리고 경우에 따라 추가적인 정보를 이 메세지와 함께 합치는 과정을 **결합(Combine)**이라고 한다.

## 3. Inductive Capability
<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/b59a0af8-cb15-47a1-9fda-655a9cd4fb99">
</p>

그래프 신경망(GNN)은 그래프 데이터의 노드와 엣지 정보를 이용하여 노드 임베딩을 생성한다. GNN의 중요한 특징 중 하나는 동일한 파라미터를 모든 노드가 공유한다는 점이다. 즉, 노드마다 임베딩 파라미터를 정의하는 노드 임베딩과는 달리, GNN은 파라미터를 공유(Parameter Sharing)한다. 이는 모델 파라미터의 수가 그래프의 노드 수($$\vert V \vert$$)에 비례하지 않고 서브리니어(sublinear)하게 유지된다는 것을 의미한다. 결과적으로, **GNN은 학습되지 않은 새로운 노드에 대해서도 일반화(Generalization)할 수 있는 능력**을 갖추게 된다.

위 그림에서 볼 수 있듯이, 특정 노드(A 또는 B)에 대해 그래프를 구성할 때, 동일한 집계 파라미터를 사용하여 임베딩을 계산한다. 이는 노드 수와 파라미터 수가 무관하게 만들며, 새로운 노드에 대해서도 모델이 일반화할 수 있게 한다.

GNN의 레이어는 크게 세 가지 단계로 구성된다: **메시지 전달(Message Passing)**, **집계(Aggregation)**, 그리고 선택적으로 **자기 정보 결합(Combine[\Self Information\])**이다. 이 과정에서 <span style="color:red">**각 노드는 이웃 노드의 정보를 받아 집계하고, 필요한 경우 자기 정보와 결합하여 최종 임베딩을 생성**</span>한다. 

그러나 모든 GNN 모델이 Inductive한 것은 아니다. 일부 모델은 Transductive한 성격을 가지며, 학습된 그래프의 구조에만 적용될 수 있다. 예를 들어, GCN(Graph Convolutional Network)은 Transductive한 반면, GraphSAGE는 Inductive한 특성을 가진다.

## 4. GNN Operation
<p align="center">
<img width="450" alt="1" src="https://github.com/user-attachments/assets/7ff55a9d-1d45-44d9-a627-bc8a6ec8b9b2">
</p>

### 4-1. 메세지 전달(Message Passing)
메시지 전달(Message Passing) 과정은 그래프 신경망(GNN)에서 중요한 단계 중 하나이다. 이 과정에서는 각 노드가 이웃 노드로부터 메시지를 받아 이를 전달할 메시지로 변환한다. 메시지 함수(MSG 함수)는 이전 레이어의 노드 특징 ($$h_u^{(l-1)}$$)을 입력으로 받아 새로운 메시지 ($$m_u^{(l)}$$)를 생성한다. 하나의 이웃 노드는 하나의 메시지를 생성하며, 이 메시지들은 집계 과정에서 사용된다.

메시지 전달 과정은 다음과 같은 세부 단계로 구성된다:  
1. **메시지 함수**:
<center>$$m_u^{(l)} = \text{MSG}^{(l)} \left( h_u^{(l-1)} \right)$$</center>

2. **각 노드가 메시지를 만듦**:
<center>$$m_u^{(l)} = W^{(l)} h_u^{(l-1)}$$</center>

<br/>

예를 들어, 선형 계층(Linear Layer)에서 노드 특징을 나타내는 가중치 행렬 \( W^{(l)} \)가 메시지 함수로 사용된다. 이 경우, 메시지 함수는 다음과 같이 표현된다.  
<center>$$m_u^{(l)} = W^{(l)} h_u^{(l-1)}$$</center>



# Reference
\[강의\]: [CS224W](https://web.stanford.edu/class/cs224w/)  





