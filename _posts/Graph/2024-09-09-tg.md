---
title: "[그래프 AI]Training GNNs - GNN 학습 시 고려할 점"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-09-09
last_modified_at: 2024-09-09
---

# GNN Layer Stacking

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/50a6f80a-cf13-453d-a2f9-f5583588f0ec">
</p>

**Graph Neural Networks(GNN)**은 노드, 엣지, 그래프와 같은 구조적 데이터를 처리하기 위해 고안된 신경망이다. GNN의 기본 구성 요소는 노드의 특성(feature)과 그래프 구조 정보(엣지)로 이루어져 있으며, 각 노드는 이웃 노드로부터 정보를 받아 자신의 **표현(representation)**을 업데이트한다.

GNN Layer를 쌓는다는 것은, 이러한 정보 교환 과정을 여러 번 반복한다는 것을 의미한다. 즉, 첫 번째 GNN Layer에서는 1-hop 거리에 있는 이웃 노드로부터 정보를 받아들여 업데이트하며, 두 번째 Layer에서는 2-hop 거리에 있는 노드까지 정보를 수집하고, 세 번째 Layer에서는 3-hop 거리까지 정보를 모은다.

이 과정에서, 첫 번째 Layer의 입력은 $$h_v(0)$$으로 나타내며, 이는 타겟 노드의 초기 특성 값이다. 각 레이어를 거칠 때마다 $$h_v(1)$$, $$h_v(2)$$ 등의 새로운 노드 표현이 계산되고, 최종적으로 $$h_v(3)$$까지 업데이트된다. 각 레이어에서의 출력은 다음 레이어의 입력으로 사용되며, 이를 통해 노드의 정보를 점점 더 넓은 이웃으로부터 받아들이게 된다. 따라서 <span style="color:red">**GNN Layer를 많이 쌓는 것은 노드가 멀리 떨어진 이웃 노드의 정보까지도 반영**</span>할 수 있다는 장점이 있다.

많은 층을 쌓으면, 층 수만큼의 hop수만큼 neighbor 정보를 모아올 수 있지만, 반대로 단점도 존재하는데, 바로 **Over-Smoothing**과 **Over Squashing**문제가 발생할 수 있다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/1b7e4191-11fc-4e61-872f-ae6d7d4163f4">
</p>

# Reference
\[1\] CS224W 강의--
title: "[그래프 AI]Training GNNs - GNN 학습 시 고려할 점"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-09-09
last_modified_at: 2024-09-09
---

# GNN Layer Stacking

<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/50a6f80a-cf13-453d-a2f9-f5583588f0ec">
</p>

**Graph Neural Networks(GNN)**은 노드, 엣지, 그래프와 같은 구조적 데이터를 처리하기 위해 고안된 신경망이다. GNN의 기본 구성 요소는 노드의 특성(feature)과 그래프 구조 정보(엣지)로 이루어져 있으며, 각 노드는 이웃 노드로부터 정보를 받아 자신의 **표현(representation)**을 업데이트한다.

GNN Layer를 쌓는다는 것은, 이러한 정보 교환 과정을 여러 번 반복한다는 것을 의미한다. 즉, 첫 번째 GNN Layer에서는 1-hop 거리에 있는 이웃 노드로부터 정보를 받아들여 업데이트하며, 두 번째 Layer에서는 2-hop 거리에 있는 노드까지 정보를 수집하고, 세 번째 Layer에서는 3-hop 거리까지 정보를 모은다.

이 과정에서, 첫 번째 Layer의 입력은 $$h_v(0)$$으로 나타내며, 이는 타겟 노드의 초기 특성 값이다. 각 레이어를 거칠 때마다 $$h_v(1)$$, $$h_v(2)$$ 등의 새로운 노드 표현이 계산되고, 최종적으로 $$h_v(3)$$까지 업데이트된다. 각 레이어에서의 출력은 다음 레이어의 입력으로 사용되며, 이를 통해 노드의 정보를 점점 더 넓은 이웃으로부터 받아들이게 된다. 따라서 <span style="color:red">**GNN Layer를 많이 쌓는 것은 노드가 멀리 떨어진 이웃 노드의 정보까지도 반영**</span>할 수 있다는 장점이 있다.

많은 층을 쌓으면, 층 수만큼의 hop수만큼 neighbor 정보를 모아올 수 있지만, 반대로 단점도 존재하는데, 바로 **Over-Smoothing**과 **Over Squashing**문제가 발생할 수 있다. 

## Problem 1. Over Smoothing
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/1b7e4191-11fc-4e61-872f-ae6d7d4163f4">
</p>

Over-Smoothing은 GNN이 깊어질수록 **모든 노드의 임베딩이 동일하게 수렴하는 현상**이다. 이는 GNN Layer가 쌓일수록 각 노드가 더 멀리 떨어진 이웃으로부터 정보를 받아들이며 발생한다. GNN에서 각 노드는 K-layer GNN일 때 **K-hop 거리**까지 이웃 노드로부터 정보를 모을 수 있는데, 이를 **Receptive Field**라고 부른다.**Receptive Field**가 증가할수록, 많은 노드들이 겹치는 정보를 받아들이게 되고, 이는 **각 노드의 임베딩이 구별되지 않고 비슷해지는 문제**를 야기한다. 

예를 들어, 1-hop 이웃의 경우 겹치는 노드가 적지만, 3-hop으로 확장되면 거의 **모든 노드가 공유되는 정보를 받아들이게 된다**. 
결과적으로 GNN의 레이어가 깊어질수록 **모든 노드가 유사한 정보를 취합하게 되고**, 그로 인해 **노드 간 차별성이 사라지면서 모든 노드의 임베딩이 동일하게 수렴하게 된다**. 이러한 현상은 **노드 간의 구별 가능한 표현을 유지하는 데 방해가 되며**, GNN 모델의 성능 저하로 이어질 수 있다.

## Problem 2. Over Squashing
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/6bd3351d-94fc-41da-8ad8-5dc2418a084c">
</p>

**Over-Squashing**은 **특정 노드로부터 이웃 노드들의 정보가 압축되어 전달되는 현상**이다. 이는 GNN이 레이어가 깊어질수록 target 노드가 **지수적으로 늘어나는 이웃 노드들의 정보를 고정된 크기의 벡터로 압축**하여 받아들이기 때문이다. Over-Squashing은 **target 노드 관점에서 발생하는 문제**로, 멀리 떨어진 이웃에서 중요한 정보를 전달받기 어렵게 만든다. 

예를 들어, GNN Layer를 깊게 쌓으면 Depth가 깊어질수록 **target 노드로부터 멀리 떨어진 이웃 노드들의 정보를 수용하기 어려워진다**. 
결과적으로 hint가 충분히 전달되지 않고, 이는 모델의 성능 저하로 이어진다.

Layer를 무작정 늘린다고 성능이 좋아지지 않으며, **Layer 수가 특정 임계값을 넘어가면 Accuracy가 급격히 떨어지는 현상**이 발생한다. 
이는 Over-Squashing으로 인해 중요한 정보가 고정된 크기의 벡터에 **압축되는 과정에서 손실**되기 때문이다. 따라서, Over-Squashing은 **target 노드가 멀리 있는 이웃 노드들의 중요한 정보를 효과적으로 수용하지 못하게 만드는 문제**로 GNN의 성능에 부정적인 영향을 미친다.



# Reference
\[1\] [CS224W 강의](https://web.stanford.edu/class/cs224w/)
