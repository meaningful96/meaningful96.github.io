---
title: Chapter 2 Graph Laplacian & Graph Fourier Transform

categories: 
  - Graph
  
tags:
  - [GNN,Graph]
  
toc: true
toc_sticky: true

date: 2023-01-25
last_modified_at: 2023-01-25
---

<span style = "font-size:110%">**Contents**</span>

- Graph Laplacian

- Eigen-decomposition of Laplacian Matrix

- Graph Fourier Transform

- Spectral Graph Filtering

  

<span style = "font-size:110%">**Goal**</span>

Graph Neural Network이론의 기본적인  Background 인 Graph Fourier Transform을 공부한다.1 


## 1. Graph Laplacian

### 1) Graph Feature 표현

<p align="center">
<img width="400" alt="1" src="https://greeksharifa.github.io/public/img/Machine_Learning/2021-08-14-GFT/graph.PNG">
</p>

위와 같은 Graph $$\mathscr{G}$$가 있을 때,  node $$v$$는  feature를 갖고 있다.  

각각의 node가 갖고 있는 feature를 그 node의 **signal**이라고 할 때, node $$v_1$$ 의 signal은 $$f_1$$이라는 함수에 의해 정의된다. 



node의 집합 $$\mathscr{V}$$ = [$$v_1, v_2, v_3, v_4$$] 에 대한 **node feature  matrix**는 $$(4,d)$$ 형태의 2차원 행렬이다. 

$$
\mathscr{V} \rightarrow 
\begin{bmatrix}
f_1\\
f_2\\
f_3\\
f_4\\
\end{bmatrix} = f
$$

### 2) Laplacian Matrix   정의

그리고 이 Graph의 **인접 행렬(Adjacency Matrix)**를 표현하면 다음과 같다.

$$
A = 
\begin{bmatrix}
0&1&1&0\\
0&0&1&0\\
0&1&1&0\\
0&0&1&0\\
\end{bmatrix}
$$

그리고 Graph의 Degree Matrix는 $$D$$이며 이 두 행렬을 이용하여 <span style = "color:aqua">**Laplacian Matrix**</span>를 정의한다.

$$
L = D\;-\;A
$$



**cf) Degree Matrix**  
Given Graph $$G = (V,E)$$ with $$|V| = n$$, the **Degree Matrix** **$$D$$** for **$$G$$** i s $$n \times n$$ diagonal matrix defined as 

$$
D_{i,j} =
\begin{cases}
deg(v_i) \;\;\;\; if\;\; i = j\\
0	\;\;\;\;\;\;\;\;\;\;\;\; otherwise
\end{cases}
$$

where the degree $$deg(v_i)$$ of a vertex counts the number of times an edge terminates at that vertex. In an undirected graph, this means that each loop increases the degree of a vertex by two. In a directed graph the term *degree* may refer either to indegree (the number of incoming edges at each vertex) or outdegree(the number of outgoing edges at each vertex).

<p align="center">
<img width="300" alt="1" src="https://user-images.githubusercontent.com/111734605/214490586-c016d961-d30b-4378-ae54-784412402ff7.png">
</p>

### 3) Laplacian Matrix를 Difference Operator로 활용 

<p align="center">
<img width="500" alt="1" src="https://user-images.githubusercontent.com/111734605/214491999-6781798b-fb2a-40f9-9ef6-765168cec521.png">
</p>

위에서 $$h$$는 벡터이고, $$h_i$$는 Scalar이다. 여기서 Degree Matrix의 일반적인 특징을 적용해서 $$h_i$$의 식을 정의할 수 있다. 한가지 예시를 들어보면 다음과 같다.

$$h_2 = 2f_2 \; - \; f_1 \; - \; f_3$$
<center><span style = "font-size:80%">Ex1</span></center>

$$h_i = \displaystyle\sum_{j \in N_i}(f_i \; - \; f_j)$$
<center><span style = "font-size:80%">일반화(Generalization)</span></center>


이어서 이를  Quadratic Form 으로 재표현한 과정은 아래와 같다.

<p align="center">
<img width="600" alt="1" src="https://user-images.githubusercontent.com/111734605/214496982-a78ce34c-8b49-48f1-ab75-75417bfa1ccb.png">
</p>

마지막 줄을 보면, 결국 위 식에서 남는 것은 <span style = "color:gold">node $$i$$와 $$j$$ 사이의 연결이 존재할 때, <u>**$$f_i - f_j$$의 값이 작으면 연결된 node의 signal이 유사**</u>하다는 의미</span>로 생각할 수 있다.

참고로 **$$D^{-1}A$$** 혹은 **$$D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$$**  같은 경우는 **Transition Matrix**라고 부른다.

## 2. Eigen-decomposition of Laplacian Matrix
앞서 정의한 **Laplacian Matrix**를 고윳값 분해(Eigen Value Decomposition)에 적용해보면 다음과 같다. ($$L \; = \; D \; - \; A$$)

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/214508622-2b6cee09-7849-441c-a9b9-85bee43ef858.png">
</p>

하나의 Eigen value에 대해서 다시 살펴보면 아래와 같은데, 이 때<span style = "color:gold"> $$\lambda_i$$가 클수록 signal $$u_i$$ 의 **frequency**가 크다</span>고 해석할 수 있다. 

$$u_i^TLu_i \; = \; u_i^T \lambda_i u_i$$

## 3. Graph Fourier Transform
Fourier Transform은 물리학적으로 다양한 분야에서 매우 중요한 개념으로 활용된다.  회로나 통신 시스템을 분석하는데 있어서 Fourier Transform은, 어떤  Input Signal에 대해 sin과 cos꼴의 여러가지 신호의 합으로 표현하는 것을 말한다. 다르게 말하면, Time Domain에서 해석하던것을, Frequency Domain으로 해석하는 방법이다.


즉, 어떤 신호가 <span style = "color:aqua">sin, cos 의 Linear combination</span>으로 나타낼 수 있고, 이는 다시 말해 <span style ="color:aqua">어떤 신호가 sin, cos의 덧셈으로 분해 가능</span>하다는 것을 의미한다. 입력 신호들이 결국은 <span style = "color:gold">**여러 종류의  frequency를 갖는 함수들의 합**</span>으로 표현하는 것이다.

- Graph Fourier Transform의 목적
  - 어떤 Signal(feature)가 존재할 때 이를 우리가 표현할 수 있는 어떤 함수들의 합으로 의.미.있.게 표현하는 것이다.
  - $$L$$ 의 eigen space $$\mathscr{F}$$ 에 속해있는  $$f \in \mathscr{F}$$ 이 존재할 때, 이를 $$\widehat{f}$$ 로 변환하는 것



Graph Fourier Mode(=Basis Graph)로 **$$u_i$$**를 설정할때, 이 벡터는 Graph Laplacian Matrix의 Orthogonal Eigen Vector이다. 참고로 $$\lambda_i$$ 는 frequency, $$\widehat{f}_i$$ 를 Graph Fourier Constant를 의미한다.

- Graph Fourier Transform의 정의

$$
\widehat{f} \; = \; U^Tf \; = \; \displaystyle\sum_if_i\textbf{u}_i\\
\widehat{f} = \textbf{u}_if
$$

이 과정은 $$f$$를 $$\mathscr{F}$$로 Projection하는 것을 의미한다. 즉 기존에 존재하던 Graph Signal을 Graph Laplacian Matrix를 통해  새롭게 정의한 eigen space로 투사하는 것이다.

- Inverse Graph Fourier Transform

$$
f = U^T \widehat{f} = \displaystyle\sum_i \widehat{f} \textbf{u}_i
$$

## 4. Spectral Graph Filtering
Spectral Graph Filtering은 Graph Fourier Transoform을 어떻게 적용하는지에 대한 방법론이다. 

GFT를 통해 얻고자 하는 것은, Graph 내에 있는 어떠한 중요한 특성을 포착하는 것이다. 그리고 이 목적은 **Graph Filtering**이라는 과정을 통해 구체화된다.

Graph Filtering은 크게 **Spectral Filtering**과 **Spatial Filtering**으로 구분힐 수 있으며, Spectral이라는 것ㅇ느 Graph 인접 행렬의 eigenvalue와 eigenvector를 구한다는 뜻을 내포한다.

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/214513954-cf695595-8240-453d-8206-a20285375b16.png">
</p>

첫번째 과정에서 GFT를 적용하여 Signal을 변환하여 Graph Fourier 계수를 얻었다. 이후 두번째 과정에서 $$\widehat{g}(\Lambda)$$ 으로 정의되는 Filter를 곱한 후 IGFT 과정을 통해 최종 결과물을 얻는다.

이 과정은 Input signal $$f$$ 혹은 input data $$X$$가 주어졌을 때, 특정한 **필터**를 선택하여 곱함으로써 이 Input에서 중요한 특성을 추출하는 의미를 갖는다.

이를 수식으로 표현하면 다음과 같다.

$$g_{\theta}*x = Ug_{\theta}U^Tx$$

여기서 $$\theta$$ 는 파라미터를 의미하고, 학습 가능 여부에 따라 서로 다른 방식으로 문제를 풀어나갈 수 있다. 이후 이 식을 통해 **Graph Convolution** 을 정의하게 된다.


## Reference

[Graph Fourier Transform 설명](https://greeksharifa.github.io/machine_learning/2021/08/14/GFT/)

