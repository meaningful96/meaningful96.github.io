---
title: "[그래프 이론]Graph Attention Network(GAT)"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-08-05
last_modified_at: 2024-08-05
---

# Graph Attention Network(GAT)
## 1. GAT의 배경
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/9196fb99-444f-4064-9ce5-689a41a962f6">
</p>

**Graph Attention Network(GAT)**는 Spatial한 방법론을 채택하여 인접 행렬를 이용하지 않는다. GAT의 핵심 아이디어는 노드들이 이웃 <span style="color:red">**노드들의 중요도를 학습하여 각 이웃 노드로부터의 메시지(특징)를 다른 가중치로 받아들일 수 있게 하는 것**</span>이다. 이를 통해 각 노드에는 가중치(Weight)가 부여되며, 이 가중치들이 적용된 가중합(Weighted Sum) 연산을 수행한다. 즉, GAT는 단순히 이웃 노드들의 특징을 합치는 것이 아니라, '나'한테 영향을 주는 '정도'까지 학습하여 각 이웃 노드로부터의 영향을 동적으로 반영하는 것이다.

[GCN](https://meaningful96.github.io/graph/GNN3(GCN)/)의 경우 이웃 정보를 집계(aggregation)시 메세지를 모두 더해주는 `Sum`을 함수로 사용한다. `Sum` 방식의 기저에는 <span style="color:red">**타겟 노드에 기여하는 이웃들의 영향력이 동일**</span>하다는 가정이 있다. GAT는 **"GCN에 타겟 노드와 이웃 노드의 중요도를 차수(degree)만 고려했는데, 과연 이것을 충분할까?"**라는 질문에서 시작한다.

위의 그림에서 메세지를 취합하는 부분(Aggregate)를 보면, 이웃에서 타겟 노드로 전달되는 정보의 방향성을 화살표로 나타내고 있다. GCN의 경우 화살표의 굵기가 동일하다. 왜냐하면 타겟 노드 $$u$$를 기준으로 이웃 노드 $$v$$의 차수를 반영하여 정규화를 $$\frac{1}{\vert N(v) \vert}$$로 하였을 때, 타겟 노드의 이웃들은 모두 차수가 동일하기 때문에 그 영향력 또한 동일하다는 것이다. 하지만, GAT의 경우 각 이웃들과 **타겟과의 연결 유무가 아닌** <span style="color:red">**연결 강도**</span>**가 다르다고 가정**하는 것이다. 따라서 그림에서와 같이 화살표의 굵기에 따라 각 이웃들이 주는 메세지의 영향력도 다르다.  

## 2. "Attention" 이란?
어텐션(Attention)은 2017년 구글에서 발표한 [트랜스포머(Transformer)](https://meaningful96.github.io/nr/01-Transformer/) 이후로 여러 AI 분야에서 보편적으로 사용되고 있는 테크닉이다. 어텐션은 **입력으로 들어온 여러 데이터들에 대해 서로 간의 "중요도"를 계산**하는 것이다. 이는 정보의 과잉 상태에서 무엇에 '집중'할 지 결정하기 위한 작업이다. GAT는 이 어텐션의 아이디어를 차용해 이웃들의 메세지에 '중요도'를 계산해서 전달받는 것이 핵심이다. 

## 3. GAT에서의 Attention
### 3-1. Hidden Representation
<p align="center">
<img width="300" alt="1" src="https://github.com/user-attachments/assets/f6a455aa-8db7-4ffc-9650-7ba3b616b1a6">
</p>

위의 그림을 통해 GAT의 메커니즘을 잘 설명할 수 있다. GCN과 GAT의 가장 큰 차이점은 이웃으로부터 타겟에 메세지 전달(Message Passing)을 수행할 때 각 메세지별 가중치의 유무이다. 위의 그림에서 타겟 노드를 $$A$$라고 가정할 때 GCN은 $$l$$번째 layer의 히든 임베딩(hidden representation)은 다음과 같이 정의된다.

<center>$$h_A^{(l)} = \sigma \left( W^{(l)}h_B^{(l-1)} + W^{(l)}h_C^{(l-1)} + W^{(l)}h_D^{(l-1)} \right)$$</center>

하지만, GAT에서는 각 노드(=이웃)이 타겟에 주는 영향력이 다르다. 각 노드 사이의 연결에는 가중치가 부여되어 있기 때문에 위의 식은 가중합(Weighted Sum)으로 바뀌게 된다.

<center>$$h_A^{(l)} = \sigma \left( \textcolor{red}{\alpha_{AB}}W^{(l)}h_B^{(l-1)} + \textcolor{red}{\alpha_{AC}}W^{(l)}h_C^{(l-1)} + \textcolor{red}{\alpha_{AD}}W^{(l)}h_D^{(l-1)} \right)$$</center>

즉 GAT의 layer는 타겟 노드 $$v$$에 대해 $$h_v^{(l)} =\sigma \left(\textcolor{red}{\alpha_{vu}}W^{(l)h_u^{(l-1)}} \right)$$와 같은 방식으로 작동한다. GAT는 어텐션 가중치(Attention Weight)를 학습해야 하기 때문에 학습해야 할 파라미터 수가 늘어난다. **노드마다 $$\alpha$$와 $$W$$가 모두 다르기 때문**이다.

### 3-2. Attention Coefficient
학습을 위해서는 타겟 노드 $$v$$와 이웃 노드 $$u$$ 사이의 가중치 $$\alpha_{vu}$$를 정의해야한다. GAT에서 Attention Coefficient를 구하는 과정은 다음과 같다.

<p align="center">
<img width="600" alt="1" src="https://github.com/user-attachments/assets/007cc45f-a678-463f-ac74-a97ffb90c60c">
</p>

**1) 특징 변환**
각 노드 $$i$$와 이웃 노드 $$j$$의 은닉 표현(hidden representation)은 각각 $$h_i$$와 $$h_j$$이다. 이 두 벡터를 가중치 행렬 $$W$$를 통해 각각 $$Wh_i$$와 $$Wh_j$$로 변환한다.

**2) 어텐션 스코어 계산**  
변환된 특징 벡터 $$\mathbf{Wh}_i$$와 $$\mathbf{Wh}_j$$를 이용하여 두 노드 간의 유사성을 계산한다. 이는 주로 내적이나 결합(Concatenation) 후 소프트맥스 함수를 통해 이루어진다. 이 과정에서 비정규화된 어텐션 스코어는 다음과 같이 만들어진다. $$a$$는 어텐션 메커니즘을 의미한다.

<center>$$e_{ij} = a(\mathbf{Wh}_i, \mathbf{Wh}_j)$$</center>

일반적으로 어텐션 메커니즘은 두 벡터를 결합한 후, 신경망(Neural Network)을 통과시켜 나온 결과값을 사용한다. 논문에서는 구체적으로 `LeakyReLU` 함수를 활성 함수로 사용하는 신경망을 사용하였다. $$\mathbf{a}$$는 학습 가능한 벡터이며 $$\vert \vert$$는 결합(Concatenation)을 의미한다.

<center>$$e_{ij} = \text{LeakyReLU}(\mathbf{a}^T [Wh_i \mid \mid Wh_j])$$</center>



# Reference
논문: [Graph Attention Network](https://arxiv.org/abs/1710.10903)  



