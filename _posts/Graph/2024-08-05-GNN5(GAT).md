---
title: "[그래프 이론]Graph Attention Network(GAT)"
categories: 
  - Graph
  
toc: true
toc_sticky: true

date: 2024-08-05
last_modified_at: 2024-08-05
---

# Graph Attention Network(GAT)
## 1. GAT의 배경
<p align="center">
<img width="800" alt="1" src="https://github.com/user-attachments/assets/9196fb99-444f-4064-9ce5-689a41a962f6">
</p>

**Graph Attention Network(GAT)**는 Spatial한 방법론을 채택하여 인접 행렬를 이용하지 않는다. GAT의 핵심 아이디어는 노드들이 이웃 <span style="color:red">**노드들의 중요도를 학습하여 각 이웃 노드로부터의 메시지(특징)를 다른 가중치로 받아들일 수 있게 하는 것**</span>이다. 이를 통해 각 노드에는 가중치(Weight)가 부여되며, 이 가중치들이 적용된 가중합(Weighted Sum) 연산을 수행한다. 즉, GAT는 단순히 이웃 노드들의 특징을 합치는 것이 아니라, '나'한테 영향을 주는 '정도'까지 학습하여 각 이웃 노드로부터의 영향을 동적으로 반영하는 것이다.

[GCN](https://meaningful96.github.io/graph/GNN3(GCN)/)의 경우 이웃 정보를 집계(aggregation)시 메세지를 모두 더해주는 `Sum`을 함수로 사용한다. `Sum` 방식의 기저에는 <span style="color:red">**타겟 노드에 기여하는 이웃들의 영향력이 동일**</span>하다는 가정이 있다. GAT는 **"GCN에 타겟 노드와 이웃 노드의 중요도를 차수(degree)만 고려했는데, 과연 이것을 충분할까?"**라는 질문에서 시작한다.

위의 그림에서 메세지를 취합하는 부분(Aggregate)를 보면, 이웃에서 타겟 노드로 전달되는 정보의 방향성을 화살표로 나타내고 있다. GCN의 경우 화살표의 굵기가 동일하다. 왜냐하면 타겟 노드 $$u$$를 기준으로 이웃 노드 $$v$$의 차수를 반영하여 정규화를 $$\frac{1}{\vert N(v) \vert}$$로 하였을 때, 타겟 노드의 이웃들은 모두 차수가 동일하기 때문에 그 영향력 또한 동일하다는 것이다. 하지만, GAT의 경우 각 이웃들과 **타겟과의 연결 유무가 아닌** <span style="color:red">**연결 강도**</span>**가 다르다고 가정**하는 것이다. 따라서 그림에서와 같이 화살표의 굵기에 따라 각 이웃들이 주는 메세지의 영향력도 다르다.  

## 2. "Attention" 이란?
어텐션(Attention)은 2017년 구글에서 발표한 [트랜스포머(Transformer)](https://meaningful96.github.io/nr/01-Transformer/) 이후로 여러 AI 분야에서 보편적으로 사용되고 있는 테크닉이다. 어텐션은 **입력으로 들어온 여러 데이터들에 대해 서로 간의 "중요도"를 계산**하는 것이다. 이는 정보의 과잉 상태에서 무엇에 '집중'할 지 결정하기 위한 작업이다. GAT는 이 어텐션의 아이디어를 차용해 이웃들의 메세지에 '중요도'를 계산해서 전달받는 것이 핵심이다. 

## 3. GAT에서의 Attention
<p align="center">
<img width="300" alt="1" src="https://github.com/user-attachments/assets/f6a455aa-8db7-4ffc-9650-7ba3b616b1a6">
</p>

위의 그림을 통해 GAT의 메커니즘을 잘 설명할 수 있다. GCN과 GAT의 가장 큰 차이점은 이웃으로부터 타겟에 메세지 전달(Message Passing)을 수행할 때 각 메세지별 가중치의 유무이다. 위의 그림에서 타겟 노드를 $$A$$라고 가정할 때 GCN은 $$l$$번째 layer의 히든 임베딩(hidden representation)은 다음과 같이 정의된다.

<center>$$h_A^{(l)} = \sigma \left( W^{(l)}h_B^{(l-1)} + W^{(l)}h_C^{(l-1)} + W^{(l)}h_D^{(l-1)} \right)$$</center>
