---

title: "[Pytorch] Clipping ( torch.nn.utils.grad_norm_() )"

categories: 
  - Pytorch
  
tags:
  - [DL, Pytorch]
  
toc: true
toc_sticky: true

date: 2023-10-16
last_modified_at: 2023-10-16
---

# Common Problems with Backpropagation

## 1. Vaninshing Gradient (기울기 소실)

딥러닝을 공부하면서 발생하는 오류 중에 가장 흔한 오류는 **층이 깊어질수록 학습이 잘 될 것**이라는 생각이다. 실제로는 Layer의 수가 증가한다고 무조건 적으로 학습의 효과가 좋아지진 않는다. 특정 임계치를 넘어가면 학습에 오히려 방해가 된다. 그 이유는 바로 <span style = "color:gold"><b>Vanishing Gradient (기울기 소실)</b></span> 현상 때문이다. Vanishing Gradient란 <u><b>역전파(Backpropagation) 과정에서 출력층으로부터 멀어질수록 Gradient값이 점점 작아져 결국에는 0에 수렴</b></u>하는 현상이다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/cba519a1-ef7e-47a1-9031-3cf339ec4438">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/3a19c01d-05c6-45ae-bd97-fc2dc30134de">
</p>

Vanishing Gradient가 생기는 원인은 주로 <span style = "colog:gold">활성화 함수(Activation function)</span>과 관련이 있다. 여러 층을 걸쳐 출력된 값이 활성화 함수를 거치고 거치고 거치다보면 층의 입출력간의 차이가 점점 작아지고 결국은 이게 미분값이 0으로 만드는 것이다. **Sigmoid**함수나 **tanh**함수를 활성화 함수로 사용할 때 발생할 수 있다. **Rectified Linear Unit (ReLU)**함수를 activation function으로 사용하면 예방할 수 있다.(또는 **Leaky ReLU** 함수를 사용)

[참고: Artificial Neural Network](https://meaningful96.github.io/deeplearning/ANN/)

## 2. Exploding Gradient (기울기 폭발)
