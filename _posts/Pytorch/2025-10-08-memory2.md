---

title: "[Pytorch] 분산 학습 2: DP와 DDP 비교"

categories: 
  - Pytorch

toc: true
toc_sticky: true

date: 2025-10-08
last_modified_at: 2025-10-08
---

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Deep_Learning/Python&PyTorch/DP1.webp?raw=true">
</p>

# DataParallel (DP)
DP는 단일 프로세스 안에서 하나의 메인 GPU가 모델의 소유권을 가지며, <span style="color:gold">**forward 시점마다 모델을 다른 GPU로 복제(copy) 하여 미니배치를 장치별 마이크로배치로 나눈 뒤 병렬로 추론을 수행**</span>하고, <span style="color:gold">**backward에서 각 보조 GPU의 그라디언트를 메인 GPU로 모아 집계**</span>한 후 메인 GPU에서만 `optimizer.step()`을 호출하는 방식이다. 이때 파라미터의 실제 원본은 메인 GPU가 가지고 있으며, 다른 GPU에는 매 스텝마다 임시 복제본이 만들어진다. 따라서 DP는 싱글 노드만을 지원한다.

<details>
  <summary>싱글 노드 vs 멀티 노드</summary>
 
간단히 말해 멀티 노드는 여러 대의 물리/가상 머신(서버)을 함께 묶어 한 번에 학습하는 구성을 뜻한다. 여기서 노드(node) 는 GPU들이 장착된 하나의 서버를 의미하며, 싱글 노드는 한 대의 서버 안에서 여러 GPU를 쓰는 경우, 멀티 노드는 두 대 이상 서버의 GPU들을 모두 묶어 하나의 분산 학습 작업으로 쓰는 경우를 말한다.

</details> 

<br/>
<br/>

# Python에서 멀티스레딩
