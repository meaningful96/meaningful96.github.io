---

title: "[Pytorch] λ¶„μ‚° ν•™μµ 3: μ™„μ „ λ¶„ν•  λ°μ΄ν„° λ³‘λ ¬ μ²λ¦¬ (FSDP)"

categories: 
  - Pytorch

toc: true
toc_sticky: true

date: 2025-10-08
last_modified_at: 2025-10-08
---

# μ™„μ „ λ¶„ν•  λ°μ΄ν„° λ³‘λ ¬ μ²λ¦¬ (Fully Sharded Data Parallel, FSDP)
## FSDPμ κ°λ…
<p align="center">
<img width="700" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Deep_Learning/Python&PyTorch/FSDP_add1.png?raw=true">
</p>
μ™„μ „ λ¶„ν•  λ°μ΄ν„° λ³‘λ ¬ μ²λ¦¬ (Fully Sharded Data Parallel, FSDP)λ” **λ¨λΈ νλΌλ―Έν„°Β·κ·ΈλΌλ””μ–ΈνΈΒ·μµν‹°λ§ μƒνƒ(optimizer states)λ¥Ό GPUλ“¤μ— μƒ¤λ”©ν•μ—¬ μ¤‘λ³µ λ©”λ¨λ¦¬λ¥Ό κ±°μ μ κ±°**ν•κ³ , κ³„μ‚°κ³Ό ν†µμ‹ μ„ κ²Ήμ³(overlap) λ€ν• λ¨λΈμ„ λ‹¨μΌ μ„λ²„μ—μ„λ„ μ•μ •μ μΌλ΅ ν•™μµν•κ² ν•΄μ£Όλ” PyTorch ν‘μ¤€ λ¶„μ‚° ν•™μµ μ—”μ§„μ΄λ‹¤. μ΄ λ•, GPU μλ¥Ό workers νΉμ€ rankλΌκ³  ν•λ‹¤. DDPμ™€ λ‹¬λ¦¬ FSDPλ” κ° GPUμ— λ¨λΈμ„ λ‚λ μ„ λ³µμ ν•κΈ° λ•λ¬Έμ— λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ ν›¨μ”¬ λ§μ΄ μ¤„μ–΄λ“ λ‹¤. 

<p align="center">
<img width="700" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Deep_Learning/Python&PyTorch/FSDP_add2.png?raw=true">
</p>

FSDPλ” GPUλ§λ‹¤ λ¨λΈ μ „μ²΄κ°€ μ•„λ‹ μΌλ¶€ shardλ§ μ €μ¥ν•κ³ , forward λ° backward μ—°μ‚° μ‹ ν•„μ”ν• νλΌλ―Έν„°λ¥Ό GPU κ°„μ— μ„λ΅ κµν™(all-gather)ν•μ—¬ μ—°μ‚°μ„ μν–‰ν• ν›„ λ‹¤μ‹ shardλ΅ λ¶„μ‚° μ €μ¥(reduce-scatter)ν•λ” λ°©μ‹μΌλ΅ λ™μ‘ν•λ‹¤.

- **μ •μ**: FSDPλ” PyTorchμ κ³µμ‹ λ¨λ“λ΅, **νλΌλ―Έν„°(param)Β·κ·ΈλΌλ””μ–ΈνΈ(grad)Β·μµν‹°λ§ μƒνƒ(opt state)** λ¥Ό **κ° GPUμ— λ¶„μ‚° μ €μ¥(μƒ¤λ”©)** ν•λ” **λ°μ΄ν„° λ³‘λ ¬**μ λ°μ „ν•μ΄λ‹¤.
- **ν•µμ‹¬ λ©ν‘**: DDPκ°€ κ°–λ” β€λ¨λ“  GPUκ°€ λ¨λΈ μ „μ²΄ λ³µμ‚¬λ³Έμ„ λ³΄μ β€ν•λ” μ¤‘λ³µ λ©”λ¨λ¦¬λ¥Ό μ κ±°ν•μ—¬ **ν›¨μ”¬ ν° λ¨λΈ/λ°°μΉ/μ‹ν€€μ¤ κΈΈμ΄λ¥Ό μ²λ¦¬**ν•λ” κ²ƒμ΄λ‹¤. ν•„μ”ν• μκ°„μ—λ§ μ΅°κ°μ„ **λ¨μ•„(All-Gather)** κ³„μ‚°ν•κ³ , κ³„μ‚°μ΄ λλ‚λ©΄ λ‹¤μ‹ **ν©λΏλ¦¬κ³ (Reduce-Scatter)** μ €μ¥ν•μ—¬ λ©”λ¨λ¦¬ μƒμ£Όλ‰μ„ μµμ†ν™”ν•λ‹¤.
- `PyTorch 2.1.0` μ΄μƒ, `accelerate` ν•„μ”

## FSDP λ™μ‘ κ³Όμ •
FSDPλ” λ¨λ“(λ μ΄μ–΄) λ‹¨μ„λ΅ λν•‘λμ–΄ λ™μ‘ν•λ©°, ν• λ¨λ“μ νλΌλ―Έν„°λ” **GPU κ°μ(world size)λ§νΌ μΌκ°μ Έ κ° GPUμ— 1/π‘μ”© λ³΄κ΄€**λλ‹¤.

- **Forward μ „ μ¤€λΉ„**: ν„μ¬ μ‹¤ν–‰ν•  λ¨λ“μ νλΌλ―Έν„°λ¥Ό **All-Gather**ν•μ—¬ **μΌμ‹μ μΌλ΅ μ™„μ „ν• νλΌλ―Έν„°λ¥Ό λ©”λ¨λ¦¬μ— λ¨μ**(μ—°μ‚° κ°€λ¥ν• μƒνƒ)μ…λ‹λ‹¤.
- **Forward/Backward κ³„μ‚°**: ν•΄λ‹Ή λ¨λ“μ μ „Β·ν›„ν–¥μ„ μν–‰ν•λ‹¤.
- **Backward ν›„ μ •λ¦¬**: κ·ΈλΌλ””μ–ΈνΈλ¥Ό **Reduce-Scatter** ν•μ—¬ λ‹¤μ‹ **μƒ¤λ“ λ‹¨μ„**λ΅ λ‚λ„μ–΄ κ° GPUμ— λ³΄κ΄€ν•λ‹¤. ν•„μ” μ‹ **νλΌλ―Έν„°λ¥Ό μ¦‰μ‹ ν•΄μ (unshard β†’ reshard)** ν•μ—¬ ν”Όν¬ λ©”λ¨λ¦¬λ¥Ό λ‚®μ¶λ‹¤.
- **Optimizer Step**: μµν‹°λ§ μƒνƒ μ—­μ‹ μƒ¤λ”©λμ–΄ μμ–΄ **κ° GPUλ” μμ‹ μ—κ² μλ” νλΌλ―Έν„° μƒ¤λ“λ§ μ—…λ°μ΄νΈ**ν•λ‹¤. κ²°κ³Όμ μΌλ΅ λ¨λ“  GPUμ— λ¶„μ‚°λ μƒ¤λ“λ“¤μ΄ ν• μ¤ν… λμ— μΌκ΄€λκ² κ°±μ‹ λλ‹¤.

> β€**ν•„μ”ν•  λ•λ§ λ¨μ•λ‹¤κ°€, λλ‚λ©΄ λ°”λ΅ ν©λΏλ¦°λ‹¤**β€ λ΅ μ΄ν•΄ν•λ©΄ λλ‹¤. μ΄λ¥Ό ν†µν•΄ λ©”λ¨λ¦¬ μƒμ£Όλ‰ μµμ†ν™” + ν†µμ‹ -κ³„μ‚° μ¤λ²„λ©μ„ λ‹¬μ„±ν•λ‹¤.

## μ‹¤μ  GPU μ‚¬μ©λ‰ λΉ„κµ
- λ¨λΈ VRAM μ”κµ¬λ‰: 12GB (λ‹¨μΌ GPU κΈ°μ¤€)
- ν•™μµΒ ν™κ²½:Β 4xΒ GPUΒ (κ°Β GPUΒ VRAMΒ μ©λ‰Β 48GBΒ μ΄μƒΒ κ°€μ •)
- μµν‹°λ§μ΄μ €:Β AdamΒ (νλΌλ―Έν„°μΒ 2λ°°Β λ©”λ¨λ¦¬Β μ‚¬μ©)

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Deep_Learning/Python&PyTorch/FSDP_1.png?raw=true">
</p>

DDP μ‚¬μ© μ‹ GPU λ‹Ή λ¨λΈ κ΄€λ ¨ν•΄μ„ λ‹¨μΌ GPUμ VRAM λ§ 48GBκ°€ ν•„μ”ν•λ‹¤.  

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Deep_Learning/Python&PyTorch/FSDP_2.png?raw=true">
</p>

- 4λ“±λ¶„ μƒ¤λ”©μΌλ΅ GPU λ‹Ή λ¨λΈ κ΄€λ ¨ VRAM λ§ 12 GB ν•„μ”.
- λ‹¨, μμ „ν/μ—­μ „ν μ‹ μ„μ‹μ μΌλ΅ All-Gather λ¥Ό μ„ν•΄ λ¨λΈ λ‚΄ κ°€μ¥ ν° Layer μ VRAM μΈ a GB λ§νΌ μ—¬λ¶„ ν•„μ”.
- 12+a + b(λ°μ΄ν„°μ…‹ λ¦¬μ†μ¤)GB λ§νΌμ GPU VRAM μ΄λ©΄ λ¨λΈ ν•™μµ κ°€λ¥.

## μ£Όμ” μµμ…κ³Ό κ°λ…
- **ShardingStrategy**
    - `FULL_SHARD`: νλΌλ―Έν„°Β·κ·ΈλΌλ“Β·μµν‹°λ§ **λ¨λ‘** μƒ¤λ”©ν•λ” κΈ°λ³ΈΒ·κ¶μ¥ μ „λµμ΄λ‹¤.
    - `SHARD_GRAD_OP`: μΌλ¶€λ§ μƒ¤λ”©ν•λ” μ μ¶©μ•(κ³Όκ±° νΈν™/νΉμ μΌ€μ΄μ¤).
- **MixedPrecision**
    - νλΌλ―Έν„°/κ·ΈλΌλ“/ν†µμ‹  dtypeμ„ μ„Έλ°€ν•κ² μ§€μ •ν•΄ **BF16/FP16**λ΅ λ©”λ¨λ¦¬Β·λ€μ—­ν­μ„ μ κ°ν•λ‹¤.
- **CPUOffload**
    - μΌλ¶€ νλΌλ―Έν„°λ¥Ό CPUλ΅ λ‚΄λ Έλ‹¤ μ¬λ¦΄ μ μμΌλ‚, λ‹¨μΌ μ„λ²„μ—μ„λ” NVLink/NVSwitch ν™κ²½μ΄ μ•„λ‹λΌλ©΄ **μ†λ„ μ €ν•**κ°€ ν”ν•λ‹¤. ν•„μ”ν•  λ•λ§ μ‹ μ¤‘ν μ‚¬μ©ν•λ‹¤.
- **Param Flattening & Bucketing**
    - λ‹¤μμ μ‘μ€ ν…μ„λ¥Ό **ν”λ«ν•κ² ν•©μ³** ν†µμ‹ /ν• λ‹Ή μ¤λ²„ν—¤λ“λ¥Ό μ¤„μΈλ‹¤.
    - **λ²„ν‚· ν¬κΈ°** μ΅°μ λ΅ ν†µμ‹ -κ³„μ‚° **μ¤λ²„λ© μ •λ„**λ¥Ό μ΅°μ •ν•λ‹¤.
- **Auto Wrap Policy**
    - λ¨λ“ ν¬κΈ°/νƒ€μ… κΈ°λ° μλ™ λν•‘μΌλ΅, **μ μ ν• νν‹°μ… κ²½κ³„**λ¥Ό λ§λ“¤κ³  ν”Όν¬ λ©”λ¨λ¦¬λ¥Ό λ‚®μ¶λ‹¤.
- **State Dict μ ν•**
    - `full_state_dict`: λ¨λ“  μƒ¤λ“λ¥Ό λ¨μ•„ **μ™„μ „ν• μ²΄ν¬ν¬μΈνΈ**(λ³΄ν†µ rank 0μ—μ„ μ €μ¥)
    - `sharded_state_dict`: μƒ¤λ“ ν•νƒλ΅ λ¶„μ‚° μ €μ¥/λ΅λ“(λ€κ·λ¨μ—μ„ ν¨μ¨μ )
    - `local_state_dict`: κ° λ­ν¬ λ΅μ»¬ μƒνƒ(νΉμ κ΄€λ¦¬μ©)

## PyTorch Code Example
```python
mport torch
import torch.distributed as dist
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.utils.data import DataLoader, DistributedSampler

# λ¶„μ‚° ν™κ²½ μ΄κΈ°ν™”
dist.init_process_group(backend='nccl')

# λ¨λΈ μƒμ„± λ° FSDP μ μ©
model = MyModel().cuda()
fsdp_model = FSDP(model)

# λ°μ΄ν„° λ΅λ” μ„¤μ •
dataset = MyDataset()
sampler = DistributedSampler(dataset)
dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)

optimizer = torch.optim.Adam(fsdp_model.parameters(), lr=1e-4)

# ν•™μµ λ£¨ν”„
for epoch in range(10):
    sampler.set_epoch(epoch)
    for inputs, targets in dataloader:
        inputs, targets = inputs.cuda(), targets.cuda()
        outputs = fsdp_model(inputs)
        loss = torch.nn.functional.cross_entropy(outputs, targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# λ¶„μ‚° ν™κ²½ μ •λ¦¬
dist.destroy_process_group()
```

<br/>
<br/>

# Reference
Blog: [**FSDP μ‰½κ² μ„¤λ…ν•κΈ°**](https://beeny-ds.tistory.com/entry/FSDP-μ‰½κ²-μ„¤λ…ν•κΈ°#google_vignette)  
Blog: [**PyTorch FSDP (Fully Sharded Data Parallel) μ™„λ²½ μ΄ν•΄ν•κΈ°!**](https://mvje.tistory.com/m/285)
