---
title: "[NLP]Word2Vec: Skip-gram"
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-15
last_modified_at: 2024-08-15
---
# Word2Vec에 관하여

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/77c76d70-12c4-43d5-b916-60ec2a3aabca" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>Word2vec은 CBOW와 Skip-gram 두 종류로 나눠진다.</em>
  </figcaption>
</figure>

**Word2Vec**는 자연어 처리 분야에서 **단어를 벡터화하는 모델** 중 하나이다. 2013년 구글의 토마스 미콜로프(Thomas Mikolov)와 그의 팀이 개발한 이 모델은 **단어 간의 의미적 유사성을 반영하는 벡터 표현을 생성**하는 것이 주요 목표이다. Word2Vec는 기본적으로 <span style="color:red">**단어를 일정한 크기의 벡터 공간에 임베딩하여 단어 간의 관계를 수치적으로 표현**</span>할 수 있게 한다.

Word2Vec 모델은 크게 두 가지 아케턱처로 나뉜다.

- **CBOW(Continuous Bag of Words)**
- **Skip-gram**

[**CBOW(Continuous Bag of Words)**](https://meaningful96.github.io/nlp/doc1/)는 지난 포스터에서 다뤘다. 이번 포스터에서는 Skip-gram에 다루도록 하겠다.

## Architecture 2) Skip-gram
**Skip-gram**은 단어 임베딩을 학습하는 모델 중 하나로, <span style="color:red">**주어진 단어로부터 주변 단어들을 예측**</span>하는 방식이다. CBOW와 마찬가지로 단어의 의미를 벡터로 표현하는데 사용된다.

