---
title: "[NLP]Skip-gram vs CBOW "
categories: 
  - NLP
  
toc: true
toc_sticky: true

date: 2024-08-10
last_modified_at: 2024-08-10
---

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/c6db86de-60f5-440c-a1c3-3b60c0aaf248" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>[1] ref) Efficient Estimation of Word Representations in Vector Space</em>
  </figcaption>
</figure>

# CBOW의 개념
CBOW(Continuous Bag of Words)  <span style="color:red">**주어진 주변 단어들로부터 중심 단어를 예측**</span>하는 방식으로 학습을 진행하는 Word2Vec 모델이다.  

# Skip-gram의 개념
**Skip-gram**은 단어의 분산 표현(distributed representation)을 학습하기 위해 사용하는 모델 중 하나이다. 이 모델은 주어진 중심 단어로부터 주변 단어를 예측하는 방식으로 단어 벡터를 학습한다. 이는 CBOW와 반대 방향으로 작동하는 Word2Vec 모델로, <span style="color:red">**중심 단어와 주변 단어 간의 관계를 잘 반영하는 벡터를 학습**</span>하는 것이다.


# Reference
\[1\] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781).
