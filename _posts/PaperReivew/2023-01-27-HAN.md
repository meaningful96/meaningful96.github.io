---
title: Heterogeneous Graph Attention Network

categories: 
  - PaperReview
  
tags:
  - [GNN,Graph]
  
toc: true
toc_sticky: true

date: 2023-01-27
last_modified_at: 2023-01-27

---

<span style = "font-size:120%">Paper: Heterogeneous Graph Attention Network</span>

Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu, Yanfang Ye (2019, WWW)

## 1. Problem Statement

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/215000073-d1317816-6f8d-434c-904e-a34e3f1828bf.png">
</p>


<span style = "font-size:110%">1) GNN 모델들에 "Attention Mechanism"이 적용된 적이 없다.</span>

Attention Mechanism은 특히 Transformer기반 모델들을 이용하는 자연어 처리 분야나 딥러닝 분야에서는 많이 사용되고 있다. 하지만, GNN 기반의 모델들에는 적용하려는 시도 자체도 적었고, 
특히 Heterogeneous Graph 를 이용하는 분야에는 적용 사례가 없었다.

실제로 세상에 있는 많은 그래프는 Heterogeneous Graph이기 때문에 이 분야에 대한 모델 개선의 필요성이 증가하고 있다. 


<span style = "font-size:110%">2) 전통적인 GNN은 Heterogeneous Graph를 처리하는데 부적합하다.</span>

Heterogeneous Graph의 복잡성 때문에 기존의 전통적인 GNN 기반의 모델들은 직접적인 적용이 어렵다.

## 2. Related Work

- Node Embedding
- GNN
- GAN
- Attetnion Mechanism
- Heterogeneity of Graph

## 3. Method

### Overview

<p align="center">
<img width="700" alt="1" src="https://user-images.githubusercontent.com/111734605/215003117-4b1a43f0-e9b9-44aa-a2dc-a6f7c048e577.png">
</p>
<center><span style = "font-size:80%">Overview of HAN Structure</span></center>

Heterogeneous Graph Attention Network(HAN) 모델의 전체적인 구조를 보면 위와 같다. HAN은 크게 두 가지 Step으로 구성되어 있다.

- Node-Level Attention
- Semantic-Level Attention

Node-Level Attention은 Heterogeneous graph의 노드들의 타입이 여러개이고 이들의 feature의 차원수가 다를 수 있으므로 차원수를 맞춰주기 위해  선형 변환을 해주고, 그런다음 노드들의 정보를 Meta-path 별로 aggregation한다.

Semantic-Level Attention은 앞서 Meta-path 별로 취합 된 정보들을 다시 하나로 aggregation. 이렇게 나온 최종 임베딩 출력값으로 Loss를 Cross-entropy로 하여 MLP를 수행하여 Prediction을 한다.

### Background - Heterogeneous Graph

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/215005601-12367cbf-30e3-4c5b-b3f3-19ca79d6b458.png ">
</p>


Heterogeneous Graph는 왼쪽 그림을 보면 이해하기 쉽다. 노드들이 총 8개로 구성된 그래프인데, 노드들의 Type이 Actor, Movie, Director로 여러 개인 것을 볼 수 있다. 이처럼  <span style = "color:aqua">**노드 타입이 여러개인 그래프**</span>를 **Heterogeneous Graph**이다. 노드의 타입이 여러 개이면 노드와 노드의 <u>관계의 다양성이 증가</u>한다. 다시 말해, 노드마다 타입이 다르기 때문에 모든 엣지들이 같은 유형의 정보만을 표현하지 않는다.

Ex)  
오른쪽 그래프에서 A와 B는 사람 타입의 노드이고, 학회1과 학회2는 학회 이름 타입의 노드이며 NLP는 연구 주제의 노드 타입이다. 즉, 총 3가지의 노드 타입으로 이루어진 그래프이다. A와 B가 무슨 연구 주제를 연구하는지 맞추는 문제를 푼다고 할 때, 만약 NLP라는 노드가 없는 타입 종류가 2개일 때  다른 사람 노드들이 학회1과 학회2에 동시에 투고하는 논문 수가 많아질수록 학습이 잘 되어 'A와 B의 연구주제가 같다' 또는 ''학회1과 학회2는 비슷한 주제를 다룬다''라고 판단한다.(**기존의 방식**)

하지만 NLP라는 노드가 있는 상태에서는 학회1과 학회2가 같은 연구 주제를 다룬다는 정보가 이미 있는 것이다. 즉 학회에 다른 정보가 반영되어 있는 것이기 때문에 A와 B의 유사함을 판단하게 된다.(**Meta-path방식**)

### Background - Meta-Path

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/215018392-d34d744e-77d4-40b5-8fb0-27a5520c8a88.png">
</p>


**Meta-Path**라는 건 Heterogeneous에서 노드 타입별로 일종의 Path를 정의하는 것이다. (d)번을 보면 그 의미를 쉽게 파악할 수 있다.

- Movie - Actor - Movie
- Movie - Director - Movie

이처럼 노드 타입별로 유의미한 정보를 이끌어 낼 수 있는 Path이다. 관계를 가지고 다른 유형의 정보를 추출하는데 용이하게 해준다. 이를 이해하기 쉽게 설명하자면, 영화배우 '드웨인 존슨'을 Actor라는 노드에 그가 출연한 영화들을 각각 Movie 노드에 할당해준다. 이 때 우리는 '드웨인 존슨'이라는 배우가 어떤 영화에 출연했었는지를 안다면, 그가 출연한 영화들 중 내가 모르는 영화가 있더라고 "액션"적 요소가 많이 있는 영화라고 추측할 수 있다.  다른 유형의 정보를 추출하는데 이처럼 Meta-Path는 유용하다.

### Structure - Step1. Node-Level Attention

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/215020477-33ff1cac-ee31-44bd-ab3f-6046e2d0d402.png">
</p>

- Node-Level Attention
    - Input    : Node Feature
    - Output : Meta-Path based Node Embedding  

<span style = "font-size:120%">1. 선형 변환</span>

먼저 Node-Level Attention이다.  먼저 빨간색 부분에 집중해서 보면 $$h_i$$ 는 노드들의 feature이다. 이 feature들은 노드 타입별로 그 사이즈가 다를수도 있다. 즉, 타입별로 Input node feature의 차원 수가 다를 수 있다. 따라서, 선형 변환을 통해 feature들의 크기를 맞춰줘야 한다.

이 선형 변환을 할 때 변환되는 **Weight 같은 경우 노드별로 공유되지 않고** 각각의 노드 타입별로 다르다.  즉, 노드 타입에 따라서 다른 가중치를 부여하지만, 같은 공간으로 임베딩을 시키려고 Projection하는 것이다.

> Due to the heterogeneity of nodes, **different types of nodes have different feature spaces.** Therefore, for each type of nodes (e.g.,node with type $$\phi_i$$ ), we design the type-specific transformation matrix $$M_{\phi_i}$$ to project the features of different types of nodes into the same feature space. Unlike [13], the type-specific transformation matrix is based on node-type rather than edge-type.

그래서, **Type-Specific Transformation Matrix**라는 것을 도입해 다른 타입의 노드들의 Input feature를 크기가 맞게 정제하는 과정, Projection을 한다.  이를 수식화하면 다음과 같다.  

$$h^{'}_i = M_{\phi_i} \cdot h_i \;\;\;(Projection) $$

Type-Specific Transformation Matrix 와 Input node feature를 내적하면 Projected node feature가 된다. 

- Type-Specific Transformation Matrix($$M_{\phi_i}$$): <span style = "color:aqua">노드 feature들의 크기를 맞춰주기위해</span> 노드 타입별로 정해주는 행렬. 
    - 내적(Inner Product)
    - 선형변환(Linear Transformation)



<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/215029003-ed779548-cb5b-4c69-804c-942e813d3965.png">
</p>

<span style = "font-size:120%">2. Attention Mechanism</span>

Projection을 통해서 feature들의 크기가 맞춰졌으면 이제 **Attention**을 한다. 여기서 주의할 점은 (2)식이다. <span style = "color:aqua">Attention을 같은 Meta-path별로 해야</span>한다. 그래서 식에 조건부로서 Meta-path를 의미하는 $$\Phi$$ 가 있는 것이다. 이를 좀 더 이해하기 쉽게 보여주자면 다음 그림과 같다.

<p align="center">
<img width="600" alt="1" src="https://user-images.githubusercontent.com/111734605/215028420-36be2deb-2603-4735-a20c-4963ff748c52.png">
</p>




동그라미를 Movie, 세모를 Actor, 네모를 Director라고 했을 때, Move-Actor-Movie와 Movie-Director-Movie의 두 Meta-Path에 대 Subgraph를 만드는 것이다. 이 Subgraph끼리 Attention을 하여 Attention Energy(Score)를 구한다.  이 때 구해진 energy(score)를 논문에서는 **Importance** 라고 지칭하였다.

Node- Level Attention에서의 Attention Score를 '**Importance of meta-path based node pair $$(i, j)$$**' 라고 한다.  이는 meta-path마다 다른 Attention vector를 i노드의 Projected feature와 j노드의 Projected vector를 concatenation한 vector와 내적을 한 후 Non-linear function을 먹인것과 같다.

 

<span style = "font-size:120%">3. Weight of meta-path based node pair</span>

구해진 attention score, Importance를 Softmax 함수를 먹이면 일종의 확률값이 얻어지는데 결론적으로 이것이 **Weight(가중치)**가된다. 이를 **Weight of meta-path based node pair $$(i,j)$$ ** 라고 한다. 이를 식으로 나타내면 (3)식과 같다.

 

<span style = "font-size:120%">4. Aggregation</span>

이제 Weight를 구했으니 meta-path별 노드 쌍의 정보를 취합해주면 된다. 즉, Weight Sum을 해줘야한다. (4)식은 먼저 노드 레벨에서 임베딩을 한 것인데, node $$i$$ 에 대한 학습된 임베딩이 $$\mathbf{z}_i^{\Phi}$$ 이다. 

<p align="center">
<img width="600" alt="1" src="https://user-images.githubusercontent.com/111734605/215036620-dbdf81cc-9bd8-4ed0-8f0b-2a17ee613570.png">
</p>

모든 노드 임베딩을 이웃 노드들로부터 Aggregating된다. 여기서 중요한 것은 하나의 single meta-path에 대하여 attention weight $$\alpha_{ij}^{\Phi}$$ 가 만들어졌기 때문에,  semantic-specific하며 하나의 의미가 있는 정보가 된다.

Heterogeneous graph는 'Scale free' 특성을 가지고 있기 때문에 데이터들의 분산(variance)이 매우 큰 편이다. 이를 해결하기 위해 논문에서는  node-level attention을 <span style = "color:aqua">**Multi-head Attention**</span> 으로 구성하였고, 이는 학습 과정을 더 안정화 시켰다.  구체적으로, Node-Level attention을 총 K번 반복하고 Aggregation까지 한 결과들을 모두 Concatenation한 식이 (5)식이다.

>Given the meta-path set {$$\Phi_1, \dots , \Phi_P$$}, after feeding node features into node-level attention, we can obtain 𝑃 groups of semantic-specific node embeddings, denoted as {$$Z_1, \dots, Z_P$$}

총 P개의 meta-path가 주어졌을 때, node feature를 node-level attention에 먹인 후 우리는 P groub의 semantic-specific한 노드 임베딩을 얻고 그 것이 {$$Z_1, \dots, Z_P$$} 이다. 이것이 Node Level-Attention의 최종 결과이다.



### Structure - Step2. Semantic-Level Attention

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/215037068-9b2be29d-c042-4c29-a4c7-cc71230ec506.png">
</p>

- Semantic-Level Attention
    - Input     : P개의 Node Embedding(Meta-path별 임베딩)
    - Output  : Final Embedding $$Z$$ 

Node-Level Attention의 결과로 얻은  {$$Z_1, \dots, Z_P$$} 있는데, 결론적으로 총 P 개의 meta-path별 노드 임베딩 값을 가진 것이다. 이를 우리는 하나의 값으로 다시 Aggregation해 주어야 한다. 
