---
title: "[논문리뷰]Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-09-20
last_modified_at: 2023-09-20
---

Zhang, H. (2023, May 17). *Investigating the effect of hard negative sample distribution on contrastive knowledge graph embedding*. ["arXiv.org. https://arxiv.org/abs/2305.10563"](https://arxiv.org/abs/2305.10563)

Knowledge Graph 분야에서 ["SimKGC"](https://meaningful96.github.io/paperreview/SimKGC/)와 같이 자연어 기반의 모델을 적용하고, negative sampling을 통한 Contrastive learning의 연구가 활발히 진행되고 있다. 
본 논문인 Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding 역시 Knolwedge Graph Complition에 Contrastive learning을 적용한 연구이며, 특히 기존의 negative sampling
방식에서 벗어나, 자연어 처리와 컴퓨터 비젼 분야에서 많은 연구가 이루어지고 있는 <span style = "color:gold"><b>Hard Negative Sampling</b></span>방식을 KGC에 성공적으로 적용했다는 것이 가장 큰 Contribution이다.

# Problem Statement

<span style="font-size:110%"><b>Gap between the negative samples and heuristic generation quality negative samples</b></span>  
Knowledge Graph Complition 분야에서도 최근들어 그래프의 Text description을 이용하여 그래프 정보를 학습하는 모델에 관한 논문이 많이 등장하였다. 특히, SimKGC같이 자연어 모델과 더불어 Contrastive learning을 접목하여 banchmakr 
dataset인 WN18RR에서 SOTA를 달성한 모델도 등장했다. Batch수를 늘리고, negative sampling을 통한 성능 향상에도 불구하고 여전히 general한 그래프에서 Link prediction에 좋은 성적을 내지는 못한다. 저자는 이러한 문제점을 <b>'Gap between the negative samples and heuristic generation quality negative samples'</b>라고 말한다. 즉, 특정 classification task에서 사용되는 negative sample과 특정 heuristic 방법을 사용하여 생성된 negative sample을 말한다. 이 때, random하게 생성된 negative sample과 특정한 메커니즘을 통해 만들어진 negative sample간에 모델을 학습하는데 있어서 random 생성 negative sample들은 오히려 noise로 작동할 수 있다. 저자는 이러한 이유로 그래프 내의 **shortest path length**를 이용한 hard negative sampling 방식을 제안한다.

<br/>
<br/>

# Related Work
## 1. Notation

Knowledge Graph는 **Triple**이라는 단위로 데이터가 저장된다. Graph는 $$\mathcal{G} \; = \; (\mathcal{E}, \mathcal{R}, \mathcal{T})$$ 로 정의한다. $$\mathcal{E}$$와 $$\mathcal{R}$$은 각각 Entity Set과 Relation Set을 의미하고 $$\mathcal{T}$$는 Triple Set을 의미한다. $$\mathcal{T} = {(h,r,t) \vert h,r \in \mathcal{E}, r \in \mathcal{R}}$$의 관계를 가진다. 즉, Triple은 head와 tail의 관계를 relation으로 표현한 것이며, 논문에 따라서 head를 subject, realtion을 predicate 그리고 tail을 object로 기술하기도 한다. <span style = "color:gold"><b>Knowledge Graph Completion (KGC)이라는 것은 $$(h,r)$$이 주어졌을 때 관계에 알맞는 $$t$$를 찾는 것</b></span>이 목표인 task이다.



## 2. InfoNCE Loss with Simple Negative Triples

InfoNCE Loss는 contrastive learning을 할 때 사용하는 대표적인 loss로, <u><b>Cross-Entropy가 InfoNCE Loss의 Special Case</b></u>라고 할 수 있다. Knowledge Graph에서 $$(h,r,t) \in \mathcal{T}$$가 주어졌을 때 K개의 negative sample을 독립적으로 동일 분포내에서 추출할 수 있다. 이 때 동일 분포에 대한 확률 값을 $$p^{-}(t)$$로 표현하고 negative sample을 $$(h,r,t^{'})$$로 표현한다. 이 때 Original InfoNCE Loss는 다음과 같다.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Paper_Reconstruction/assets/111734605/9bc514e3-65c8-42c0-b3b4-dd46188c0367">
</p>

이 때, <b>$$e_{t_j}^{-}$$</b>는 negative tail $$t_j^{-}$$의 임베딩이다. 기존의 연구들을 통해 **InfoNCE loss**는 *log Bayesian* 모델과 동일하다. 따라서 아래와 같이 loss식을 정의할 수 있다. $$\mathcal{T_{batch}} \subseteq \mathcal{T}$$이며, \#$$(t)$$가   $$\mathcal{T_{batch}}$$에서 발생한 횟수를 의미한다.

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Paper_Reconstruction/assets/111734605/d3abc5c7-149f-4561-9169-eddee2a71c42">
</p>

이 때 $$p^{-}(t)$$는 negative sample의 분포를 의미한다. 이 분포는 simple distribution으로 단순히 batch안에서 negative sample이 **발생 횟수**만을 따지기 때문이다. 기존의 SimKGC는 이처럼 batch안에서 단순한 방식으로 Negative sampling을 진행하였다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Paper_Reconstruction/assets/111734605/9b675077-933d-4b06-9ca1-d811c29376ae">
</p>


<br/>
<br/>

# Method
## 1. InfoNCE Loss with Hard Negative Triples
> Hard negatives triples are harder to distinguish from the triples in the KG than arbitrarily generated negative samples.
> One way to generate hard negative triples is to sample the tail entity from a negative sample distribution that also considers the context.

KGE 알고리즘에서는 종종 heuristics를 이용해 hard negative를 생성해낸다. 이런 hard negative를 통해 성능을 최대화하기 위함이다. Hard negative triple들은 <span style = "color:gold"><b>실제 정답과 구분하기 힘든</b></span> negative triple이다. 다시 말해, $$(h,r)$$이 주어졌을 때 <u>정답 tail과 가까운 엔티티들로 이루어진 triple</u>이다. 이러한 hard negative를 추찰하는 방법 중 한 가지는 앞서 언급한 random하게 추출하는 것이다.

논문에서는 이러한 방식은 너무 단순하기에 다른 sampling방식을 제안한다. 바로 tail 엔티티를 negative sampling 하되, **context를 고려해서 추출**하자는 것이다.(일종의 Context Sub-Graph) 정확하게 말하면, <span style = "color:gold">**Structural Context information**</span>을 사용하여 negative tail을 뽑는 것이다. 그래프의 구조적인 정보를 이용하여, Training set으로 들어온 triple의 tail을 hop수에 dependent하게 바꿔가며 negative를 주는 방식이다.

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b081b775-6a8e-4dcd-88ce-347ae04c6d34">
</p>

이와 같은 negative sampling방식을 InfoNCE Loss에 적용하면 위와 같이 식이 변형된다. 본 논문에서는 hard negative triple을 만들 때 Shortest path를 고려하였고, 이를 위해 새로운 확률 분포를 정의하였다. 다시 말해, 정답 triple(Ground Truth)의 hr 임베딩 $$e_{hr}$$에 대한 여러 tail(candidate entities)의 preference를 구하고, 그 임베딩이 가까우면(문맥적으로 의미가 있으면) 더 높은 preference를 부여한다. 이 방식은 KBGAN과 RotatE와 유사하다.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/ae117ae5-6046-4ab4-b0d3-83bb3086b465">
</p>

> generate hard negative triples by giving higher preference to the tail entities whose embeddings are close to the context embedding
