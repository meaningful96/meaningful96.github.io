---
title: "[논문리뷰]JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-03-28
last_modified_at: 2023-03-28
---

[JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs]("https://arxiv.org/abs/2106.10502")  
ACL, 2021

# Problem Statement

<span style = "font-size:120%">1. Structural information loss during encoding</span>    

기존의 Transformer 모델을 기반으로 한 Pre-trained 모델들의 경우 **Fully Connected Self-Attention**을 중심으로 모델이 구성된다. FC Self-Attention의 경우 <u>하나의 노드에
대해 missing link를 고려하지 않고 모든 노드에 대해 attention</u>을 구하기 때문에 <span style = "color:gold">**그래프의 구조적 정보가 무시되는**</span> 문제점이 발생한다.
    
<br/>  

<span style = "font-size:120%">2. Absense of Explicit graph-text aliggnments</span>  

기존의 Text generation을 위한 Pre-Trained 모델들의 경우 **auto-encoding**이나 **auto-regressive text reconstruction**방식을 채택한다. 이 방식은 유실된 정보가 있는(또는 
Masking 처리 된) 'Corrupted Text Sequence'을 인코딩하고 디코딩 결과 'Original Sequence'가 출력된다.

한 가지 명백한 사실은, <u>Knowledge Graph가 일반적인 Text Sequence보다 더 구조적으로 복잡</u>하기 때문에 <span style = "color:gold">**text reconstruction에 기반한 pre-training task를 다이렉트하게 이용하여 grap와 text를 매칭시키는 배열인 graph-text alignment를 학습시키기 난해**</span>하다.

# Related Work
<span style = "font-size:120%">1. KG-to-Text Generation</span>  

KG-to-Text는 세 가지 측면으로 나눠진다. 각각 **Encoder modification, Unsupervised Learning, Building pre-training models**이다. 
1) Encoder modification
  - Linearized Graph를 input으로 하는 Sequence Encoder의 고질적 문제인 그래프 <u>구조 정보 손실을 완화</u>하기위해 보다 더 복잡한 Encoder 모델이 필요하다.

2) Unsupervised Learning
  - graph-to-text task와 text-to-graph task를 non-parallel 한 graph-text 데이터를 이용하여 동시에 conversion하는 것을 목표로한다.
  - Unsupervised training objective to jointly learn = Joint Optimization

3) Building Pre-Training Models       
  - 기존의 모델들은 KG-to-Text 데이터셋을 text-to-text 모델에 직접적으로 fine-tuning함.
  - GPT, BART, T5
  - 이 논문에서는 직접적으로 fine-tuning하는 것이 아닌 <u>pre-training을 graph-text alighmnet를 명시적으로 학습하는데 이용</u>한다.
    
<br/> 

<span style = "font-size:120%">2. KG-Enhanced Pre-Trained Models</span>
KG-Enhanced Pre-trainined model들이 나오게 된 계기는 Knowledge Graph를 Pre-trained 모델에 **통합**하기 위한 시도에서 출발했다. 이렇게 함으로써, <u>그래프의 Entity와 Relation의 이해(Understanding entities and relations)를 자연어를 이용해 더 용이하게 하기 위함</u>이다. 


# Method
# Experiment & Result
# Contribution
