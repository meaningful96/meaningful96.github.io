---
title: "[논문리뷰]JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-03-28
last_modified_at: 2023-03-28
---

[JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs]("https://arxiv.org/abs/2106.10502")  
ACL, 2021

# Problem Statement

<span style = "font-size:120%">1. Structural information loss during encoding</span>    

기존의 Transformer 모델을 기반으로 한 Pre-trained 모델들의 경우 **Fully Connected Self-Attention**을 중심으로 모델이 구성된다. FC Self-Attention의 경우 <u>하나의 노드에
대해 missing link를 고려하지 않고 모든 노드에 대해 attention</u>을 구하기 때문에 <span style = "color:gold">**그래프의 구조적 정보가 무시되는**</span> 문제점이 발생한다.
    
<br/>  

<span style = "font-size:120%">2. Absense of Explicit graph-text aliggnments</span>  

기존의 Text generation을 위한 Pre-Trained 모델들의 경우 **auto-encoding**이나 **auto-regressive text reconstruction**방식을 채택한다. 이 방식은 유실된 정보가 있는(또는 
Masking 처리 된) 'Corrupted Text Sequence'을 인코딩하고 디코딩 결과 'Original Sequence'가 출력된다.

한 가지 명백한 사실은, <u>Knowledge Graph가 일반적인 Text Sequence보다 더 구조적으로 복잡</u>하기 때문에 <span style = "color:gold">**text reconstruction에 기반한 pre-training task를 다이렉트하게 이용하여 grap와 text를 매칭시키는 배열인 graph-text alignment를 학습시키기 난해**</span>하다.

<br/> 
<br/> 

# Related Work
<span style = "font-size:120%">1. KG-to-Text Generation</span>  

KG-to-Text는 세 가지 측면으로 나눠진다. 각각 **Encoder modification, Unsupervised Learning, Building pre-training models**이다. 
1) Encoder modification
  - Linearized Graph를 input으로 하는 Sequence Encoder의 고질적 문제인 그래프 <u>구조 정보 손실을 완화</u>하기위해 보다 더 복잡한 Encoder 모델이 필요하다.

2) Unsupervised Learning
  - graph-to-text task와 text-to-graph task를 non-parallel 한 graph-text 데이터를 이용하여 동시에 conversion하는 것을 목표로한다.
  - Unsupervised training objective to jointly learn = Joint Optimization

3) Building Pre-Training Models       
  - 기존의 모델들은 KG-to-Text 데이터셋을 text-to-text 모델에 직접적으로 fine-tuning함.
  - GPT, BART, T5
  - 이 논문에서는 직접적으로 fine-tuning하는 것이 아닌 <u>pre-training을 graph-text alighmnet를 명시적으로 학습하는데 이용</u>한다.
    
<br/> 

<span style = "font-size:120%">2. KG-Enhanced Pre-Trained Models</span>  
KG-Enhanced Pre-trainined model들이 나오게 된 계기는 Knowledge Graph를 Pre-trained 모델에 **통합**하기 위한 시도에서 출발했다. 이렇게 함으로써, <u>그래프의 Entity와 Relation의 이해(Understanding entities and relations)를 자연어를 이용해 더 용이하게 하기 위함</u>이다. 

<br/> 
<br/> 

# Method
## 1. Task Definition & Overview

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228316119-36225e91-8c04-4904-99fe-61fbba60fd5c.png">
</p>

먼저 Notation을 살펴보면 Knowlege Graph $$\mathcal{G} = (\mathcal{V,E})$$ 이다. $$\mathcal{V} = \{e_1, e_2, \cdots, e_{|\mathcal{V}|}\}$$로 엔티티(Entity) 집합을 의미하고, $$\mathcal{E} = (r_{ij})$$로 엔티티와 연결된 릴레이션(relation)을 의미한다. 당연히 i와 j는 모든 노드를 한 번씩 지칭하기에 $$r_{ij}$$의 크기는 $$ㅣ\mathcal{V}ㅣ \times ㅣ\mathcal{V}ㅣ$$이 된다. 이 때, Knowlege Graph의 정보를 Input으로 넣어주기 위해 <span style = "color:aqua">**Linearize**</span>를 하고, $$\mathcal{G}$$의 linearize를 한 수식이 $$\mathcal{G_{linear}} = (w_1, w_2, \cdots, w_m)$$이다. Linearize된 <span style = "color:aqua">$$\mathcal{G_{linear}}$$는 총 **m개**의 token</span>으로 되어 있고, 이를 통해 <span style = "color:aqua">생성되는 Text Sequence는 **n개**의 Token으로 이루어진 $$X = (x_1, x_2, \cdots, x_n)$$</span>이다. X는 이 모델 자체가 Transformer를 기반으로한
Encoder-Decoder 모델이며, 이는 **Auto-Regressive**하므로 모델의 전체 입력은 $$\mathcal{G_{linear}}$$와 $$X$$를 연속적으로 연결한 벡터가된다. 

- JoingGT 모델의 특징
  - KG의 Triple을 Linearize하고, 이를 이전 Layer의 출력값과 연결해 하나의 Input을 만든다.
  - Input Graph의 구조 정보를 보존하기위해 Structure-Aware Semantic Aggregation Module을 제시한다.
  - 그리고 3가지 새로운 Pre-Training을 제시하고 이에 접목시켜 디코더 부분을 조금씩 바꿔준다.

## 2 Model Structure

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228418591-d997ce8c-426a-4ece-9e57-aea38d9fd4e1.png">
</p>

3가지 Pre-Training Task에 대하여 공통적인 부분을 차지하는 Encoder Module을 살펴보면 크게 네 부분으로 나눠진다. 

- Encoder
  - Vainilla Self-Attention Layer
  - Pooling Layer
  - Structure-Aware Self-Attention Layer
  - Residual Layer

### 1) Vanilla Self-Attention

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228421899-2c46e6df-6343-4de7-89b3-f2d5e67bf684.png">
</p>

먼저 Vailla Self-Attention Layer를 살펴보면, 전통적인 Transformer 모델의 Self-Attention기법을 사용한다. 즉, 한 토큰이 전체 시퀀스에 있는 모든 토큰과 Attention을 통해 각각의 토큰과 얼마나 연관성이 있는지에 대한 정보를 얻게된다. 이를 통해 결론적으로 입력 시퀀스의 <span style = "color:gold">Textual Information(Contextual Semantic relations)</span>을 얻게된다. Pre-Training과정 중의 입력과 출력을 정리하면 다음과 같다. 

- Goal: Capturing Contextual Semantic Information 
- Input: Sequence ($$\mathcal{G_{Linear}} ㅣㅣ X$$), lineaized graph & corresponding text sequence  
- Output: Attention Value in $$l^{th}$$-layer $$h_i^l$$, 여기서 i는 노드(Head Entity) 번호이고 l은 레이어의 층 수를 의미한다.

다시 한 번 강조하면, Vanilla Self-Attention(Fully connected self-attention)의 결과로 엔티티간 <span style = "color:aqua">**풍부한 Contextual Semantic Relationship**</span>을 얻을 수 있고, 이 정보를 담고있는 Sequence가 Pooling layer의 입력으로 들어간다.

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228432118-6e271514-0497-4ff6-9626-6f37d26fede7.png">
</p>  
<center><span style = "font-size:80%">Vanilla Self-Attention 수식</span></center>

<br/>

### 2) Pooling Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228431733-b31c369a-fb28-4fe8-8ed4-827f9b718be2.png">
</p>

Vanilla Self-Attention의 결과로 얻은 **Contextual Semantic information**에 **Structural Information**을 넣어주기 위해서 약간의 Sequence를 변형해야 한다. 이 과정을 Pooling Layer에서 수행하게 된다. Pooling은 <span style = "color:aqua">**mean pooling**</span>으로 하며 이 결과로 엔티티의 representation과 릴레이션의 representaion을 각각 얻게된다.

- Goal: Entitiy Representation과 Relation Representation으로 Input Sequence를 분리
- Input: Vanilla Self-Attention의 결과로 얻은 Contextual semantic vector
- Output: Entity Representation & Relation Representation

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228432365-cbce93ad-c0a1-49c3-be04-f8b30427b947.png">
</p>  
<center><span style = "font-size:80%">mean pooling 수식</span></center>

<br/>

### 3) Structure-Aware Self-Attention Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228433728-2a982a70-8533-4d08-8199-339e4644f129.png">
</p>

Pooling Layer를 통해서 문맥 정보가 담겨져있는 시퀀스의 정보를 엔티티와 릴레이션의 표현식으로 각각 나뉘어졌다. 이 두 표현식을 입력으로 Structure-Aware Self-Attention에 넣어준다.
수식을 살펴보면 <u>한 엔티티의 정보를 모든 엔티티와 릴레이션의 정보를 Attention하는 것</u>을 알 수 있다. 즉, head 엔티티와 다른 모든 엔티티를 비교해 tail 엔티티가 될 수 있는 확률을 보는 한편, 그에 맞는 릴레이션까지 찾아내기위한 과정임을 알 수 있다. 즉, <span style = "color:aqua">**Contextual Semantic Information을 가진 Input Sequence에 대해 Local Structure Information를 주입**</span>하게 되는 것이다.

```
This layer integrates the contextual semantic representation of entities and relations 
based on graph structure, therby injecting the structural information into the vanila
Transformer layer which contains textual informations
```

- Goal: Graph Structure Information을 Contextual Semantic Information에 주입
- Input: Semantic Information이 있는 Entity & Relation Representation
- Output: (Semantic + Structural) 엔티티에 대한 Attenton Value

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228440150-ce38f74c-71b1-4fed-b043-bba0097ba2af.png">
</p>
<center><span style = "font-size:80%">Structural-Aware Self-Attention 수식</span></center>

<br/>

### 4) Residual Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228440513-0a5f3af5-4f75-4fbc-962c-92016fcd857b.png">
</p>

레이어의 이름에서도 알 수 있듯이, Residual(잔차)를 이용하는 레이어이다. Residual Connection을 통해서 정보를 결합하는 역할을 한다. 먼저 Structural-Aware Self-Attention의 결과로
나온 <span style = "color:aqua">**엔티티의 Semantic representation과 Structural representation을 결합**</span>하고 최종적인 Hidden State를 뽑아낸다. 

- Goal: Fusing Semantic and Structural representation
- Input: Structural-Aware Self-Attention의 결과로 얻은 엔티티의 representation(Semantic 정보 + Structural 정보)
- Output: l번째 layer의 최종 Hidden state Sequence(처음 입력값인 $$h^{l-1}$$과 동일한 형태)

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228443038-f427cbe5-fb4c-4094-a79b-10fd3c62152d.png">
</p>
<center><span style = "font-size:80%">Residual Layer 수식</span></center>

<br/>

### 5) Summary of Encoder

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228443221-7b95a3bf-3901-4a4d-928e-22bdaa3a3fdc.png">
</p>

기존의 Structure-Aware Transformer 인코더는 모델 파라미터를 통해 직접적으로 학습시키거나, 다른 KG 임베딩 모델로부터 엔티티 임베딩과 릴레이션 임베딩을 얻는다. 반면에 JointGT 인코더의 경우, <u>엔티티와 릴레이션 임베딩을 Contextual semantic representation을 이용해 얻는다.</u> 이 디자인은 구조적인 정보를 보존하면서 동시에 기존의 Pre-trained 모델로부터 얻은  contextual representation을 완전히 이용한다. 그리고 다른 KG dataset을 fine-tuning하여 새로운 엔티티와 릴레이션을 더 잘 생성한다.

## 3. Pre-Training Tasks

논문에서 제시한 Pre-Traninig Task의 목표는 <span style = "color:gold">**Input graph '$$\mathcal{G}$$'와 그에 상응하는 Text Sequence '$$X$$'가 주어졌을 때, Graph Encoder와 Sequence Decoder를  Joinly하게 학습시켜 graph-text alignment(그래프-텍스트 정렬)를 향상**</span>시키는 것이다. 총 세 가지의 새로운 Pre-Traning Task를 제안한다. 
- Graph Enhanced Text Reconstruction
- Text Enhanced Text Reconstruction

### 1) Graph Enhanced Text Reconstruction

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228449128-8b9b0e1a-a547-461f-afec-bf09c05d62f3.png">
</p>

이 Task가 풀고자 하는 것은 <span style = "color:aqua">**완전한 KG 그래프를 이용하여 Masking된 Text Sequence를 복원**</span>하는 것이다. Graph Enhanced Text Reconstruction을 풀 때 모델의 구조를 보면 앞서 제안된 JointGT의 Encoder 모듈의 입력으로 Triple정보를 Linearize한 '$$\mathcal{E_{linear}}$$'와 마스킹 처리된 Text Sequence '$$\widehat{X}	
$$'가 들어가고 Structual-Aware Self-Attention layer와 Residual Layer를 거쳐나온 최종 Output Sequence가 Transformer 기반 모델(BART, T5)의 디코더의 입력으로 들어간다. 또한 디코더의 또 다른 입력은 마스킹 처리되지 않은 Text Sequence가 들어가 Self-Attention을 진행하게 된다. 결과적으로 디코더의 출력은 Token으로 이루어진 **Text Sequence**가된다.

- Encoder
  - Input: Linearized Graph + Masked Text Sequence
  - Output: Hidden state Sequence
- Decoder
  - Input: Encoder's Output + Fully Text Sequence
  - Output: Text Sequence

Task를 풀기위해서 모델을 정의했으니 이제 최적화를위한 Loss를 설정해야한다. Loss는 $$\mathcal{L_{text}}$$로 표현하고 그 수학적인 형태는 **Negative MLE**와 비슷하다  

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/228452304-9252029a-3e33-4eff-ade2-aa885f613829.png">
</p>
<center><span style = "font-size:80%">Graph Enhanced Text Reconstruction Loss</span></center>



# Experiment & Result
# Contribution
