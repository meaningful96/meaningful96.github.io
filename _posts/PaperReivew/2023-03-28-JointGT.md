---
title: "[논문리뷰]JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-03-28
last_modified_at: 2023-03-28
---

[JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs]("https://arxiv.org/abs/2106.10502")  
ACL, 2021

# Problem Statement

<span style = "font-size:120%">1. Structural information loss during encoding</span>    

기존의 Transformer 모델을 기반으로 한 Pre-trained 모델들의 경우 **Fully Connected Self-Attention**을 중심으로 모델이 구성된다. FC Self-Attention의 경우 <u>하나의 노드에
대해 missing link를 고려하지 않고 모든 노드에 대해 attention</u>을 구하기 때문에 <span style = "color:gold">그래프의 구조적 정보가 무시되는</span> 문제점이 발생한다.
    
<br/>  

<span style = "font-size:120%">2. Absense of Explicit graph-text aliggnments</span>  

기존의 Text generation을 위한 Pre-Trained 모델들의 경우 **auto-encoding**이나 **auto-regressive text reconstruction**방식을 채택한다. 이 방식은 유실된 정보가 있는(또는 
Masking 처리 된) 'Corrupted Text Sequence'을 인코딩하고 디코딩 결과 'Original Sequence'가 출력된다.

한 가지 명백한 사실은, <u>Knowledge Graph가 일반적인 Text Sequence보다 더 구조적으로 복잡</u>하기 때문에 <span style = "color:gold">text reconstruction에 기반한 pre-training task를 다이렉트하게 이용하여 grap와 text를 매칭시키는 배열인 graph-text alignment를 학습시키기 난해</span>하다.

# Related Work
# Method
# Experiment & Result
# Contribution
