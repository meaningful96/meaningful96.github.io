---
title: "[논문리뷰]JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-03-28
last_modified_at: 2023-03-28
---

[JointGT: Graph-Text Representation Learning for Text Generation from Knowledge Graphs]("https://arxiv.org/abs/2106.10502")  
ACL, 2021

# Problem Statement

<span style = "font-size:120%">1. Structural information loss during encoding</span>    

기존의 Transformer 모델을 기반으로 한 Pre-trained 모델들의 경우 **Fully Connected Self-Attention**을 중심으로 모델이 구성된다. FC Self-Attention의 경우 <u>하나의 노드에
대해 missing link를 고려하지 않고 모든 노드에 대해 attention</u>을 구하기 때문에 <span style = "color:gold">**그래프의 구조적 정보가 무시되는**</span> 문제점이 발생한다.
    
<br/>  

<span style = "font-size:120%">2. Absense of Explicit graph-text aliggnments</span>  

기존의 Text generation을 위한 Pre-Trained 모델들의 경우 **auto-encoding**이나 **auto-regressive text reconstruction**방식을 채택한다. 이 방식은 유실된 정보가 있는(또는 
Masking 처리 된) 'Corrupted Text Sequence'을 인코딩하고 디코딩 결과 'Original Sequence'가 출력된다.

한 가지 명백한 사실은, <u>Knowledge Graph가 일반적인 Text Sequence보다 더 구조적으로 복잡</u>하기 때문에 <span style = "color:gold">**text reconstruction에 기반한 pre-training task를 다이렉트하게 이용하여 grap와 text를 매칭시키는 배열인 graph-text alignment를 학습시키기 난해**</span>하다.

<br/> 
<br/> 

# Related Work
<span style = "font-size:120%">1. KG-to-Text Generation</span>  

KG-to-Text는 세 가지 측면으로 나눠진다. 각각 **Encoder modification, Unsupervised Learning, Building pre-training models**이다. 
1) Encoder modification
  - Linearized Graph를 input으로 하는 Sequence Encoder의 고질적 문제인 그래프 <u>구조 정보 손실을 완화</u>하기위해 보다 더 복잡한 Encoder 모델이 필요하다.

2) Unsupervised Learning
  - graph-to-text task와 text-to-graph task를 non-parallel 한 graph-text 데이터를 이용하여 동시에 conversion하는 것을 목표로한다.
  - Unsupervised training objective to jointly learn = Joint Optimization

3) Building Pre-Training Models       
  - 기존의 모델들은 KG-to-Text 데이터셋을 text-to-text 모델에 직접적으로 fine-tuning함.
  - GPT, BART, T5
  - 이 논문에서는 직접적으로 fine-tuning하는 것이 아닌 <u>pre-training을 graph-text alighmnet를 명시적으로 학습하는데 이용</u>한다.
    
<br/> 

<span style = "font-size:120%">2. KG-Enhanced Pre-Trained Models</span>  
KG-Enhanced Pre-trainined model들이 나오게 된 계기는 Knowledge Graph를 Pre-trained 모델에 **통합**하기 위한 시도에서 출발했다. 이렇게 함으로써, <u>그래프의 Entity와 Relation의 이해(Understanding entities and relations)를 자연어를 이용해 더 용이하게 하기 위함</u>이다. 

<br/> 
<br/> 

# Method
## 1. Task Definition & Overview

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228316119-36225e91-8c04-4904-99fe-61fbba60fd5c.png">
</p>

먼저 Notation을 살펴보면 Knowlege Graph $$\mathcal{G} = (\mathcal{V,E})$$ 이다. $$\mathcal{V} = \{e_1, e_2, \cdots, e_{|\mathcal{V}|}\}$$로 엔티티(Entity) 집합을 의미하고, $$\mathcal{E} = (r_{ij})$$로 엔티티와 연결된 릴레이션(relation)을 의미한다. 당연히 i와 j는 모든 노드를 한 번씩 지칭하기에 $$r_{ij}$$의 크기는 $$ㅣ\mathcal{V}ㅣ \times ㅣ\mathcal{V}ㅣ$$이 된다. 이 때, Knowlege Graph의 정보를 Input으로 넣어주기 위해 <span style = "color:aqua">**Linearize**</span>를 하고, $$\mathcal{G}$$의 linearize를 한 수식이 $$\mathcal{G_{linear}} = (w_1, w_2, \cdots, w_m)$$이다. Linearize된 <span style = "color:aqua">$$\mathcal{G_{linear}}$$는 총 **m개**의 token</span>으로 되어 있고, 이를 통해 <span style = "color:aqua">생성되는 Text Sequence는 **n개**의 Token으로 이루어진 $$X = (x_1, x_2, \cdots, x_n)$$</span>이다. X는 이 모델 자체가 Transformer를 기반으로한
Encoder-Decoder 모델이며, 이는 **Auto-Regressive**하므로 모델의 전체 입력은 $$\mathcal{G_{linear}}$$와 $$X$$를 연속적으로 연결한 벡터가된다. 

- JoingGT 모델의 특징
  - KG의 Triple을 Linearize하고, 이를 이전 Layer의 출력값과 연결해 하나의 Input을 만든다.
  - Input Graph의 구조 정보를 보존하기위해 Structure-Aware Semantic Aggregation Module을 제시한다.
  - 그리고 3가지 새로운 Pre-Training을 제시하고 이에 접목시켜 디코더 부분을 조금씩 바꿔준다.

## 2 Model Structure

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228418591-d997ce8c-426a-4ece-9e57-aea38d9fd4e1.png">
</p>

3가지 Pre-Training Task에 대하여 공통적인 부분을 차지하는 Encoder Module을 살펴보면 크게 네 부분으로 나눠진다. 

- Encoder
  - Vainilla Self-Attention Layer
  - Pooling Layer
  - Structure-Aware Self-Attention Layer
  - Residual Layer

### 1) Vanilla Self-Attention

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228421899-2c46e6df-6343-4de7-89b3-f2d5e67bf684.png">
</p>

먼저 Vailla Self-Attention Layer를 살펴보면, 전통적인 Transformer 모델의 Self-Attention기법을 사용한다. 즉, 한 토큰이 전체 시퀀스에 있는 모든 토큰과 Attention을 통해 각각의 토큰과 얼마나 연관성이 있는지에 대한 정보를 얻게된다. 이를 통해 결론적으로 입력 시퀀스의 <span style = "color:gold">Textual Information(Contextual Semantic relations)</span>을 얻게된다. Pre-Training과정 중의 입력과 출력을 정리하면 다음과 같다. 

- Input: Sequence ($$\mathcal{G_{Linear}} ㅣㅣ X$$), lineaized graph & corresponding text sequence  
- Output: Attention Value in $$l^{th}$$-layer $$h_i^l$$, 여기서 i는 노드(Head Entity) 번호이고 l은 레이어의 층 수를 의미한다.

다시 한 번 강조하면, Vanilla Self-Attention(Fully connected self-attention)의 결과로 엔티티간 <span style = "color:gold">**풍부한 Contextual Semantic Relationship**</span>을 얻을 수 있고, 이 정보를 담고있는 Sequence가 Pooling layer의 입력으로 들어간다.

<p align="center">
<img width="300" alt="1" src="https://user-images.githubusercontent.com/111734605/228432118-6e271514-0497-4ff6-9626-6f37d26fede7.png">
</p>  
<center><span style = "font-size:80%">Vanilla Self-Attention 수식</span></center>

<br/>

### 2) Pooling Layer

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228431733-b31c369a-fb28-4fe8-8ed4-827f9b718be2.png">
</p>

Vanilla Self-Attention의 결과로 얻은 **Contextual Semantic information**에 **Structural Information**을 넣어주기 위해서 약간의 Sequence를 변형해야 한다. 이 과정을 Pooling Layer에서 수행하게 된다. Pooling은 <span style = "color:aqua">mean pooling</span>으로 하며 이 결과로 엔티티의 representation과 릴레이션의 representaion을 각각 얻게된다.

- Input: Vanilla Self-Attention의 결과로 얻은 Contextual semantic vector
- Output: Entity Representation & Relation Representation

<p align="center">
<img width="300" alt="1" src="https://user-images.githubusercontent.com/111734605/228432365-cbce93ad-c0a1-49c3-be04-f8b30427b947.png">
</p>  
<center><span style = "font-size:80%">mean pooling 수식</span></center>

<br/>

# Experiment & Result
# Contribution
