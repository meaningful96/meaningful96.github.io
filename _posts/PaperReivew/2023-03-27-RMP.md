---
title: "[논문리뷰]Relational Message Passing for Knowledge Graph Completion"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

published: true

date: 2023-03-27
last_modified_at: 2023-03-27
---

Wang, H. (2020, February 17). Relational Message Passing for *Knowledge Graph Completion. arXiv.2002.06757*    
[Relational Message Passing for Knowledge Graph Completion]("https://arxiv.org/pdf/2002.06757.pdf")  

# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230568984-7b90f5d6-62df-4644-9170-dc5bb53ae12e.png">
</p>

<span style = "font-size:120%">**Incompleteness and noisy**</span>    
Knowledge Graph는 head와 relation, tail로 이루어진 트리플(Triple, <$$h,r,t$$>)형태로 정보를 표현한다. Knowledge Graph는 보통의 Homogeneous Graph와는 다르게 엔티티(Entity, Node)나 릴레이션(Relation, Edge)이 여러 가지 타입을 가지는 Heterogeneous Graph이며 엔티티와 릴레이션의 수가 매우 많은 Large-Scale Graph이다. 이러한 일련의 이유로, <span style = "color:aqua">1)**KG는 불완전(Incomplete)할 수 있으며 noisy**</span>할 수 있다. 이는 다시 말해서, 노드 또는 엔티티의 수가 많다 보니 그래프 전반적으로 **missing link**가 많고, 그에 따라 여러 hop을 거친 path information이 불완전하기에 정보가 noisy하다는 것이다.

<span style = "font-size:120%">**Limitation of existing message passing models**</span>    
두 번째로 <span style = "color:aqua">2)**기존의 존재하는 message passing 모델들은 한계**</span>점으로 Knowledge Graph에 부적합하다는 것이다. 그 모델들은 모두 Input을 Entity의 Embedding vector로 받아 이웃 노드들의 정보를 Aggregation하고 그 메세지 정보로 hidden state를 업데이트 시키는 방식으로 학습이 진행된다. 이럴 경우 노드 수가 많은 Knowledge Graph의 경우 Computational Complexity가 압도적으로 증가하기 때문이다.

따라서, Knowledge Graph에 적당한 Message Passing 방법을 적용시킨 모델이 <span style = "color:gold">**'PathCon'**</span>이다.

<br/>
<br/>

# Related Work

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230572221-8b097544-7c92-463e-bfea-fd2a8507f3e3.png">
</p>

<span style = "font-size:120%">**Relation Prediction**</span>    
관련된 연구로는 Relation Prediction이 있다. Knowledge Graph Completion task는 쉽게 말하면 head나 tail을 찾는 엔티티 기반의 추론 문제이다. 비슷하게 Relation Prediction은 Triple에서 relation edge를 찾는 것을 목적으로 하는 추론 문제이다. 논문에서는 더 나아가, Relation Prediction을 <u>확률 분포로 주어진 head와 tail에 대한 relation type의 분포를 모델링 하는 것이라고 정의</u>한다.

<span style = "font-size:120%">**Knowledge Graph Completion(KGC)**</span>    
두번째로는 KGC이다. KGC는 위의 노란색 박스에서와 같이, head와 relation의 임베딩이 주어졌을 때 tail임베딩을 찾는 것이 목표이다. Link Prediction과 비슷하다.

<br/>
<br/>

# Method
## 1. Notation

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230575766-8d58579c-9f11-414c-89ef-809a3f87c3b3.png">
</p>

논문에서 나오는 Notation으로는 왼쪽의 표와 같다. 트리플은 <$$h,r,t$$>로 정의된다. $$s^i$$는 Hidden state 임베딩을 의미한다. 이 노테이션을 바탕으로 Relation Prediction의 내용을 보충하자면, Relation Prediction을 다음과 같이 베이즈 정리(Bayes' Theorem)로 나타내면 다음과 같다.

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/230725747-b3ca7551-ffb1-461e-87eb-3fd13de4baa6.png">
</p>

Relation Prediction은 앞서 말했듯, 헤드와 테일이 주어졌을 때 릴레이션을 찾는 것이다. 그리고 이는 <span style = "color:aqua">**릴레이션의 유형(Relation Type)에 의한 분포를 모델링**</span>하는 것과 같다. 따라서 Posterior의 경우 조건 그대로 <$$h,t$$> 가 주어졌을 때 릴레이션을 찾는 것이므로 조건부 확률(Conditional Probability)의 조건 부에 $$h,t$$가 들어간다. 하지만 , Posterior의 경우 직접적으로 모델링하기 힘들다.

따라서 베이즈 정리를 이용해 Likelihood와 Prior의 곱과 비례한다는 식으로 바꿔 모델링이 가능하다. Likelihood의 경우 조건식과 구하려던 확률식의 위치가 바뀌게 되고 이는 $$p(h,t \vert r)$$으로 표기한다. 이 Likelihood(우도)가 의미하는 것은 특정 가설이나 모델이 참인 경우 특정 데이터 또는 증거 집합을 관찰할 확률을 나타낸다. <span style = "color:aqua">**Likelihood의 의미를 좀 더 쉽게 설명하자면 결국 릴레이션 r에 관한 다른 서브 루프가 없는가를 측정해 확률로서 나타낸 것**</span>이다. 

마지막으로 Prior는 일종의 사전 지식으로 여기서는 $$h(r)$$로 표현된다. 이 때, prior의 수식은 간단하게 전체 릴레이션의 수에 대한 특정 Relation Type의 출현 확률이다. 따라서 (1)식으로 정의된다.

이를 (2)번식과 같이 Decomposition할 수 있다. Decompostion을 함으로써 직접적으로 구해야하는 확률의 무엇인지 정해진다. PathCon모델에서는 엔티티의 정보(Entitiy's Identity)를 고려하지 않고, 엔티티의 <span style = "color:aqua">**Local Relational Subgraph**</span>만을 고려한다. 다시 말해서 엔티티의 Local Relational Subgraph를 $$C(\cdot)$$으로 표현했을 때 $$p\left(C(h) \vert r \right)$$ 와  $$p\left(C(t) \vert r \right)$$로 정의된다.

<p align="center">
<img width="500" alt="1" src="https://user-images.githubusercontent.com/111734605/230727754-31e7953e-ad33-49ef-8669-67edd3ec17a7.png">
</p>

이 그림을 보면, 진한 빨간색이 head와 tail로부터 1-hop만큼 떨어진 릴레이션의 정보를 모으는 것을 시각화 한 것이고, 옅은 빨간색은 2-hop만큼의 릴레이션 정보를 Aggregation하는 것을 시각화 한 것이다. 또한 초록색선이 바로 Relation Path가 된다. Relation Path는 head에서 tail로 릴레이션을 따라서 가는 path를 말한다. 이는 Shortest path일 수도 있고, 몇 hop까지 정보를 aggregation하느냐에 따라 달라질 수 있다.

## 2. Overview of Relational Context and Path

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230727883-4c24292d-6c3b-4ba6-9b6b-9c1dc69e811e.png">
</p>

논문에서 제안한 PathCon 모델은 기존의 Relational message passing모델들과는 다르게 <span style = "color:gold">**Context뿐만 아니라 Path에 대한 정보도 같이 이용해 훈련**</span>한다.

왼쪽그림은 **Relation Context**에 관한 그림이다. Harry potter라는 노드를 중심으로 tail이 Ron일지 Hadwig인지 추론하려고 할 때, 논문에서는 1-hop만큼 떨어진 이웃 노드의 정보 뿐만 아니라 <span style = "color:aqua">**Multi-hop 떨어진 릴레이션 타입까지 정보를 모으는 방식**</span>을 제안한다. 이러한 정보는 추론 과정에서 유요한 정보로 중요한 역할을 할 수 있다. 그림에서 조지 위즐리 노드와 론위즐리 노드는 형제관계로 정의된다. 이를 통해 론과 조지는 같이 산다라고 추측할 수 있다. 따라서, Lives with라는 릴레이션에 적합한 tail은 해드위그라고 추론하는 것이 합당하다.

오른쪽 그림은 **Relation Path**의 중요성을 보여준다. Relation Path는 head에서 tail로 가는 relation들의 조합이다. 예컨대 <span style = "color:aqua">**서로 다른 Path는 엔티티들의 서로 다른 관계 정보를 주고 이 정보가 많아질수록 추론하는데 도움**</span>이 된다. 그림은 해리포터가 head이고, 헤르미온느 그레인저와 드레이코 말포이중 tail이 무엇인지를 추측하는 문제이다. 이 때 실제로 연결되어 있는 Positive relation path를 보면 해리포터에서 Wizard로 가는 릴레이션인 Occupation과 Wizard에서 후보군으로 가는 릴레이션인 Occupation이 있다. 즉 후보 노드로 가는 두 Path모두 <Occupation, Occupation>으로 동일하다. 따라서 추론에서는 도움이 되지 않는 정보이다. 

따라서 다른 Relation Path의 정보를 이용해야 한다. 그 예시가 바로 다른 Postivie relation Path인 <house, house>이다. 헤르미온느 그레인저와 <house, house> path를 따라가면 연결이 되므로 tail은 헤르미온느 그레인저라고 추론할 수 있다.

## 3. Pros of Relational Message Passing

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230729033-e0d850ab-42ee-4a48-9bda-9170347ba4be.png">
</p>

기존의 방식인 Node-to-Node Message Passing 대신에 Relational Message Passing 방식을 사용하는 이유는 여러가지가 있다.

먼저 <span style = "color:gold">**Inductive Learning**</span>이 가능하다. <u>추론 과정에서는 절대 나오지 않는 엔티티들을 사용해 학습에 이용</u>하기 때문이다. 이는, 이웃 노드들 뿐만 아니라, Multi-hop에 대한 릴레이션의 정보를 받기 때문에 실제로는 여러 노드를 거쳐서 정보를 모으는 것이며, 이에 따른 Relation Path도 다양해져 실제로는 Shortest Path가 아닌 Path에 대해서도 정보를 모아오기 때문에 추론 과정에서는 보이지 않는 정보도 학습에 반영이된다. 따라서, 추론에서 나오는 정보 외에도 사용하므로 Inductive하다.

두번째 이유는 <span style = "color:gold">**Storage-Efficient**</span>이다. 아무래도 KG에서 엔티티의 수가 Relation Type보다는 많기 때문에 기존의 node-to-node Message Passing 방식은 Computational Complexity가 매우 크다. 하지만, <u>Relational Message Passing은 릴레이션에 의존하므로 엔티티 임베딩을 계산하지 않아 시간 복잡도도 감소하고 이에 따라 메모리 측면에서 저장 효율성도 좋다.</u>

마지막으로 <span style = "color:gold">**Explainable**</span>하다는 것이다. 추론한 결과를 <u>Relational Path를 이용하여 쉽게 설명</u>을 할 수 있다.

## 4. PathCon Model

### 1) Definition

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230729622-4500316c-9c3d-49a2-aa75-ddd0afe99e03.png">
</p>

노드를 이용한 기존의 Message Passing은 $$i$$번째 Layer의 노드의 임베딩 정보인 Hidden state 임베딩을 Aggregation 함수를 이용해 메세지 정보를 만든다. 이 만들어진 정보를 Hidden state 임베딩을 업데이트 함수를 통해 다음 Layer의 Hidden state 임베딩을 생성한다. 이 때 수식에서도 알 수 있듯 $$s_u^i$$이므로 노드 기반의 임베딩인 것을 알 수 있다.
노드 베이스 방식은 N개의 엔티티와 M의 릴레이션에 대해 Cost가 **2M + 2N**이 된다. 

반면, Relational Message Passing은 Hidden state 임베딩이 $$s_{e^'}^i$$로 정의되며 릴레이션 기반의 임베딩 정보이다. 이 때 앞서 말했듯, Relational Message Passing은 릴레이션의 분포(Variance)에 대한 수식으로 정의되므로, 이 임베딩 정보도 분포에 대한 식이다. 하지만, 실제로 <span style = "color:aqua">**Knowledge Graph에서는 Long-tail때문에 매우 크므로 Cost가 훨씬 더 크게**</span> 나온다. 이럴 경우 Relational Message Passing방식이 적합한지 의문이 생기게 된다. 

이런 이유로 논문에서는 이를 해결하기 위한 방식으로 alternative 방식을 제안했다. 이 방식은 쉽게 말해서 <span style = "color:gold">**릴레이션의 aggregation을 나눠서 하는 것**</span>이다. 7번식에서 Hidden state의 정보를 취합한다. 7번식은 앞서 언급했던 Relational Message Passing의 특징인 Relation Path의 end-point인 head와 tail의 이웃 노드들로 뻗어나가는 릴레이션의 정보를 취합한다. 즉, <span style = "color:gold">**7번식은 Context 정보 취합한 정보**</span>이다. 

Context 정보를 모았으면 이제 Relational Path정보를 모아야 한다. 이렇게 End Point에 대한 이웃 노드들의 릴레이션 정보를 모았으므로 이를 토대로 Path를 구성하여 Aggregation function을 통해 취합한다. 즉, <span style = "color:gold">**Multi-hop의 릴레이션 정보를 기반으로 Path를 구성하고 그 정보를 취합한 것이 8번식**</span>이다. 마지막으로 이를 통해 업데이트 함수를 이용해 Hidden state를 업데이트한다.

중요한 <u>사실은 노드는 분포의 중심 역할(End-Point)을 하는 것이고 메세지 정보를 임시로 저장</u>하는 역할을 한다고 논문에서 언급한다. 이를 통해 알 수 있는 것은 <span style = "color:gold">**Alternative Relational Message Passing**</span>의 특징인데 일반적인 Relational Message Passing과는 다른점을 말해준다. Alternative방식은 Edge ➜ Edge 로의 정보 전달이 아닌, <span style = "color:gold">**Node ➜ Edge 로 정보를 보내는 Cross Passing방식**</span>이다. 이를 통해 Cost가 굉장히 완화되는 것을 볼 수 있다. 이 방식을 통해 모델을 <span style = "color:gold">**Cross Context Aggregator**</span>라고 한다.

### 2) Relational Context 정의

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230731445-149c5cd7-87cb-46a8-bced-fa03eb43f19a.png">
</p>

**Alternative Relational Message Passing**의 전반적인 과정을 앞서 살펴보았고, 따라서 Context와 Path를 수학적으로 명확하게 정의해야 한다. 

Konwledge Graph의 Triple에서 head와 tail은 대체적으로 매우 높은 연관성을 보인다. 에를 들어 릴레이션이 graduated from 일 때 head가 출생지와 성별에 관한 릴레이션으로 둘러싸여 있고, tail이 학교 위치, 창립자, 교장에 관한 릴레이션으로 둘러싸여 있다면, 쉽게 head와 tail의 관계성에 r이 들어갈 수 있다라고 추측할 수 있다.

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/230732297-a251d06a-c2bf-4c19-94d6-e303858a3872.png">
</p>

이처럼 노드 v에 대한 메세지를 이웃 노드와 Multi-hop 정보를 모두 취합한 것으로 정의한다. 그 정의된 정보를 토대로 v노드에 대한 메세지 정보와 u노드에 대한 메세지 정보, Hidden State 임베딩을 <span style = "color:aqua">**Concatenation하고 이를 Weight와 곱한 후 bias를 더해준 후 Nonliearity를 먹인 것**</span>으로 정의한다. 

### 3) Relational Path 정의

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/230732399-d227e0a8-c32d-412e-9de4-f4bcd5625474.png">
</p>



<br/>
<br/>

# Experiment & Result


<br/>
<br/>

# Contribution
