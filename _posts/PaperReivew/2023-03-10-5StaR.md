---
title: "[논문리뷰]Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-03-10
last_modified_at: 2023-03-10
---

Wang, B., Shen, T., Long, G., Zhou, T., Wang, Y., & Chang, Y. (2021). Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion. In arXiv (Cornell University). Cornell University. *arXiv:2004.14781* 


# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/233827606-36d2b05b-ba40-43c2-8490-05efa9a5bf0e.png">
</p>

논문에서는 먼저 기존 **그래프 임베딩(Graph Embedding)** 모델들의 문제점을 지적한다. 기존 모델들은 엔티티와 릴레이션의 상대적인 <span style = "color:aqua">**Spatial Distance**</span>를 이용하여 **Triple-Level Relationship**을 포착한다. 하지만 이러한 방식은 추론시 <span style = "color:aqua">**Training 중에 한 번도 방문하지 않은 element를 일반화하기 어렵다**</span>. 또한 상대적인 거리를 Scoring fucntion으로 이용하므로, <span style = "color:aqua">**Graph Incompleteness에 취약**</span>하다.

다음으로는 기존에 존재하던 <b>사전 학습된 인코더(Pre-Trained Encoder)</b>에 대한 한계점을 지적한다. Transformer 기반의 Pre-trained model의 가장 큰 특징은 **Self-Attention mechanism**을 이용한다는 것이다. 이를 Knowledge Graph에 적용하면 <span style = "color:aqua">**모든 Vertax에 대하여 어텐션 스코어를 구하므로 매우 많은 자원을 필요**</span>로 한다. 또한, 모든 노드에 대해 어텐션을 진행하면, <span style = "color:aqua">**그래프의 구조 정보(Structural Information)가 제대로 반영되지 않는다**</span>는 한계가 있다. 



<br/>
<br/>

# Related Work

<span style = "font-size:120%">**Knowledge graph completion**</span>  

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/233843357-5215de7f-1fe6-4afb-8a82-eb2e99cf40f3.png">
</p>

**Knowledge Graph Completion**은 기본적으로 아이디어 자체는 Link prediction과 유사하다. 다만, Knowledge Graph와 일반적인 Graph에서 차별점을 주기위해서 KG에서 노드를 **엔티티(Entity)**, Edge를 **릴레이션(Relation)** 부르며 Triple이라는 것을 정의한다. Triple은 <head, relation, tail>로 구성된다. 여기서 head와 tail은 각각 end-point가 되는 엔티티를 말하며, 그 둘 사이의 관계성을 보여주는 것이 Relation(Edge)가 된다. 

Knowledge Graph Completion은 <span style = "color:gold">**head와 realation, 또는 relation과 tail이 주어졌을 때 Triple에서 나머지 한 End-point 엔티티를 찾는**</span> Task이다.

<br/>

<span style = "font-size:120%">**Graph Embedding**</span>  

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234193074-e0b98e2f-3b8b-4d75-979e-64e92b9e0365.png">
</p>

그래프의 엔티티와 릴레이션의 <span style = "color:gold">**구조 정보를 저차원 벡터의 representation으로 학습**</span>을 목표로 한다. 트리플의 공간적인 관계성을 정의하여 직접적으로 이용하는 방식이다. 이 그래프 임베딩 접급법은 두 가지 방식으로 나눠진다. 

먼저 **Translation-based approach**이다. <head, relation>의 임베딩을 Translation function에 적용해 Triple의 신뢰성을 점수화(Scoring)한다. 대표적인 모델로는 TransE와 TransR이 있다.

반면 **Semantic matching approach** 방식은 Triple에 직접 작용하는 matching function을 통해 Triple의 신뢰성을 도출한다. 대표적인 모델은 DistMult와 QuatE가 있다. 

그래프 임베딩의 경우는 그래프의<span style = "color:gold">**구조 정보**</span>를 잘 반영하지만 치명적인 단점이 존재한다. 첫 번째로 학습 중 <u>Unseen entity/relation에 부적합하다는 것</u>이다. 또한 구조 정보를 이용하므로 <u>Graph Incompleteness</u>에도 매우 취약하다.

<br/>

<span style = "font-size:120%">**Texture Encoding**</span>  

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234210237-78a05ef0-8d10-41f2-b18d-02711a02dd22.png">
</p>

텍스쳐 인코딩은 Triple의 <span style = "color:gold">**자연어 텍스트의 Contextualized representaion을 이용해 KGC의 missing link를 예측**</span>하는 것이다. 텍스트는 엔티티와 릴레이션의 텍스쳐 내용을 참조할 수 있다. 대표적인 모델로는 KG-BERT가 있다. KG-BERT는 word2vec tokenizer를 통해 변환된 Triple sequence를 입력으로 받는다. KG-BERT처럼 Pre-trained된 모델을 이용하면 결론적으로 <u>Attention을 이용하는 것이고 따라서 모든 Triple Set에 대한 Importance를 구하는 과정이 들어가므로 <b>Unseen graph</b> element를 쉽게 generaliztion</u>할 수 있다. 또한 Global하게 정보를 이용하므로 <u>Graph Incompleteness</u>에도 더 좋은 성능을 보인다.

하지만, Link prediction시 모든 엔티티를 비교해야 하므로 추론시 cost가 많이 든다. 또한 모든 노드들의 Importance를 구하는 것은 Global information을 반영할 지언정, Locality를 고려하지 않으므로 그래프의 구조 정보를 제대로 반영하지 못한다는 단점이 있다.

<br/>

<span style = "font-size:120%">**Pre-Trained MLM**</span>  

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234210312-07bf799b-1893-46e0-8976-e2da19b23331.png">
</p>

MLM은 **Masked Langauge Model**의 약자이다. Input sequence의 토큰들을 랜덤하게 마스킹해서 pre-training을 진행하는 방식이다. 이는 더 강력하고 성능이 좋은 Texture Encoding을 얻기 위함이다. 매우 크기가 큰 raw Corpora로 사전 학습된 MLM모델은 generic한 contextualized representation을 <u>self-supervised learning</u>을 통해 학습하게 된다. 대표적인 모델로는 BERT와 RoBERa가 있다. BERT처럼 이름에서도 알 수 있듯이 <u>랜덤하게 마스킹된 토큰들을 Bidirectional하게 양방향 모두의 문맥을 고려해 예측</u>한다. 

<br/>

<span style = "font-size:120%">**KG-BERT**</span>  

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234210375-42e7546f-652a-4485-b5f7-36df086ac4f7.png">
</p>

KG-BERT는 대표적인 Texture Encoding 모델이며 MLM 모델이다. Triple-level에서 Contexturalized representation을 scoring한다. 먼저, word2vec tokenizer를 이용해 Text를 Word Embedding Sequence로 변환하고 MLM과 마찬가지로 [CLS] 토큰과 [SEP]과 함께 사용해 Concatenation하고 새로운 시퀀스를 만든다. 그 후 Transformer의 인코더를 거치면 NSP과 MLM task에 대한 pre-training이 진행된다. 위에서 Pool은 Triple이 맞는지 아닌지를 판단하는 일종의 NSP task의 결과를 나타내주는 함수이며, 이는 [CLS] 토큰에의해 결정된다.


<br/>
<br/>

# Method
## 1. Overview

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234280335-fb0e9200-bef9-43d7-ab33-356b5d040d91.png">
</p>

## 2. Structure-Aware Triple Encoding

### 1) Siamese-Style Encoder

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234283377-299b54b3-d99e-42c7-9bd7-3bc38c472314.png">
</p>

샴 네트워크(Siamese Network) 비슷한 구조의 네트워크 두 개를 병렬로 놓고 파라미터를 공유(Parameter Sharing)해서 학습하는 방식이다. StAR모델에서는 이 샴 네트워크의 아이디어를 차용해 새로운 <span style = "color:gold">**Siamese-Style Encoder**</span>를 제안한다. Query와 Candidate를 각각 인코딩하여 Pairwise된 input을 bypass한다. StAR 모델이 이러한 Siamese-Style Encoder를 사용한 이유는 <u>Combinatorial explosion을 피하기 위함</u>이다.

하지만, 이 방식을 채택했을 때는 여러가지 의문점이 발생한다. 세 가지 의문점은 다음과 같다. (1)어떻게 하면 엔티티와 릴레이션에 걸친 **Contextualized Knowledge를 보존**할 수 있는가? (2)**Siamese architecture를 어떻게 Triple에 적용**할 것인가? (3)Downstream model에서 어떻게 **구조 학습(Structure Learning)을 촉진**할 수 있는가?   

<br/>

### 2) Using Translation-based graph embedding

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/234340114-c8d1383d-bb71-4b8d-a9b3-d71847e5efb0.png">
</p>

StAR의 모델 아키텍쳐를 좀 더 보기전에 다시 한번 Translation-based 모델을 리뷰하며 TransE모델과 RotatE모델을 조금 자세히 다뤄보면 다음과 같다. 


<br>

### 3) Structure-aware triple encoding 구조



<br/>
<br/>

# Experiment

<br/>
<br/>

# Contribution
