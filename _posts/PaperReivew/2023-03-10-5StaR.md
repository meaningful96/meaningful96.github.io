---
title: "[논문리뷰]Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-03-10
last_modified_at: 2023-03-10
---

Wang, B., Shen, T., Long, G., Zhou, T., Wang, Y., & Chang, Y. (2021). Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion. In arXiv (Cornell University). Cornell University. *arXiv:2004.14781* 


# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/233827606-36d2b05b-ba40-43c2-8490-05efa9a5bf0e.png">
</p>

논문에서는 먼저 기존 **그래프 임베딩(Graph Embedding)** 모델들의 문제점을 지적한다. 기존 모델들은 엔티티와 릴레이션의 상대적인 <span style = "color:aqua">**Spatial Distance**</span>를 이용하여 **Triple-Level Relationship**을 포착한다. 하지만 이러한 방식은 추론시 <span style = "color:aqua">**Training 중에 한 번도 방문하지 않은 element를 일반화하기 어렵다**</span>. 또한 상대적인 거리를 Scoring fucntion으로 이용하므로, <span style = "color:aqua">**Graph Incompleteness에 취약**</span>하다.

다음으로는 기존에 존재하던 <b>사전 학습된 인코더(Pre-Trained Encoder)</b>에 대한 한계점을 지적한다. Transformer 기반의 Pre-trained model의 가장 큰 특징은 **Self-Attention mechanism**을 이용한다는 것이다. 이를 Knowledge Graph에 적용하면 <span style = "color:aqua">**모든 Vertax에 대하여 어텐션 스코어를 구하므로 매우 많은 자원을 필요**</span>로 한다. 또한, 모든 노드에 대해 어텐션을 진행하면, <span style = "color:aqua">**그래프의 구조 정보(Structural Information)가 제대로 반영되지 않는다**</span>는 한계가 있다. 



<br/>
<br/>

# Related Work

<br/>
<br/>

# Method

<br/>
<br/>

# Experiment

<br/>
<br/>

# Contribution
