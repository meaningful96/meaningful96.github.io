---
title: "[논문리뷰]Relation-Aware Language-Graph Transformer for Question Answering"

categories: 
  - GR
  
toc: true
toc_sticky: true

date: 2024-03-14
last_modified_at: 2024-03-14
---

Authors: *Jinyoung Park, Hyeong Kyu Choi, Juyeon Ko, Hyeonjin Park, Ji-Hoon Kim, Jisu Jeong, Kyungmin Kim, Hyunwoo J. Kim*  
Paper: *[Relation-Aware Language-Graph Transformer for Question Answering](https://arxiv.org/abs/2212.00975) in AAAI 2023*.    

# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Model_Experiment/assets/111734605/ea4f11ee-e96e-4386-91eb-6607a92c323d">
</p>


<span style = "font-size:110%"><b>1. 기존 GNN 기반 모델들의 한계</b></span>  
Question-Answering(QA)을 위한 기존의 GNN 기반의 모듈은 트리플(Triple)로 이루어진 Knowledge Graph(KG)의 풍부한 정보를 제대로 활용하지 못했다. GNN은 기본적으로 노드의 임베딩을 학습한다. 다시 말해, 노드의 정보만을 학습하며 두 노드 사이의 연결 유무만을 학습하게 된다. 하지만, KG에는 많은 릴레이션에 자연어 정보가 포함되어 있고, 그 종류가 매우 많다. 따라서 GNN을 통한 KG학습에는 한계가 존재한다.

<span style = "font-size:110%"><b>2. Langauge Model(LM)과 Knowledge Graph(KG) 사이 매우 적은 정보만을 교환</b></span>  
기존 모델들 중 LM과 KG 사이 매우 적은 정보 교환만 이루어졌다. 예를 들어, LM모델을 학습하고 추론 시에만 KG를 처리하여 학습된 LM에 통합시키려 하였다. 이후 연구에서는 학습 단계에서 KG와 LM을 합치려는 시도가 있었으며, 이는 special token node나 cross-attention을 통해 이루어졌다. 하지만, 이런 접근 방식은 GNN을 modality-specific하게 하며, KG모듈과 LM모듈 간의 정보 교환이 연산을 진행하는 **fusion부분에서만 이루어지기에** 여전히 제한적으로 일어난다는 한계가 있다.

<br/>

# Related Work

<br/>

# Method

<br/>

# Experiments & Results

<br/>

# Contribution
