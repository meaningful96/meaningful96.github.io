---
title: "[논문리뷰]Relation-Aware Language-Graph Transformer for Question Answering"

categories: 
  - GR
  
toc: true
toc_sticky: true

date: 2024-03-14
last_modified_at: 2024-03-14
---

Authors: *Jinyoung Park, Hyeong Kyu Choi, Juyeon Ko, Hyeonjin Park, Ji-Hoon Kim, Jisu Jeong, Kyungmin Kim, Hyunwoo J. Kim*  
Paper: *[Relation-Aware Language-Graph Transformer for Question Answering](https://arxiv.org/abs/2212.00975) in AAAI 2023*.    

# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/ecae1066-36e2-4aa9-9253-de12734ec39f">
</p>

<span style = "font-size:110%"><b>1. 기존 GNN 기반 모델들의 한계</b></span>  
Question-Answering(QA)을 위한 기존의 GNN 기반의 모듈은 트리플(Triple)로 이루어진 Knowledge Graph(KG)의 풍부한 정보를 제대로 활용하지 못했다. GNN은 기본적으로 노드의 임베딩을 학습한다. 다시 말해, 노드의 정보만을 학습하며 두 노드 사이의 연결 유무만을 학습하게 된다. 하지만, KG에는 많은 릴레이션에 자연어 정보가 포함되어 있고, 그 종류가 매우 많다. 따라서 GNN을 통한 KG학습에는 한계가 존재한다.

<span style = "font-size:110%"><b>2. Langauge Model(LM)과 Knowledge Graph(KG) 사이 매우 적은 정보만을 교환</b></span>  
기존 모델들 중 LM과 KG 사이 매우 적은 정보 교환만 이루어졌다. 예를 들어, LM모델을 학습하고 추론 시에만 KG를 처리하여 학습된 LM에 통합시키려 하였다. 이후 연구에서는 학습 단계에서 KG와 LM을 합치려는 시도가 있었으며, 이는 special token node나 cross-attention을 통해 이루어졌다. 하지만, 이런 접근 방식은 GNN을 modality-specific하게 하며, KG모듈과 LM모듈 간의 정보 교환이 연산을 진행하는 **fusion부분에서만 이루어지기에** 여전히 제한적으로 일어난다는 한계가 있다.

<br/>

# Related Work

<span style="font-size:110%"><b>1. Knowledge Graph Question-Answering (KGQA)</b></span>    
KG에서 경로 상에 존재하는 정보를 취합하여 하나의 질문(question)에 대한 정답(answering)을 찾아내는 문제로, Knowledge Base Question-Answering(KBQA)로도 불린다. 초기에는 GNN만을 사용한 연구가 대부분이었다. GNN은 기본적으로 message passing과 aggregation을 통해 이웃 노드(엔티티)의 정보를 취합하여 구조 정보를 학습한다. 이후 Knowledge Graph Completion(KGC)분야에도 LM 모델을 활용한 연구가 진행되면서 LM과 KG를 모두 활용하는 multi-modal형태의 연구가 진행되었다.

<p align="center">
  <img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/0d11c9ea-72f3-4b5a-be9e-8f58aef562a3">
  <center><figcaption>ref: Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals</figcaption></center>
</p>

위의 그림은 KGQA의 예시이다. 질문으로 "Q) *What types are the film starred by actors in the nine lives of fritz the cat?*" 주어졌을 때, 정답을 올바르게 찾아가는 경로는 빨간색이다. 하지만 Question-answering이 어려운 대표적인 이유 중 하나가 바로 정답을 맞추더라도 올바른 경로로 가지 않는 경우가 발생하기 때문이다. 그림에서 파란색 경로를 보면, 질문과는 거리가 먼 추론을 하지만 정답인 *Comedy*를 맞춘 것을 확인할 수 있다. 이러한 이유 때문에 <span style="color:gold">**Hallucination**</span>이 발생하는 것이다. Hallucination이란 예를 들면, "아인슈타인이 중력을 발견한 연도가 언제야?" 라고 자연어 모델에게 질문하였을 때, 잘못된 질문임에도 불구하고 "아인슈타인은 중력을 1925년에 발견하고 논문을 발표했어."와 같이 잘못된 답변을 그럴듯하게 생성해내는 문제이다. 이러한 이유로, QA를 올바르게 할 수 있도록 모델을 학습하는 것이 중요하다.

<span style="font-size:110%"><b>2. Question-Answering for Graphs</b></span>    
QA task의 목표는 주어진 질문의 context를 자연어와 구조화된 관계 정보로 이해하는 것에 초점을 맞춘다. SapBERT와 ConceptNet등이 이 연구에 해당한다.

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/ba5ceeb7-3c3a-439a-97f1-fc6425e3cd05">
</p>

Multiple-Choice Question-Answering (MCQA)는 question의 $$q$$의 단어 엔티티와 선택한 답변 $$a \in C$$ 엔티티를 Concatenation하여 시퀀스 $$X$$로 정의하고 이를 입력으로 넣는다. 그리고 사전 학습된 자연어 모델 $$\mathcal{g_{\text{LM}}}$$이 입력 시퀀스 $$X$$를 받아 출력된 값이 바로 Context Token $$\mathcal{H_{\text{LM}}}$$이 된다. 

<span style="font-size:110%"><b>3. Meta-Path</b></span>  

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/1aab8b5e-7410-48b7-b89a-0b55b84204a0">
</p>

Meta-path란 여러 타입의 릴레이션을 가지는 knoweldge graph에서 특정 엔티티와 엔티티 사이의 경로를 표현한 것이다. $$v$$를 엔티티, $$r$$을 릴레이션으로 정의할 때 meta-path는 위와 같이 정의된다. 본 논문에서는 mata-path상의 트리플을 모델의 입력으로 넣는다.

<br/>

# Method

## 1. Overview

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/762058ef-a39d-4871-9c6b-280eb8c0d839">
</p>

- Input
  - Question-Answer Choice Pair
  - Meta-path token(= subgraph edges and paths)

이 논문의 architecture는 위와 같다. <span style = "color:gold">**Question-Answer choice pair**</span>와 <span style="color:gold">**Subgraph edges and paths(meta-path)**</span>을 입력으로 받는다. 두 입력은 각각 language model(LM)과 MLP layer로 들어가 임베딩된다. 이후 논문에서 제안한 modality embedding과 각각 더해진 후 concatenation되어 Relation-Aware Self-Attention(RASA) 모듈에 입력으로 들어가게된다. 이 때, modality embedding은 학습 가능한 임베딩이다. 논문에서는 이 **Question Answering Transformer(QAT)**를 통해 LM과 KG를 하나로 통합하여 공동 추론(jointly reasoning)을 수행한다.

## 2. Meta-Path Token Embedding

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/88722bcc-caa9-4815-9757-eabb915d1ccd">
</p>

Question answering이란 엔티티 사이의 관계(relationship)을 이해하는 것이 핵심이다. QAT는 각 엔티티 쌍의 관계를 나타내는 임베딩을 학습한다. 이러한 임베딩을 만들어 내기 위해 **Meta-Path**를 정의하며 이 path상에 존재하는 triple들의 자연어 토큰을 나열한 것이 meta-path token이라 한다. 논문에서는 서브그래프(subgraph)를 추출함에 있어서, path상에 존재하는 엔티티와 릴레이션들을 서브그래프로 정의한다. 이 때, <u>엔티티의 타입을 4가지로 분류하며 이는 입력으로 미리 주어진다. 주의할 점은, 엔티티 feature를 직접적으로 임베딩을 학습하는데 이용하지 않는다</u>는 점이다. (The node types are also provided as input, but the node features are not directly employed when learning the embeddings)

<p align="center">
  <img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/a7744e66-a763-4371-bb71-cf9437b93d79">
  <figcaption>A node type among {<b>Z, Q, A, O</b>} is selected based on the node's source. To elaborate, <b>Z</b> is a node which is deliberately inserted to connect all question and answer entities for context aggregation (similar to the cls token in Transformers), <b>Q</b> and <b>A</b> each denote the entities from the question and answer choice, respectively. <b>O</b> is the “other” type of node that does not fall in the three categories, but constitutes the extracted subgraph.</figcaption>
</p>

그래프 임베딩의 이전 연구들은 여러 기법이 있으며 특히 트리플의 관계를 **translation function**으로 정의하는 방법이 있다. Translation(번역)이란 지식 그래프의 엔터티 간 관계에 대한 기하학적 해석을 의미한다. 예를 들어 TransE는 $$h + r \approx t$$라는 관계를 정의했다. 즉, 'h와 r의 임베딩 **벡터 합**이 t의 임베딩이다.'라는 관계를 정의한 것이다. 본 논문에서도 이와 유사하게 <span style="color:gold"><b>feature translation</b></span>을 정의한다. 

<p align="center">
<img width="900" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/2c696d15-63bd-40ca-a094-89046ae1a347">
</p>

Feature translation은 $$\delta_{h, t}$$로 표기하며, tail의 feature에서 head의 feature를 뺀 것으로 표현한다. 최종적으로 feature translation은 위와 같이 $$\delta_{h, t} = f_t - f_h$$로 표기되며 $$f_t$$와 $$f_h$$는 각각 tail과 head의 node feature이다. 이렇게 트리플에서 두 엔티티의 관계가 정의되면 릴레이션의 임베딩을 계산할 수 있다. 이는 하나의 트리플에서 두 엔티티가 주어졌을 때의 릴레이션 임베딩이며, $$h_{(h, r, t)} \in \mathcal{E}$$로 표기한다. 이 식에서 $$g_{\theta_1}(\cdot)$$은 MLP이며, $$\phi(\cdot)$$ $$v$$의 노드 타입에 따른 원핫 인코더이다. 한 서브그래프에서 노드 타입은 위에서와 같이 4개가 정의되었으므로, $$\phi(\cdot)$$ 인코더를 통과한 원핫 결과는 4개 중 하나가 된다. 

또한 임베딩의 다양성을 위해 multi-hop path를 구성하며 각 path를 $$\psi$$로 표시한다. KGQA의 핵심은 여러 트리플을 거쳐 하나의 질문을 추론하는 것이다. 따라서 당연히 one-hop path을 각각 학습하는 것보다는 <u>multi-hop path를 학습하는 것이 합리적</u>이다. 최종적으로 Meta-path token은 다음과 같이 표현된다.

<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/00a01b86-fc41-43ed-b5be-11d07d099b27">
</p>

QAT와 달리 이전 연구들은 knowledge graph를 인코딩하기 위해 GNN을 사용하였다. 일반적으로 message passing은 메커니즘을 활용하여 이웃 엔티티의 feature와 릴레이션의 feature를 한 layer에 전파(passing)함으로써 이웃 정보를 취합(aggregation)한다. 그리고 최종적으로 엔티티들은 예측을 위해 풀링(pooling)된다. 엔티티(노드)가 결과적으로 릴레이션 정보를 포함하지만, message passing의 목적은 릴레이션이 아닌 엔티티의 feature를 학습하는 것이므로 GNN방식은 <span style="color:coral"><b><i>node-centric</i></b></span>하다. 반면 QAT는 릴레이션 정보를 meata-path token을 통해 명시적으로 임코딩하여 이를 공동 추론(jointly reasoning)에 이용하기에 모델이 서브그래프내의 구조 및 의미적 관계를 포착할 수 있다. 따라서 QAT는 <span style = "color:gold"><b><i>relation-centric</i></b></span>하다.

<br/>

# Experiments & Results

<br/>

# Contribution
