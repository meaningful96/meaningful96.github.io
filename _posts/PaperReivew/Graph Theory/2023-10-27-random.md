---
title: "[논문리뷰]Structure Aware Negative Sampling in Knowledge Graphs"

categories: 
  - GR
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-10-27
last_modified_at: 2023-10-27
---

*Ahrabian, K., Feizi, A., Salehi, Y., Hamilton, W. L., & amp; Bose, A. J. (2020). Structure aware negative sampling in knowledge graphs. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). [https://doi.org/10.18653/v1/2020.emnlp-main.492 ](https://arxiv.org/pdf/2009.11355.pdf)* 

# Problem Statement(Abstract)
최근 자연어 처리(NLP)분야와 컴퓨터 비젼 분야를 기점으로, 많은 인공지능 분야에서 Contrastive learning 방식을 사용한다. Contrastive learning의 중요한 측면 중 하나는 hard negative sampling을 생성하는 손상 분포(corruption distribution)의 선택이다. 이는 임베딩 모델이 차별적 표현(discriminative representation)을 학습하고 관찰된 데이터의 중요한 특성을 찾도록 강요(**forcing**)하는 역할을 한다.

하지만 기존의 연구들에서는 negative sampling을 하는 일반적인 방법으로 corruption distribution을 단순하게 **uniform distribution**으로 간주한다. 이는 결과적으로 Knowledge Graph에 부적합하다. 명시적인 통합(의미적인 통합, semnatically incorporate)을 이루지 못한다. <u>Uniform하고 고정된 sampling 방식은 학습 중 쉽게 분류 가능한 negative triple을 생성하며, 이는 유의미한 정보를 모델에 전혀 제공하지 못한다.</u> 이와 같은 이유로 <span style = "color:aqua"><b>'계산 비용(Computational cost)가 크지 않으며, 좀 더 그래프 구조를 반영할 수 있는 방법이 없는가?'</b></span>라는 open question을 야기한다.

<br/>
<br/>

# Related Work
## 1. Contrastive learning in Knowledge Graph
Knowledge Graph(KG)는 낮은 차원의 벡터 공간으로 인코딩하는 그래프 인베딩 기술을 활용한 방법이 급증하고 있으며, 이는 데이터 조작(data manipulation)을 용이하게 하는 동시에 데이터 희소성과 불완전성(sparsity & incompleteness)을 다루는 프레임워크이다. Contrastive estimation을 활용하여 KG 임베딩을 학습시키는 것ㅇ느 모델이 최적화하기 위해 관측된 positive triplet에 대한 energy, 영향력를 올리는 동시에 negative triple에 대한 energy를 낮추는 과정을 포함한다. 결과적으로, negative sampling 분포의 선택은 Energy landscape를 형성하는 데 중요한 역할을 한다. 기존의 연구들을 간단하게 random sampling과 같은 방식으로 energy landscape를 구성한 것이다.

## Self-adversarial negative sampling
이 방식이 처음 Knowledge Graph에 적용된 것은 [Rotate](https://arxiv.org/pdf/1902.10197.pdf)모델이다. 간단하게 말하면 모든 negative sample의 score에 대해 weight를 부여하는 방법이다.

<br/>
<br/>

# Method
## 1. Overview
