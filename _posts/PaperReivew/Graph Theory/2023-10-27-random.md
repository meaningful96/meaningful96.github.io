---
title: "[논문리뷰]Structure Aware Negative Sampling in Knowledge Graphs"

categories: 
  - GR
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-10-27
last_modified_at: 2023-10-27
---

*Ahrabian, K., Feizi, A., Salehi, Y., Hamilton, W. L., & amp; Bose, A. J. (2020). Structure aware negative sampling in knowledge graphs. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). [https://doi.org/10.18653/v1/2020.emnlp-main.492 ](https://arxiv.org/pdf/2009.11355.pdf)* 

# Problem Statement(Abstract)
최근 자연어 처리(NLP)분야와 컴퓨터 비젼 분야를 기점으로, 많은 인공지능 분야에서 Contrastive learning 방식을 사용한다. Contrastive learning의 중요한 측면 중 하나는 hard negative sampling을 생성하는 손상 분포(corruption distribution)의 선택이다. 이는 임베딩 모델이 차별적 표현(discriminative representation)을 학습하고 관찰된 데이터의 중요한 특성을 찾도록 강요(**forcing**)하는 역할을 한다.

하지만 기존의 연구들에서는 negative sampling을 하는 일반적인 방법으로 corruption distribution을 단순하게 **uniform distribution**으로 간주한다. 이는 결과적으로 Knowledge Graph에 부적합하다. 명시적인 통합(의미적인 통합, semnatically incorporate)을 이루지 못한다. <b>Uniform하고 고정된 sampling 방식은 학습 중 쉽게 분류 가능한 negative triple을 생성하며, 이는 유의미한 정보를 모델에 전혀 제공하지 못한다.</b> 이와 같은 이유로 <span style = "color:gold"><b>'계산 비용(Computational cost)가 크지 않으며, 좀 더 그래프 구조를 반영할 수 있는 방법이 없는가?'</b></span>라는 open question을 야기한다.

<br/>
<br/>

# Related Work
## 1. Contrastive learning in Knowledge Graph
Knowledge Graph(KG)는 낮은 차원의 벡터 공간으로 인코딩하는 그래프 인베딩 기술을 활용한 방법이 급증하고 있으며, 이는 데이터 조작(data manipulation)을 용이하게 하는 동시에 데이터 희소성과 불완전성(sparsity & incompleteness)을 다루는 프레임워크이다. Contrastive estimation을 활용하여 KG 임베딩을 학습시키는 것ㅇ느 모델이 최적화하기 위해 관측된 positive triplet에 대한 energy, 영향력를 올리는 동시에 negative triple에 대한 energy를 낮추는 과정을 포함한다. 결과적으로, negative sampling 분포의 선택은 Energy landscape를 형성하는 데 중요한 역할을 한다. 기존의 연구들을 간단하게 random sampling과 같은 방식으로 energy landscape를 구성한 것이다.

## Self-adversarial negative sampling
이 방식이 처음 Knowledge Graph에 적용된 것은 [Rotate](https://arxiv.org/pdf/1902.10197.pdf)모델이다. 간단하게 말하면 모든 negative sample의 score에 대해 weight를 부여하는 방법이다.

<br/>
<br/>

# Method
## 1. Overview

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/47fc7e44-edca-435a-af54-43361ba9e661">
</p>

이 논문에서는 negative sampling distribution으로 uniform distribution을 사용하지 않는다. 논문에서의 핵심은 바로 <span style = "color:aqua"><b>Random Walk Algorithm</b></span>이다. 이 논문에서는 서로의 이웃(neighbor)에 존재하지만 직접적인 관계를 공유하지 않는, Disconnected된 엔티티는 구조적으로 서로 관련되어 있을 가능성이 높고, 따라서 서로에 대한 negative sampling의 좋은 후보일 것이라 가정한다. 즉, target entity의 이웃에 존재하지만 직접적으로 연결이 되어있지 않은 엔티티들을 Center triple의 negative로 사용하는 방식이다.

## 2. Structure Aware Negative Sampling
Triple ($$h, r, t$$)가 주어졌을 때, negative sample을 만드는 방법은 head 또는 tail을 바꾸는 것이다. 그러면 negative sample은 ($$h^{'}, r, t$$) 또는 ($$h,r,t^{'}$$)로 표현할 수 있으며, negative를 만들어 내는 엔티티는 그래프 내에 존재하는 엔티티이다. 논문에서는 여러 가지 major한 Knowledge Graph Emnedding 모델을 가지고 실험을 하였는데, 기존의 모델들과는 달리 <span style = "color:gold"><b>Self-Adversarial Loss</b></span>를 사용하여 negative에 대한 정보를 좀 더 강조해서 모델을 학습시켰다.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/fb86e6dd-9dbc-4eef-b8f3-2309613c8252">
</p>

위 식에서 $$d_r(h,t)$$는 head와 tail에 대한 scoring function이다. Scoring function은 Loss에 직접적으로 들어가는 logit을 말한다. $$\gamma$$는 고정된 mar-gin값이며 $$\sigma$$는 Sigmoid function이다. $$n$$은 negative sample의 수이며 논문에서는 정보가 많은 그래프 구조를 이용하여, 특정 target에 이웃에 있지만 disconnected 된 명시적인 엔티티를 negative로 sampling하는 방식을 사용하였다. 

