---
title: "[논문리뷰]Think-On-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2024-07-02
last_modified_at: 2024-07-02
---

*Sun, J., Xu, C., Tang, L., Wang, S., Lin, C., Gong, Y., Ni, L. M., Shum, H., & Guo, J*. (2023, July 15). **Think-on-Graph: Deep and responsible reasoning of large language model on knowledge graph**. arXiv.org. [https://arxiv.org/abs/2307.07697](https://arxiv.org/abs/2307.07697)

# Problem Statement

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/f44435d2-0527-485f-9042-67e28d8a9cd3">
</p>

## 1. 추론 작업에 대한 정확한 답변 제공 실패
LLM은 특히 특화된 지식(specialized knowledge)와 복잡한 추론 작업에 대해 정확한 답변을 내지 못하는 경우가 종종 있다. 이는 모델의 훈련에 사용된 대규모 텍스트 데이터에 포함되지 않은 최신 정보나 흔하지 않은 정보 등을 포함하기 때문이다. 또한, 논리적 연결 고리(reasoning chain)이나 다중 단계 추론(multi-level reasoning, 여러 정보를 연결해야 답이 도출되는 경우)이 필요한 작업에서는 LLM이 정확한 결과를 제공하지 못하는 경우가 많다. 위의 그림처럼, LLM은 종종 매우 긴 context나 Multi-hop reasoning을 하에 있어서 잘못된 대답을 생성하는 경우가 종종 발생한다.

## 2. 책임감(Responsibility), 설명 가능성(Explainability), 투명성(Transparency)의 부재
LLM은 보통 블랙박스처럼 작동하며, 출력에 대한 명확한 이유나 설명을 제공하지 않는다. 이러한 불투명성은 생성된 텍스트의 신뢰성과 잠재적 편향에 대한 우려를 불러일으킨다. 또한, LLM은 그럴듯하지만 잘못되거나 무의미한 답변을 생성하는 "**환각(hallucination)**" 현상이 발생할 수 있다. 이러한 설명 가능성의 부족과 잘못된 또는 유해한 콘텐츠를 생성할 위험성은 높은 신뢰성과 책임이 요구되는 응용 분야에서 LLM을 신뢰하기 어렵게 만든다.

## 3. 비싼 학습 비용
LLM을 훈련하는 데는 매우 많은 양의 컴퓨터 자원(computer resource)를 요구한다. 이로 인해 모델을 최신 상태로 유지하는 것이 어렵다. 결과적으로, LLM은 시간이 지남에 따라 성능이 저하될 수 있으며, 이는 초기 훈련 데이터에 포함되지 않은 새로운 정보와 지식이 등장하기 때문이다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/2dc5b480-b6dc-479f-aecd-382abd5e03e7">
</p>

이중에서도 특히 LLM의 <span style="color:gold">**환각(Hallucination)**</span>에 대한 문제가 매우 중요하다. LLM에서 환각 문제는 크게 다섯 가지 카테고리로 분류된다.
- **환각(Hallucination)**
  - **사실적 환각(Factual Hallucination)**: 모델이 **존재하지 않는 사실을 생성**하는 경우이다. 예를 들어, 실제로 존재하지 않는 사건이나 인물에 대한 정보를 생성하는 경우가 이에 해당한다.
  - **언어적 환각(Linguistic Hallucination)**: 생성된 텍스트가 **문법적으로나 언어적으로 비논리적**인 경우이다. 이는 문장이 비문법적이거나, 문맥적으로 일관성이 없는 경우를 포함한다.
  - **맥락적 환각(Contextual Hallucination)**: 질문의 **맥락과 관련 없는 정보를 생성**하는 경우이다. 모델이 질문을 잘못 이해하거나, 관련 없는 정보를 답변에 포함시키는 경우가 이에 해당한다.
  - **내부적 환각(Intrinsic Hallucination)**: 모델의 **내부 일관성이나 논리와 충돌하는 정보를 생성**하는 경우이다. 예를 들어, 문장의 앞부분과 뒷부분이 모순되는 내용을 담고 있는 경우이다.
  - **외부적 환각(Extrinsic Hallucination)**: 모델이 **외부 지식과 충돌하는 정보를 생성**하는 경우이다. 이는 모델이 잘못된 외부 정보를 기반으로 답변을 생성할 때 발생한다.

이런 환각 문제를 완화하여 LLM이 정확한 추론을 할 수 있도록 만들기 위해 최근 프롬프트 엔지니어링(Prompt Engineering)과 지식 그래프(Knowledge Graph)를 활용하는 것이 최근 연구의 트렌드이다. 

# Related Work

# Method

# Experiments

# Contribution

