---
title: "[논문리뷰]Reformer: The Effocoent Transformer"

categories: 
  - NR
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-11-02
last_modified_at: 2023-11-02
---

*Kitaev, Nikita, et al. “[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451).” ArXiv:2001.04451 [Cs, Stat], 18 Feb. 2020, arxiv.org/abs/2001.04451.*

# Problem Statement

## Drawbacks of Vanilla Transformer
<span style = "font-size:110%"><b>1. Attention 구조에 의한 메모리 문제</b></span>  

입력으로 길이가 $$L$$인 Sequence를 받는데, 시간 복잡도와 공간 복잡도는 $$O(L^2)$$이 된다. Transformer에서 Attention 연산은 **Dot-Product Attnetion**이다. 수식으로 좀 더 구체화하면 다음과 같다.

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

- Query($$Q$$): 영향을 받는 단어 (객체)
- Key($$K$$): 영향을 주는 단어 (주체)
- Value($$V$$): Key에 대응되는 값

시간 복잡도(Time Complexity)가 $$Q$$의 크기와 $$K$$의 크기에 곱에 비례한다. 이는 데이터의 길이가 10배 길어지면 연산 자체는 100배 더 많아지는 것과 같다. 

<span style = "font-size:110%"><b>2. N-stacked Residual Connection에 의한 메모리 문제</b></span>  
Vanilla Transformer의 Encoder와 Decoder의 Layer수가 너무 많다는 문제점이 있다. N개의 Layer 층을 이루기 때문에 N배 많은 메모리를 필요로 한다. 마지막 Layer에서부터 시작 Layer까지 backpropagation을 하면서 미분값을 구하고, parameter를 업데이트 히는데, 층 수가 많아지면 많아질수록 연산에 더 많은 메모리가 요구된다. 이는 N개의 Layer가 쌓이면 그만큼 Residual Connection도 늘어나므로, 연산이 증가함을 의미한다.

<span style = "font-size:110%"><b>3. Feed Forward Layer에 의한 메모리 문제</b></span>  
Feed Forward Layer(FFN)이 attention activation의 깊이보다 더 클 수 있다. 실제로 FFN이 각 Attention Layer의 출력에 모두 적용이 되어야 한다. 이 구조가 차지하는 메모리는 따라서 Sequence의 길이($$L$$)와 해당 layer의 입출력 차원의 곱에 비례한다. 문제는 보통 이 때 사용되는 입출력 차원은 대개 모델 임베딩의 차원에 비해 크다는 것이다. 

$$FFN(x) = max(0, x \cdot W_1 + b_1) \; \cdot \; W_2 + b_2$$

Transformer에서도 입력 시퀀스는 512개의 토큰을 maximum으로 하지만, 실제로 입력 차원수는 2048이다. 따라서 데이터의 길이가 충분히 길면 이 FFN구조가 차지하는 메모리도 무시할 수 없게 된다.

<br/>
<br/>

# Method
## 1. Locality-Sensitive Hashing

<br/>
<br/>

# Experiment

<br/>
<br/>

# Contribution
1. Locality-Sensitive-Hashing (LSH)를 통해 서로 비슷한 임베딩이 비슷한 단어 쌍만 Attention을 진행한다.
  a. 영향력이 높은 단어 쌍끼리만 가중치를 계산할 수 있다면 성능 저하 없이 복잡도가 대폭 줄어든 구조를 얻을 수 있다.
  b. 영향력이 높은 단어 쌍은 임베딩 공간에서 서로 비슷한 pair이며, 이런 pair들은 LSH를 이용해서 빠르게 찾을 수 있다.
  c. 결론적으로 attention computation이 $$O(L^2)$$에서 $$O(LlogL)$$로 감소한다.

3. Chunking을 이용해 메모리를 줄인다.
  a. Feed-Forward Layer (FFN)는 Attention Layer와 다르게 데이터 포인트의 위치에 무관하게 계산된다.
  b. 따라서 데이터 포인트들을 묶어줄 수 있다면 계산하는 단위를 나눌 수 있고, 전체 데이터 포인트에 대한 FFN의 가중치를 한 번에 메모리에 올리지 않아도 된다.
    
4. Reversible Layer를 이용해 메모리를 줄인다.
  a. Transformer는 Attention Layer와 FFN의 residual connection으로 되어 있다. 
  b. 본 논문에서는 기존 수식을 살짝 변형해 출려을 이용해서 입력을 다시 복원할 수 있는 형태로 기술했다.
  c. 출력으로 입력을 복원할 수 있으므로, 각 중간 단계의 입출력을 저장할 필요 없이 바로 출력에서 미분값을 계산하여 훈련할 수 있다.
