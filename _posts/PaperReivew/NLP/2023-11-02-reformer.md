---
title: "[논문리뷰]Reformer: The Effocoent 트랜스포머"

categories: 
  - NR
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-11-02
last_modified_at: 2023-11-02
---

*Kitaev, Nikita, et al. “[Reformer: The Efficient 트랜스포머](https://arxiv.org/abs/2001.04451).” ArXiv:2001.04451 [Cs, Stat], 18 Feb. 2020, arxiv.org/abs/2001.04451.*

# Problem Statement

## Drawbacks of Vanilla 트랜스포머
<span style = "font-size:110%"><b>1. Attention 구조에 의한 메모리 문제</b></span>  

입력으로 길이가 $$L$$인 Sequence를 받는데, 시간 복잡도와 공간 복잡도는 $$O(L^2)$$이 된다. 트랜스포머에서 Attention 연산은 **Dot-Product Attnetion**이다. 수식으로 좀 더 구체화하면 다음과 같다.

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

- Query($$Q$$): 영향을 받는 단어 (객체)
- Key($$K$$): 영향을 주는 단어 (주체)
- Value($$V$$): Key에 대응되는 값

시간 복잡도(Time Complexity)가 $$Q$$의 크기와 $$K$$의 크기에 곱에 비례한다. 이는 데이터의 길이가 10배 길어지면 연산 자체는 100배 더 많아지는 것과 같다. 

<span style = "font-size:110%"><b>2. N-stacked Residual Connection에 의한 메모리 문제</b></span>  
Vanilla 트랜스포머의 Encoder와 Decoder의 Layer수가 너무 많다는 문제점이 있다. N개의 Layer 층을 이루기 때문에 N배 많은 메모리를 필요로 한다. 마지막 Layer에서부터 시작 Layer까지 backpropagation을 하면서 미분값을 구하고, parameter를 업데이트 히는데, 층 수가 많아지면 많아질수록 연산에 더 많은 메모리가 요구된다. 이는 N개의 Layer가 쌓이면 그만큼 Residual Connection도 늘어나므로, 연산이 증가함을 의미한다.

<span style = "font-size:110%"><b>3. Feed Forward Layer에 의한 메모리 문제</b></span>  
Feed Forward Layer(FFN)이 attention activation의 깊이보다 더 클 수 있다. 실제로 FFN이 각 Attention Layer의 출력에 모두 적용이 되어야 한다. 이 구조가 차지하는 메모리는 따라서 Sequence의 길이($$L$$)와 해당 layer의 입출력 차원의 곱에 비례한다. 문제는 보통 이 때 사용되는 입출력 차원은 대개 모델 임베딩의 차원에 비해 크다는 것이다. 

$$FFN(x) = max(0, x \cdot W_1 + b_1) \; \cdot \; W_2 + b_2$$

트랜스포머에서도 입력 시퀀스는 512개의 토큰을 maximum으로 하지만, 실제로 입력 차원수는 2048이다. 따라서 데이터의 길이가 충분히 길면 이 FFN구조가 차지하는 메모리도 무시할 수 없게 된다. 즉, Sequence의 길이에 보통 4배 정도로 Layer의 차원수를 설정하고, Sequence가 길어지면 그에 따라 계산해야하는 파라미터수가 늘어나므로, 이것의 계산 복잡도가 시퀀스에 비례해 커지는 것을 말한다.

<br/>
<br/>

# Method
## 1. Locality-Sensitive Hashing (LSH)
Hashing은 해시 function(해시 함수) algorithm을 말하며 임의의 길이의 데이터를 고정된 길이의 데이터로 매핑하는 함수를 해시 함수이라고 한다. 데이터를 미리 Hashing해두면 해시값만으로 쉽게 데이터를 찾을 수 있다. 보통 해시값은 연결된 데이터와 전혀 관련이 없을 때가 만고, 그렇기 때문에 전체 데이터 분포에서 상대적 위치를 확인하거나 한 데이터와 가장 가까운 다른 데이터를 찾는 등 데이터에 대한 비교 분석을 할 때 반드시 실제 데이터 값을 비교하는 연산이 필요하다. 

이 논문에서 Hashing을 사용함에 있어 핵심은, <span style = "color:gold"><b>가까운 거리에 위치한 데이터들은 가까운 해시값을 갖도록 구성</b></span>하는 것이다. 이러면 **비교 연산을 해시값에 대한 연산으로 근사**할 수 있다. 이렇게 가까운 값들을 가까운 해시값을 가지도록 Hashing하는 것을 Locality-Sensitive Hashing(LSH)라 한다. LSH를 보기 전 기존 트랜스포머의 Attention을 먼저 알아야 한다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/5e49bf9e-ba75-4541-bef6-5d2850a45d96">
</p>

Vanilla 트랜스포머에서는 세 가지 종류의 Attention이 존재한다. 바로 Encoder의 Self-Attention과 Decoder의 Masked Self-Attention, 그리고 Encoder의 최종 레이어의 출력과 디코더의 Masked Self-Attention을 거친 출력과 Cross Attention을 하는 Encoder-Decoder Attention이다. 또한 이 세 가지 Attention은 기본적으로 Dot-Product Attention이다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/16e8996f-72cb-4268-b11a-8abc7ca35093">
</p>

이때 `it`의 Attention 중 5개 ('The', 'animal', 'street', 'it', '.' )  를 제외하면 제대로 그 영향력이 전달되지 않은 것을 볼 수 있다. 그래서 `didn't`나 `cross`등과 같은 단어들은 연산 결과 Attention을 받지 않았기 때문에 실제로는 Sparse하게 된다. 이를 바꿔말하면, 우리는 Query(Q)에 대해 밀접한 연관을 가진 K에 대해서만 Attention을 하면 된다. 하지만 기존 트랜스포머는 이 부분을 비효율적으로 찾고, 모든 단어와 단어 사이 Attention을 계산하기 때문에 비효율 적인 것이다. Reformer에서는 LSH를 통해 이 문제를 해결하였다. 그러면 여기서 문제는 어떻게 LSH를 기계적으로 수행할 수 있을지이다.

### 1) Simple LSH

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/a2159ae7-6f2d-4ae8-98bf-adc23e55107c">
</p>

LSH는 고차원 데이터에 대해 *nearest neighbor* 알고리즘을 이용해 효율적으로 근사하는 방법이다. 두 점 $$p, \; q$$가 충분히 가깝다면 해시 함수을 거친 결과가 $$해시(p) == 해시(q)$$일 것이다. 임의의 직선 $$h1, \; h2, \; h3$$를 그어 영역에 따라 0과 1로 분류 한다. 각 포인트는 3개의 해시 값을 가지게 되고, 그 해시값이 버켓이라고 보면 된다. 이때 각 충분히 가까운 포인트는 거의 같은 버켓에 들어갔을음 볼 수있다.

### 2) Angular LSH
Reformer에서 실제로 사용한 방법은 **Angular LSH**이다. 이 방법은 <u>방향 성분만을 활용하여 해시값을 생성</u>한다. 전체적인 과정은 다음과 같다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/76fde40f-3fa8-4634-a85e-a916130c8c06">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/062cce4c-fda8-44fe-9206-f1daaea223ab">
</p>

1. 전체 데이터 포인트들의 벡터를 단위 구면에 맵핑한다. 이러면 전체 데이터 포인트를 오직 **각도**만 사용해서 기술할 수 있다. ($$r=1$$인 구면 좌표계)
2. 이제 각 각도가 어느 사분면에 있는지 확인한다. 비슷한 데이터들은 같은 사분면에 있다. 따라서 사분면의 번호를 해시값으로 사용한다면, 비슷한 데이터들을 가깝게 구성할 수 있다.
3. 이제 사상한 구면을 필요한 만큼 임의로 회전시킨다. 데이터가 가까우면 가까울수록 전체 해값을 공유할 가능성이 높아지고, 충분히 많은 해시값을 사용하면 데이터를 구별하는 변별력이 생긴다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/a2f40798-f545-4320-b3af-3b6fb4407a40">
</p>

예를 들어, 2차원 임베딩 벡터가 $$X_1 = (3, 4)$$, $$X_2 = (-12,5)$$가 있다고 가정하고, 이를 반지금 1인 구에 맵핑하면 각각의 점들은 $$X_1^{'} = (\frac{3}{5}, \frac{4}{5})$$와  $$X_2^{'} = ( - \frac{12}{13}, \frac{5}{13})$$이 된다.(그림에서는 이해를 위해 축을 회전) 다음으로 임의의 벡터 $$Y = (4,3)$$이 주어졌다고 가정하면 이 벡터는 원 위에서 $$Y^{'} = (\frac{4}{5}, \frac{3}{5})$$으로 맵핑된다. 똑같이 원을 돌리면서 해시값을 표현해보면 $$Y$$의 해시값은 <b>(1, 4, 2)</b>가 된다.

이 값은 $$X_1$$의 해시갑과 같고, $$X_2$$의 해시값과는 다르므로 데이터 간의 직접 비교 연산 없이 <span style = "color:gold"><b>해시값이 일치하는지 보는 것만으로도 가까운 점들을 선별</b></span>할 수 있다. 하지만 '이런 방식으로 데이터를 저장하면 방향 값은 살아있어도 크기에 대한 값은 손실되는 것이 아닌가?' 라는 의문이 생기고 이에 대한 실험은 Ablation Study에서 기술하였다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/e1596ed3-d49d-4e42-a38b-5be2b43f3946">
</p>

<span style = "font-size:110%">LSH Attention</span>  

기존의 트랜스포머에서 길이가 $$L$$인 시퀀스가 입력으로 들어오면 그에 따른 $$(Q, K, V)$$값들은 $$(batch, length, d_{model})$$의 차원수를 가진다. 그리고 $$QK^T$$는 $$(batch, length, length)$$의 shape을 가지기 때문에 결론적으로 시간 복잡도와 공간 복잡도가 모두 $$O(L^2)$$이 된다. 이로 인해 Long-Sequence가 들어오면 학습에 드는 비용이 기하급수적으로 증가한다. 하지만, LSH등을 사용하면 가까운 임베딩끼리만 Attention을 진행할 수 있기 때문에 수식이 다음과 같이 바뀌며, 이는 메모리 효율적이다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/33d6720b-fb6a-40fa-8ace-1bfe2ad41f59">
</p>

> But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately,
> only calculating $$softmax(\frac{q_iK^T}{\sqrt{d_k}})$$ once in memory, and then re-computing it on the backward pass when needed for gradients.
> This way of computing attention may be less efficient but it only uses memory proportional to length.

최종적으로 LSH attention의 softmax안에 들어가는 식은 다음과 같다. 여기서 $$\mathcal{P_i}$$는 query의 position이고, $$z$$함수는 partition function이다. 여기서 batch까지 고려하면 다시 식은 아래와 같이 바뀌고, 최종적인 attention식이 된다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/8e4dfcb5-41b6-440c-ad1c-db77b7536171">
</p>

기존의 Attention에서는 입력으로 들어온 Sequence 내 모든 토큰들이 서로 서로 attention을 진행하기 때문에 히트맵에서 모든 부분에 영향력을 나타내는 정도가, 비록 그 어텐션 값이 작을지라도 표시된다. 반면, LSH Attention의 히트맵 같은 경우 같은 bucket안에 들어있는 단어들끼리만 어텐션을 진행하므로 다음과 같이 히트맵이 변한다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b0b6314f-4bb6-4bd8-a2e3-7d632b82d277">
</p>

## 2. Reversible Transformer

Reverseible Transformer는 기존에 연구된 [The Reversible Residual Network (RevNet)](https://arxiv.org/abs/1707.04585)의 아이디어를 적용한 것이다. 구조를 요약하면 Encoder, Decoder에서 attention layer와 feed-forward layer를 하나로 묶어버린 것이다. 이렇게하면, 각 layer안에 activation을 저장할 필요가 없다. 따라서 저장 공간을 줄어든다.

<p align="center">
<img width="200" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/f553e9f3-67f9-4e47-98af-eb6c37810172">
</p>

RevNet은 원래 이미지 처리에 사용되는 ResNet 구조에서 메모리를 효율적으로 사용하기 위해 고안되었다. 기존의 ResNet에서 사용되는 계산, 일반적인 Residual Connection의 연산은 위와 같다.

<br/>
<br/>

# Experiment

<br/>
<br/>

# Contribution
1. Locality-Sensitive-Hashing (LSH)를 통해 서로 비슷한 임베딩이 비슷한 단어 쌍만 Attention을 진행한다.
  - 영향력이 높은 단어 쌍끼리만 가중치를 계산할 수 있다면 성능 저하 없이 복잡도가 대폭 줄어든 구조를 얻을 수 있다.
  - 영향력이 높은 단어 쌍은 임베딩 공간에서 서로 비슷한 pair이며, 이런 pair들은 LSH를 이용해서 빠르게 찾을 수 있다.
  - 결론적으로 attention computation이 $$O(L^2)$$에서 $$O(LlogL)$$로 감소한다.

3. Chunking을 이용해 메모리를 줄인다.
  - Feed-Forward Layer (FFN)는 Attention Layer와 다르게 데이터 포인트의 위치에 무관하게 계산된다.
  - 따라서 데이터 포인트들을 묶어줄 수 있다면 계산하는 단위를 나눌 수 있고, 전체 데이터 포인트에 대한 FFN의 가중치를 한 번에 메모리에 올리지 않아도 된다.
    
5. Reversible Layer를 이용해 메모리를 줄인다.
  - 트랜스포머는 Attention Layer와 FFN의 residual connection으로 되어 있다. 
  - 본 논문에서는 기존 수식을 살짝 변형해 출려을 이용해서 입력을 다시 복원할 수 있는 형태로 기술했다.
  - 출력으로 입력을 복원할 수 있으므로, 각 중간 단계의 입출력을 저장할 필요 없이 바로 출력에서 미분값을 계산하여 훈련할 수 있다.

<br/>
<br/>

# Reference
[Blog: Reformer, The Efficient 트랜스포머](https://velog.io/@nawnoes/Reformer-%EA%B0%9C%EC%9A%94)    
[Blog: 꼼꼼하고 이해하기 쉬운 Reformer 리뷰](https://tech.scatterlab.co.kr/reformer-review/)
