---
title: "[논문리뷰]Reformer: The Effocoent Transformer"

categories: 
  - NR
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-11-02
last_modified_at: 2023-11-02
---

*Kitaev, Nikita, et al. “[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451).” ArXiv:2001.04451 [Cs, Stat], 18 Feb. 2020, arxiv.org/abs/2001.04451.*

# Problem Statement

## Drawbacks of Vanilla Transformer
<span style = "font-size:110%"><b>1. Attention 구조에 의한 메모리 문제</b></span>  

입력으로 길이가 $$L$$인 Sequence를 받는데, 시간 복잡도와 공간 복잡도는 $$O(L^2)$$이 된다. Transformer에서 Attention 연산은 **Dot-Product Attnetion**이다. 수식으로 좀 더 구체화하면 다음과 같다.

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
<center><span style = "font-size:80%">Dot-Product Attention<span></center>

- Query($$Q$$): 영향을 받는 단어 (객체)
- Key($$K$$): 영향을 주는 단어 (주체)
- Value($$V$$): Key에 대응되는 값

시간 복잡도(Time Complexity)가 $$Q$$의 크기와 $$K$$의 크기에 곱에 비례한다. 이는 데이터의 길이가 10배 길어지면 연산 자체는 100배 더 많아지는 것과 같다. 

<span style = "font-size:110%"><b>2. N-stacked Residual Connection에 의한 메모리 문제</b></span>  
Vanilla Transformer의 Encoder와 Decoder의 Layer수가 너무 많다는 문제점이 있다. N개의 Layer 층을 이루기 때문에 N배 많은 메모리를 필요로 한다. 마지막 Layer에서부터 시작 Layer까지 backpropagation을 하면서 미분값을 구하고, parameter를 업데이트 히는데, 층 수가 많아지면 많아질수록 연산에 더 많은 메모리가 요구된다. 이는 N개의 Layer가 쌓이면 그만큼 Residual Connection도 늘어나므로, 연산이 증가함을 의미한다.

<span style = "font-size:110%"><b>3. Feed Forward Layer에 의한 메모리 문</b></span>  
Feed Forward Layer(FFN)이 attention activation의 깊이보다 더 클 수 있다. 실제로 FFN이 각 Attention Layer의 출력에 모두 적용이 되어야 한다. 이 구조가 차지하는 메모리는 따라서 Sequence의 길이($$L$$)와 해당 layer의 입출력 차원의 곱에 비례한다. 문제는 보통 이 때 사용되는 입출력 차원은 대개 모델 임베딩의 차원에 비해 크다는 것이다. 

$$FFN(x) = max(0, x \cdot W_1 + b_1) \; \cdot \; W_2 + b_2$$

Transformer에서도 입력 시퀀스는 512개의 토큰을 maximum으로 하지만, 실제로 입력 차원수는 2048이다. 따라서 데이터의 길이가 충분히 길면 이 FFN구조가 차지하는 메모리도 무시할 수 없게 된다.

<br/>
<br/>

# Method

<br/>
<br/>

# Experiment

<br/>
<br/>

# Contribution
