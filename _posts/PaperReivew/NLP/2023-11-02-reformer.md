---
title: "[논문리뷰]Reformer: The Effocoent Transformer"

categories: 
  - NR
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-11-02
last_modified_at: 2023-11-02
---

*Kitaev, Nikita, et al. “[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451).” ArXiv:2001.04451 [Cs, Stat], 18 Feb. 2020, arxiv.org/abs/2001.04451.*

# Problem Statement

## Drawbacks of Vanilla Transformer
<span style = "font-size:110%"><b>1. Attention 구조에 의한 메모리 문제</b></span>  

입력으로 길이가 $$L$$인 Sequence를 받는데, 시간 복잡도와 공간 복잡도는 $$O(L^2)$$이 된다. Transformer에서 Attention 연산은 **Dot-Product Attnetion**이다. 수식으로 좀 더 구체화하면 다음과 같다.

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

- Query($$Q$$): 영향을 받는 단어 (객체)
- Key($$K$$): 영향을 주는 단어 (주체)
- Value($$V$$): Key에 대응되는 값

시간 복잡도(Time Complexity)가 $$Q$$의 크기와 $$K$$의 크기에 곱에 비례한다. 이는 데이터의 길이가 10배 길어지면 연산 자체는 100배 더 많아지는 것과 같다. 

<span style = "font-size:110%"><b>2. N-stacked Residual Connection에 의한 메모리 문제</b></span>  
Vanilla Transformer의 Encoder와 Decoder의 Layer수가 너무 많다는 문제점이 있다. N개의 Layer 층을 이루기 때문에 N배 많은 메모리를 필요로 한다. 마지막 Layer에서부터 시작 Layer까지 backpropagation을 하면서 미분값을 구하고, parameter를 업데이트 히는데, 층 수가 많아지면 많아질수록 연산에 더 많은 메모리가 요구된다. 이는 N개의 Layer가 쌓이면 그만큼 Residual Connection도 늘어나므로, 연산이 증가함을 의미한다.

<span style = "font-size:110%"><b>3. Feed Forward Layer에 의한 메모리 문제</b></span>  
Feed Forward Layer(FFN)이 attention activation의 깊이보다 더 클 수 있다. 실제로 FFN이 각 Attention Layer의 출력에 모두 적용이 되어야 한다. 이 구조가 차지하는 메모리는 따라서 Sequence의 길이($$L$$)와 해당 layer의 입출력 차원의 곱에 비례한다. 문제는 보통 이 때 사용되는 입출력 차원은 대개 모델 임베딩의 차원에 비해 크다는 것이다. 

$$FFN(x) = max(0, x \cdot W_1 + b_1) \; \cdot \; W_2 + b_2$$

Transformer에서도 입력 시퀀스는 512개의 토큰을 maximum으로 하지만, 실제로 입력 차원수는 2048이다. 따라서 데이터의 길이가 충분히 길면 이 FFN구조가 차지하는 메모리도 무시할 수 없게 된다.

<br/>
<br/>

# Method
## 1. Locality-Sensitive Hashing (LSH)
Hashing은 Hash function algorithm을 말하며 임의의 길이의 데이터를 고정된 길이의 데이터로 매핑하는 함수를 Hash function이라고 한다. 데이터를 미리 Hashing해두면 Hash값만으로 쉽게 데이터를 찾을 수 있다. 보통 Hash값은 연결된 데이터와 전혀 관련이 없을 때가 만고, 그렇기 때문에 전체 데이터 분포에서 상대적 위치를 확인하거나 한 데이터와 가장 가까운 다른 데이터를 찾는 등 데이터에 대한 비교 분석을 할 때 반드시 실제 데이터 값을 비교하는 연산이 필요하다. 

이 논문에서 Hashing을 사용함에 있어 핵심은, <span style = "color:gold"><b>가까운 거리에 위치한 데이터들은 가까운 Hash값을 갖도록 구성</b></span>하는 것이다. 이러면 **비교 연산을 Hash값에 대한 연산으로 근사**할 수 있다. 이렇게 가까운 값들을 가까운 Hash값을 가지도록 Hashing하는 것을 Locality-Sensitive Hashing(LSH)라 한다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/5e49bf9e-ba75-4541-bef6-5d2850a45d96">
</p>

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/16e8996f-72cb-4268-b11a-8abc7ca35093">
</p>

Vanilla Transformer에서는 세 가지 종류의 어텐션이 존재한다. 바로 Encoder의 Self-Attention과 Decoder의 Masked Self-Attention, 그리고 Encoder의 최종 레이어의 출력과 디코더의 Masked Self-Attention을 거친 출력과 Cross Attention을 하는 Encoder-Decoder Attention이다. 또한 이 세 가지 어텐션은 기본적으로 Dot-Product Attention이다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/a2159ae7-6f2d-4ae8-98bf-adc23e55107c">
</p>

<br/>
<br/>

# Experiment

<br/>
<br/>

# Contribution
1. Locality-Sensitive-Hashing (LSH)를 통해 서로 비슷한 임베딩이 비슷한 단어 쌍만 Attention을 진행한다.
  - 영향력이 높은 단어 쌍끼리만 가중치를 계산할 수 있다면 성능 저하 없이 복잡도가 대폭 줄어든 구조를 얻을 수 있다.
  - 영향력이 높은 단어 쌍은 임베딩 공간에서 서로 비슷한 pair이며, 이런 pair들은 LSH를 이용해서 빠르게 찾을 수 있다.
  - 결론적으로 attention computation이 $$O(L^2)$$에서 $$O(LlogL)$$로 감소한다.

3. Chunking을 이용해 메모리를 줄인다.
  - Feed-Forward Layer (FFN)는 Attention Layer와 다르게 데이터 포인트의 위치에 무관하게 계산된다.
  - 따라서 데이터 포인트들을 묶어줄 수 있다면 계산하는 단위를 나눌 수 있고, 전체 데이터 포인트에 대한 FFN의 가중치를 한 번에 메모리에 올리지 않아도 된다.
    
5. Reversible Layer를 이용해 메모리를 줄인다.
  - Transformer는 Attention Layer와 FFN의 residual connection으로 되어 있다. 
  - 본 논문에서는 기존 수식을 살짝 변형해 출려을 이용해서 입력을 다시 복원할 수 있는 형태로 기술했다.
  - 출력으로 입력을 복원할 수 있으므로, 각 중간 단계의 입출력을 저장할 필요 없이 바로 출력에서 미분값을 계산하여 훈련할 수 있다.

<br/>
<br/>

# Reference
[Blog: Reformer, The Efficient Transformer](https://velog.io/@nawnoes/Reformer-%EA%B0%9C%EC%9A%94)    
[Blog: 꼼꼼하고 이해하기 쉬운 Reformer 리뷰](https://tech.scatterlab.co.kr/reformer-review/)
