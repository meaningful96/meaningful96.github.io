---
title: "[논문리뷰]LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation(COLING, 2025)"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-16
last_modified_at: 2025-06-16
---

*Keheng Wang, Feiyu Duan, Peiguang Li, Sirui Wang, and Xunliang Cai**. “[**LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation**](https://aclanthology.org/2025.coling-main.163/).” In Proceedings of the 31st International Conference on Computational Linguistics, pages 2379–2400, Abu Dhabi, UAE, January 2025.

# Problem Statement
이 논문은 기존 RAG 시스템이 복잡한 multi-hop 질문을 처리하는 데 있어 두 가지 핵심적인 한계를 갖고 있음을 지적한다.

- **첫째, 질의 자체의 복잡성(query-side issue):** 기존 질문이 다중 정보를 암시적으로 포함하거나 중간 추론 단계를 요구할 경우, 관련 문서를 찾기 어렵다. 예를 들어, "Oh Billy, Behave의 감독 출생지는 어디인가?" 같은 질문은 중간 개념(감독 이름)을 알아야 최종 정답을 도출할 수 있다.
- **둘째, 문서 내 정보의 혼잡성(document-side issue):** RAG가 검색한 문서에는 관련 없는 노이즈가 많아, 정답 도출에 필요한 핵심 정보 추출이 어렵다.

이러한 문제를 해결하기 위해 이 논문은 LLM이 질의에 답하기 위해 **무엇이 부족한지를 인식하고**, 그 결핍된 정보를 기반으로 **단계별로 검색-추출-정답 생성**을 수행할 수 있도록 유도하는 새로운 RAG 프레임워크 **MIGRES**를 제안한다.

<br/>
<br/>

# Methodology

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.16%5DMIRGES/MIRGES1.png?raw=true">
</p>


MIGRES는 Missing Information Guided Retrieve-Extraction-Solving 패러다임으로, 네 개의 주요 모듈로 구성된다.

1. **Main Module**이 질의와 현재까지 확보된 정보로 정답을 유도할 수 있는지 판단한다.
  - 가능할 경우 정답과 설명을 생성한다.
  - 불가능할 경우, 부족한 정보를 명시하고 다음 검색을 유도한다.
2. **Retrieval Module**이 해당 정보를 바탕으로 새로운 질의들을 생성하고 외부 지식에서 관련 문서를 검색한 뒤, 문장 수준에서 노이즈 필터링을 수행한다.
3. **Leaf Module**은 필터링된 문서에서 정답 추론에 필요한 유용한 정보를 추출하고, 해당 정보를 검증한다(NLI 모델 사용).
4. **Memory Module**은 이전에 생성된 질의 및 검색된 문서를 저장하여 중복 질의 및 무의미한 문서 재사용을 방지한다.

## Main Module
- **입력**: 사용자 질의 + 현재까지 확보된 정보
- **출력**: 정답 + 설명 또는 unanswerable + 결핍 정보
- LLM이 "정답 도출 가능 여부"를 판단하여, 가능하면 정답을 생성하고, 불가능하면 어떤 정보가 부족한지 명시함.

**Main Module**은 질문과 추출된 유용한 정보를 입력으로 받아 질문이 답변 가능한지 판단한다. 충분한 정보가 있으면 최종 답변과 설명을 반환하고, 불충분하면 "unanswerable"과 함께 부족한 정보를 식별하여 출력한다. 입력은 질문과 추출된 유용한 정보이며, 출력은 최종 답변 또는 부족한 정보다.

## Retrieval Module
- **입력**: 질의, 이전 질의 기록, 결핍 정보
- **출력**: 외부 지식 문서 (sentence-level filter 완료된 상태)
- 내부 구성:
    - **Query Generator**: 최대 3개의 단순하고 다양한 질의 생성
    - **Retriever**: BM25 기반으로 문서 검색
    - **Knowledge Filter**:
        - 문장 단위로 relevance 점수 계산 후 threshold 이하인 문장 제거
        - 전체 문장이 무의미할 경우, LLM의 파라메트릭 지식을 활용하여 직접 관련 정보를 생성함

**Retrieval Module**은 Query Generator, Retriever, Knowledge Filter로 구성된다. 원본 질문, 이전 질문들, 추출된 유용한 정보, Main module의 부족한 정보를 입력으로 받아 Query Generator가 더 간단하고 다양한 새로운 질문들을 생성한다. Retriever는 이 질문들에 대한 관련 외부 지식을 획득하고, Knowledge Filter는 passage-level과 sentence-level에서 노이즈를 제거한다. 입력은 원본 질문, 이전 질문들, 유용한 정보, 부족한 정보이며, 출력은 필터링된 관련 문서들이다.

## Leaf Module
- **입력**: 질의 + 지식 문서
- **출력**: 정답 도출에 필요한 정보 리스트 (supporting 문서 인덱스 포함)
- 정보의 근거로 참조된 문서를 명시하게 하여 환각 방지. 추가로 NLI 모델로 entailment 여부를 검증함.

**Leaf Module**은 검색된 외부 지식을 읽어 유용한 정보를 추출하고 지원 passage의 인덱스를 함께 제공한다. 추출된 정보에 환각이 포함될 수 있으므로 NLI 모델을 사용하여 인용된 passage가 실제로 추출된 정보를 뒷받침하는지 검증한다. 입력은 질문들과 획득된 지식이며, 출력은 검증된 유용한 정보다.

## Memory Module
- **입력**: 이전 질의들 및 검색 결과들
- **출력**: 새로운 질의 생성 시 중복 및 무의미한 문서 제거
- query/knowledge diversity 확보 및 hard negative 방지

**Memory Module**은 과거에 검색된 passage들과 생성된 질문들을 기록한다. 이를 통해 새로운 질문의 다양성을 향상시키고 hard negative 지식을 필터링한다. 입력은 검색 기록과 질문 기록이며, 출력은 업데이트된 메모리다.

<br/>
<br/>

# Experiments
## Main Results
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.16%5DMIRGES/MIRGES2.png?raw=true">
</p>

MIGRES는 Multi-hop QA(Wikihop, HotpotQA, Musique), Open-domain QA(NQ, TriviaQA), Commonsense QA(StrategyQA) 등 총 세 분야의 다섯 개 대표적인 지식 집약형 QA 데이터셋에서 성능을 평가받았으며, 주요 결과는 Table 3에 제시되어 있다. Zero-shot 설정에서 MIGRES는 기존 ALCE 프레임워크 내의 모든 방법들(VANILLA, SUMM, SNIPPET, RERANK)을 전반적으로 상회하였다. 특히, Wikihop에서 EM 33.6% / Acc† 58.5%, HotpotQA에서 EM 38.0% / Acc† 66.6%, StrategyQA에서 EM 73.4% / Acc† 73.4%를 기록하며, 각각 RERANK 대비 EM 기준으로 5.1%, 8.0%, 7.5%의 향상폭을 보였다. 이는 <span style="color:gold">**단순히 문서를 많이 활용하는 접근보다, 결핍 정보에 기반한 목표 지향적 검색과 추론이 LLM의 응답 품질을 효과적으로 제고할 수 있음을 시사**</span>한다.

Few-shot 설정에서의 MIGRES는 더욱 뛰어난 성능을 보였다. Wikihop에서 EM 47.6%를 기록하며, 기존 최고 성능 baseline인 VTG의 41.5%를 6.1% 능가하였고, HotpotQA에서도 EM 46.8%로 ITER-RETGEN의 44.1%를 2.7% 초과하였다. StrategyQA에서는 Acc† 74.2%로 ITER-RETGEN의 73.0%를 상회하며 거의 모든 데이터셋에서 최고 성능을 달성하였다. 특히 오라클 지식 기반의 corpus를 활용한 MIGRES†는 더욱 강력한 성능을 나타내며 Wikihop에서 EM 54.0%, HotpotQA에서 EM 49.4%를 기록하였다. 이는 MIGRES의 성능 한계가 주로 외부 지식 pool의 recall 한계에서 비롯됨을 보여준다.

## Ablation 1. Sentence-Level Filtering의 효과
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.16%5DMIRGES/MIRGES3.png?raw=true">
</p>

Sentence-Level Filtering의 기여도는 Table 5를 통해 분석되며, 토큰 효율성과 모델 성능 모두에서 유의미한 개선을 보였다. 예를 들어 Wikihop에서는 필터링 미적용 시 평균 877개의 토큰이 사용되었으나, 필터링 적용 시 733개로 16.4% 절감되었고, EM 성능은 33.0%에서 33.6%로 소폭 향상되었다. NQ에서도 필터링을 적용함으로써 토큰 수는 404개에서 333개로 17.6% 감소하였으며, EM은 43.5%에서 43.0%로 약간 낮아졌으나 큰 성능 손실 없이 효율성을 확보하였다. 반면 Musique에서는 필터링 미적용 시 EM 19.0%, 필터링 적용 시 EM 18.0%로 소폭 성능 저하가 있었으며, Acc†도 함께 하락하였다. 그럼에도 불구하고 **평균 토큰 소비량은 1697개에서 1224개로 27.9%나 절감**되어, 토큰 비용 측면에서의 효과는 매우 컸다.

## Ablation 2. GPT Knowledge Prompting의 효과
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.16%5DMIRGES/MIRGES4.png?raw=true">
</p>

GPT Knowledge Prompting의 효과는 Table 6에 제시된 바와 같이, 문서 검색이 실패했을 경우 LLM의 내재적 지식을 활용해 추가 정보를 생성하는 전략이 성능 개선에 기여함을 보여준다. Wikihop에서 knowledge prompting을 적용했을 때 EM은 32.0%에서 34.8%로, Acc†는 53.8%에서 54.4%로 각각 2.8%, 0.6% 향상되었고, NQ에서도 EM이 41.5%에서 42.5%로 개선되었다. 생성된 지식의 정확도를 평가한 Table 7에 따르면, GPT-3.5 기반 생성 지식은 Wikihop에서 45.5%, Musique에서 54.6%의 EM 기준 정확도를 보였으며, GPT-4를 사용할 경우 각각 87.7%, 77.5%로 크게 상승하였다. 이는 단순 검색 기반 시스템에서 벗어나, LLM의 parametric knowledge를 전략적으로 활용하는 방식이 retrieval 실패를 보완할 수 있음을 시사한다.

## Analysis. Efficiency (문서 수 & API 호출 효율성)
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.16%5DMIRGES/MIRGES5.png?raw=true">
</p>

문서 수 및 API 호출 측면에서의 효율성은 Table 4를 통해 확인되며, MIGRES의 실용적 강점을 명확히 드러낸다. MIGRES는 평균적으로 5개 미만의 문서만을 활용하여 정답을 도출하였다. 예를 들어, Wikihop에서는 평균 1.3개의 문서와 2.5회의 반복(iteration)으로 정답을 생성했으며, HotpotQA에서도 3.1개의 문서만으로 충분했다. 반면, ReAct는 Wikihop에서 평균 15.0개의 문서, Self-Ask는 15.9개의 문서를 사용하였다. 이는 MIGRES가 약 90% 가까이 적은 문서 수로도 기존 방법들과 동등하거나 그 이상의 성능을 달성할 수 있음을 의미한다. API 호출 횟수 역시 MIGRES는 Wikihop에서 평균 8.4회, HotpotQA에서 9.5회로 비교적 효율적인 수준을 유지하였다. 이러한 결과는 MIGRES가 불필요한 지식 탐색을 줄이면서도 효과적인 추론 경로를 구성할 수 있음을 보여준다.

<br/>
<br/>

# Conclusion
## Contribution 
이 논문은 LLM이 정보 추출 능력과 함께 부족한 정보를 인식하는 능력을 가지고 있다는 것을 실험적으로 검증했다(평균 95.6% 정확도). 이러한 발견을 바탕으로 MIGRES 프레임워크를 제안했으며, 이는 부족한 정보 식별을 통해 targeted query 생성과 후속 지식 검색을 안내하는 방식으로 RAG의 효과를 향상시킨다. 또한 sentence-level re-ranking filtering 전략을 도입하여 무관한 콘텐츠를 제거하고 LLM의 정보 추출 능력과 결합하여 전체 RAG 시스템의 효율성을 증대시켰다. 실험 결과 제안된 방법은 최신 baseline들과 비교해 우수하거나 경쟁력 있는 성능을 달성하면서도 외부 지식에 대한 토큰 소비를 현저히 줄였다.

## Limitations
더 강력한 dense retrieval 방법(예: BGE)을 사용했을 때 오히려 성능이 BM25보다 약간 떨어지는 현상이 관찰되었다. 이는 dense retrieval이 의미적으로는 관련성이 있지만 실제로는 유용한 정보가 부족한 지식을 회상하는 경향이 있어 정보 추출의 정확도를 감소시키고 비효율적인 반복을 초래하기 때문이다. 또한 Musique 데이터셋에서 상대적으로 낮은 성능을 보였는데, 이는 해당 데이터셋의 질문들이 더 모호하고 애매해서 Retriever가 검색 코퍼스에서 관련 지식을 효과적으로 회상하기 어렵기 때문이다.
