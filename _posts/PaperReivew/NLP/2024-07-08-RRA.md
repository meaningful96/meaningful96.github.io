---
title: "[논문리뷰]Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2024-07-08
last_modified_at: 2024-07-08
---

*Wu, Y., Hu, N., Bi, S., Qi, G., Ren, J., Xie, A., & Song, W.* (2023, September 20). **Retrieve-Rewrite-Answer: A KG-to-Text enhanced LLMS framework for knowledge Graph question answering**. arXiv.org. [https://arxiv.org/abs/2309.11206](https://arxiv.org/abs/2309.11206)

# Problem Statment
<span style="font-size:110%">**1. LLM은 여전히 long-tail knowledge등의 모든 지식을 저장하는 것이 불가능하다**.</span>  
LLM은 여전히 long-tail knowledge등의 모든 지식을 저장하는 것이 불가능하기 때문에 specialized knowledge를 다루는 Knowledge-Intensive Task에서 낮은 성능을 보인다. 이는 **Hallucination**과 **Factual Inaccuracy**로 이어진다. 

<span style="font-size:110%">**2. 많은 양의 자원을 요구함**.</span>  
LLM을 지속적으로 pre-training하는 연구들은 방대한 양의 corpora를 학습한다. 하지만, 이는 **매우 많은 양의 text data, 컴퓨팅 자원, 학습 시간을 요구**한다는 단점이 있다.

<span style="font-size:110%">**3. 프롬프트 엔지니어링은 지식 표현의 중요성을 간과한다**.</span>
3.	일부 LLM 연구들은 지식을 보다 직접적인 방식으로 풍부하게 하기 위해, 질문과 관련된 사실적 정보를 질문 앞에 추가하여 지식이 보강된 프롬프트를 구성한다. 예를 들어 프롬프트 엔지니어링이 이에 해당한다. 이는 성능측면에서 성공적이고 cost-effective하다. 하지만, 이 방식은 **지식의 표현(knowledge representation)의 중요성을 간과**한다. (e.g., 단순 정보 추가로 인한 맥락과 관계성 부족, 효율적 활용의 한계)

# Related Work
<span style="font-size:110%">**1.	KG-Augmented LLM for KGQA**</span>  
- 사전에 정의된 템플릿을 통해 Triple과 Question의 textual representation을 knowledge-augmented prompt로 변환한다. 이렇게 변환된 프롬프트를 QA를 위한 LLM에 입력시켜 정답을 생성한다.
- 이 논문은 이러한 지식 표현 형식이 KGQA(Knowledge Graph Question Answering) 작업에서 LLM의 성능에 미치는 영향을 고려하지 않았다고 지적하고 있다. 즉, 지식을 어떤 형식으로 표현하여 LLM에 제공하느냐에 따라 성능이 달라질 수 있는데, 이전 연구들은 이 점을 충분히 고려하지 않았다는 것이다.

<span style="font-size:110%">**2. KG-to-Text**</span>  
- GNN 기반 접근법
  - GNN기반의 접근법은 subgraph의 구조적 정보를 효율적으로 인코딩할 수 있다. 이를 위해 더 복잡한 인코더를 설계하는 방향으로 연구가 진행중이다. 하지만, GNN은 locality정보만을 처리할 수 있고, 그래프의 global한 정보를 추출하지는 못한다.
  - 이를 해결하고자 Transformer기반의 아키텍쳐를 사용하여 인코더를 설계하는 연구들이 진행중이다.

- PLMs 기반 접근법
  - KG-to-Text를 end-to-end generation task로 모델링한다. 이 연구들은 모델 아키텍처를 수정하고 구조적 정보를 추출하는 능력을 향상시키기 위한 pre-training tasks을 도입하는 것을 포함한다.

# Method
## 1. Preliminary

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/c9c887e7-599c-4f1a-89e1-9ed46f66120a">
</p>


# Experiments

# Limitations and Contributions
