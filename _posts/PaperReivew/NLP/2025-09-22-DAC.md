---
title: "[논문리뷰]DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression (ACL, 2025)"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-09-22
last_modified_at: 2025-09-22
---

*Yi Zhao, Zuchao Li, Hai Zhao, Baoyuan Qi, and Liu Guoming*. 2025. [**DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression**](https://aclanthology.org/2025.acl-long.952/). In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 19395–19407. https://doi.org/10.18653/v1/2025.acl-long.952


# Problem Statement
이 논문은 Task-augnostic Prompt Compression 문제를 다루며, 긴 입력 프롬프트를 압축해도 정보 손실을 최소화하면서 LLM의 성능을 유지하는 방법을 제안한다.

**[Task-aware 방식의 한계]** 기존 방법들은 질문이나 태스크와 관련된 부분만 압축 대상으로 삼아 높은 성능을 보였으나, 다중 질문·다중 태스크 상황에서는 프롬프트를 반복적으로 압축해야 하고, 장기 대화에서는 사용자의 의도를 명확히 정의하기 어려워 범용성이 떨어진다.

**[Entropy 기반 방식의 한계]** 기존 Task-agnostic 방식은 모델 출력의 로그 확률에서 계산한 **정보 엔트로피**를 기준으로 압축하지만, (i) Attention 메커니즘에서 중요한 토큰을 무시하고, (ii) 기존 방법은 엔트로피를 정적으로 계산하여, 압축 중 토큰이 제거되면서 발생하는 **엔트로피 이동(Entropy Shift)** 현상을 반영하지 못한다.

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure1.png?raw=true">
</p>

DAC의 전체 과정은 입력 프롬프트에서 토큰별 <span style="color:gold">**정보 엔트로피(information entropy)**</span>와 <span style="color:gold">**누적 어텐션 스코어(accumulative attention score)**</span>를 계산한 뒤, 이를 통합한 지표를 기반으로 여러 단계에 걸쳐 동적으로 압축을 수행하는 흐름이다. 먼저 Attention-critical 토큰과 엔트로피 변화를 동시에 고려해 중요도를 평가하고, 각 단계에서 엔트로피를 다시 계산하여 Entropy Shift 문제를 보완한다. 이어서 연속된 토큰의 과도한 삭제를 방지하는 제약을 적용하며, 최종적으로 압축된 프롬프트를 생성한다. 이 과정을 통해 DAC는 정보 손실을 최소화하면서도 효율적이고 범용적인 Prompt Compression을 달성한다.

## Problem Formulation
<center>$$\min_{x, \tau} \mathcal{D}(P(\widetilde{y}\mid \widetilde{x}), P(y \mid x))$$</center>

원래 입력 토큰을 $$x=\{x_i\}_{i=1}^L$$ 압축된 입력을 $$\widetilde{x} = \{\widetilde{x_i} \}_{i=1}^L$$라 할 때, 압축 비율은 $$\tau = \tilde{L}/L$$이다. 원래 출력 $$P(y \mid \x)$$와 압축된 출력 $$P(\tilde{y} \mid \tilde{x})$$간 분포 차이를 최소화하는 것이다.

## Information Entropy
<center>$$I_{t}(x) = -\log_2P(x_t \mid x_0, x_1, \cdots, x_{t-1})$$</center>

토큰 $$x_t$$의 정보 엔트로피(information entropy)는 위의 식처럼 정의된다. <span style="color:gold">**확률 분포가 뚜렷할수록 엔트로피가 낮아지고, 이렇게 엔트로피가 낮아진 토큰은 정보량이 적다고 판단**</span>된다.

## Attention Score
Transformer의 query, key 행렬을 이용하여 누적 어텐션 스코어(accumulative attention score)를 정의한다.
- 레이어 $$i$$, 헤드 $$j$$의 어텐션 행렬
- 
<center>$$\text{SoftMax} \Big( \frac{Q_{ij}K_{ij}^{\top}}{\sqrt d_h} \Big) \in \mathbb{R}^{n \times n}$$</center>

- 토큰별 누적 어텐션 스코어
  
<center>$$s_v^{ij} = \displaystyle\sum_{u=1}^n q_{uv}, \quad F_{score}^{ij} = (s_1^{ij}, \cdots, s_n^{ij})$$</center>

- 전체 평균
  
<center>$$F_{score} = \frac{1}{MN} \cdot \displaystyle\sum_{i=1}^M \displaystyle\sum_{j=1}^N F_{score}^{ij}$$</center>

## Integrated Compression Metric
정보 엔트로피와 어텐션 스코어를 결합하는 직관적인 방법을 두 가지 제안한다.
- 가산형(Additive)
  
<center>$$M_t^a = (1-\alpha)\cdot I_t(x) + \alpha \cdot s_t$$</center>

- 곱셈형(Multiplicative)
  
<center>$$M_t^{m} = I_t (x) \cdot s_t$$</center>

이 때, $$t$$는 토큰의 위치를 나타내는 index이고, $$M$$에 윗첨자에 해당하는 $$a, m$$은 각각 가산형(**a**dditive)와 곰셈형(**m**ultiplicative)를 의미한다. 위 결합 메트릭에서 $$I_t(x)$$는 토큰 $$x_t$$가 생성될 때의 **정보 엔트로피**이며, 각 토큰의 “**불확실성(정보량)**”을 측정하는 값이다. $$s_t$$는 $$i$$번째 레이어의 $$j$$번째 어텐션 헤드의 정규화된 어텐션 행렬로부터 산출한 **누적 어텐션 스코어(accumulative attention socre)**로, <span style="color:gold">**각 토큰이 시퀀스 전반에서 참조받는 중요도**</span>를 나타낸다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure2.png?raw=true">
</p>

논문에서는 정보 엔트로피와 누적 어텐션 스코어를 모두 활용해야하는 명확한 근거를 제신한다. Observation 1은 토큰별 누적 어텐션 점수와 엔트로피 사이의 상관이 매우 낮음(평균 피어슨 상관계수 약 0.095)과, 어텐션 핵심 토큰을 무시하면 랜덤 압축보다도 성능이 더 크게 저하되는 현상을 보인다. 즉, <span style="color:gold">**엔트로피만으로는 어텐션에서의 중요 토큰을 보전하지 못하므로 두 신호를 통합하는 메트릭이 필요**</span>하다.

## Dynamic Compression with Entropy Shift
단일 스텝 압축 대신 여러 단계로 나누어 압축하며, 각 단계마다 엔트로피를 다시 계산한다. 연속된 토큰 제거를 제한하여 엔트로피 이동 문제를 방지한다.

- 동적 압축률
<center>$$\Delta \tau = \tau^{1/D} + \Delta P$$</center>

- 임계값 결정
<center>$$T_{\Delta \tau} = \text{np.percentile}([M_0], \cdots, [M_n], \Delta \tau)$$</center>

- 압축 결과 집합

<center>$$\widetilde{x} = \{ \widetilde{x_i} \mid M(\widetilde{x}_i) \geq T_{\Delta \tau} \vee \widetilde{x}_{i-1} \notin \widetilde{x} \}$$</center>

- $$\tau=\frac{\widetilde{L}}{L}$$: 목표 압축률(남기는 비율)
- $$D$$:  동적 반복 횟수이다.
- $$[M_0],\ldots,[M_n]$$: 현 단계에서 모든 토큰에 대해 계산된 통합 점수 배열(예: $$M^a$$ 또는 $$M^m$$)을 의미
- $$T_{\Delta\tau}$$: $$\text{np.percentile}(\cdot,\Delta\tau)$$로 계산되며, 이번 단계에서 **제거 후보와 보존 후보를 가르는 임계값**

이후 규칙은 “낮은 점수 토큰을 제거하되 연속 제거를 금지”하는 것이다. 이전 토큰 $$\widetilde{x}_{i-1}$$이 이미 제거되었다면(즉,  $$\widetilde{x}_{i-1}\notin \widetilde{x}$$) 현재 토큰은 강제로 보존하여 연속 삭제를 차단한다. 이때 $$\Delta P$$는 바로 이 연속-삭제 제한 때문에 삭제 대신 보존된 토큰의 비율로, 다음 단계의 유효 압축률 $$\Delta\tau$$에 가산되어 단계별 압축 강도를 입력-적응적으로 보정한다. 

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure3.png?raw=true">
</p>

각 단계의 시작 전에 **sLLM**(e.g., Qwen2-0.5B)으로 모든 토큰의 $$I_t(x)$$를 재계산하여, 직전 단계의 삭제로 인해 **엔트로피 분포가 변형**된 상황을 반영한다. 이러한 다단계·재평가·연속 삭제 제한이라는 설계는 **Observation 2**에서 직접 파생되었다. Observation 2는 토큰 선행부 삭제만으로도 잔여 토큰의 엔트로피가 크게 **이동(Shift)**하며, 압축률이 높아질수록 원본 대비 엔트로피 상관이 뚜렷하게 **하락**함을 보인다 .

따라서 <span style="color:gold">**단일 스냅샷 엔트로피로 일괄 압축하면 손실이 커지므로**</span>, 단계별 재계산과 연속-삭제 제한, 그리고 $$\Delta P$$에 의한 다음 단계 압축률 보정이 필요하다는 결론에 이른다. 이 동적 절차는 LongBench/GSM8K/BBH 전반에서 **엔트로피 이동의 악영향을 상쇄**하며, 특히 높은 압축률 구간에서 **성능 유지력 향상**으로 이어진다.

## Algorithm: Dynamic Attention-aware Prompt Compression (DAC)
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure4.png?raw=true">
</p>

알고리즘 1은 먼저 입력 프롬프트의 모든 토큰에 대한 누적 어텐션 점수를 계산한 뒤, **동적 반복 횟수 $$D$$**만큼 단계를 나누어 진행하면서 매 단계마다 실제 상황에 맞게 엔트로피를 재평가하고 **통합 점수(가산형/곱셈형)**를 갱신하여 percentile 임계값으로 제거 대상을 고르는 절차로 구성된다.

**[Line 3~6]** 구체적으로, 각 단계에서 목표 압축률을 $$\Delta\tau=\tau^{1/D}+\Delta P$$로 보정하여 이번 단계의 삭제 강도를 정하고, sLM으로 재계산한 $$I_t(x)$$와 미리 산출한 $$s_t$$를 결합해 $$M_t$$를 얻은 다음, $$T_{\Delta\tau}$$ 이상인 토큰은 보존하고 미만인 토큰은 제거 후보로 둔다.

**[Line 7~16]** 이때 **연속 삭제를 금지**하기 위해 직전 토큰이 이미 제거된 경우 현재 토큰은 **강제로 보존**하여 엔트로피 이동에 의한 오판을 줄이고, 이렇게 **보존으로 전환된 비율을 $$\Delta P$$로 집계해 다음 단계의 $$\Delta\tau$$에 반영**함으로써 단계별 압축률을 입력 적응적으로 조정한다.

모든 단계를 마치면 연속 삭제 제한과 동적 보정이 반영된 압축 프롬프트 $$\tilde{x}$$가 출력되며, 전체적으로 **어텐션-중요 토큰 보존 + 엔트로피 이동 대응**을 통해 높은 압축률에서도 성능 저하를 최소화한다.

<br/>
<br/>

# Experiments
## Main Results
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure5.png?raw=true">
</p>

LongBench에서 DAC는 두 압축률 모두에서 일관된 우위를 보였다. Qwen2-7B 기준 $$\tau=0.5$$에서 전체 평균 37.76으로 Selective-Context(33.50), LLMLingua(33.73), LLMLingua-2(36.43)를 모두 상회했고, $$\tau=0.2$$에서도 평균 32.63으로 비교군 대비 가장 높거나 동급의 성능을 유지했다. 단일 문서 QA·요약·few-shot에서 격차가 특히 뚜렷하며, 이는 <span style="color:gold">**어텐션 핵심 토큰 보존과 엔트로피 이동을 단계별로 교정하는 설계가 단일 스텝·정적 엔트로피 기반 방식의 손실을 줄였음**</span>을 시사한다.

## Ablation Study
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure6.png?raw=true">
</p>

단일 문서 QA에서 구성요소 제거 실험은 세 모듈이 모두 기여함을 보여줍니다. DAC 전체: 32.81 대비, <span style="color:gold">**어텐션 인지 메트릭을(attention-aware metric) 제거하면 28.16으로 가장 크게 하락**</span>했고, 동적 절차 제거 시 29.84, 연속 삭제 제한을 풀면 31.88로 소폭 하락했다. 이는 Observation 1(어텐션 핵심 토큰을 놓치면 성능 급락)과 Observation 2(압축 진행 중 엔트로피 분포가 크게 이동)의 경험적 근거를 뒷받침하며, 통합 점수＋동적 재평가＋연속 삭제 제한이 상호보완적으로 성능을 지탱한다는 점을 의미한다.

## Comparison Results on LongBench
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure7.png?raw=true">
</p>

LLaMA3 계열로 교체해도 DAC의 우위는 유지되었다. LLaMA 3.2-1B(SLM)/LLaMA 3.1-8B(LLM) 설정에서 $$\tau=0.5$$일 때 평균 점수는 DAC 35.42로, LLMLingua-2(34.82), LLMLingua(33.31), Selective-Context(32.16)보다 높았다. 단일/멀티 문서 QA, 요약, few-shot 전 범주에서 동급 최상 또는 동급 성능을 보이며, 이는 <span style="color:gold">**작은 모델 엔트로피로도 신뢰 가능한 압축 결정을 내릴 수 있고, 설계가 모델 불가지론적임을 시사**</span>한다.

## Inference Efficiency
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure8.png?raw=true">
</p>

GovReport 샘플**(입력 12,908토큰, 출력 500, $$\tau=0.2$$)**에서 DAC는 동적 절차로 압축 시간이 상대적으로 길지만, 전체(압축＋추론) 시간은 풀 프롬프트 대비 크게 단축되었다. LLMLingua/LLMLingua-2/Selective-Context 대비  DAC는<span style="color:gold">**압축 시간 오버헤드는 있으나, 추론 단계의 토큰 수 감소가 지배적이어서 총 시간은 유리**</span>한 것을 알 수 있다. 특히 실사용 LLM 파라미터가 7B를 크게 상회할수록 Self-Attention 비용이 급증하므로, DAC의 시간·메모리 이득이 더 커짐을 의미한다.

## Compression Rates
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.22%5DDAC/DAC_figure9.png?raw=true">
</p>

GSM8K에서 압축 강도를 변화시킨 결과, 중·저압축 구간($$\tau \geq 0.5$$)**에서는 정확도 저하가 작고, 압축 강도가 큰 구간에서도 실용 가능한 정확도를 유지했다. 이는 <span style="color:gold">**단계별 엔트로피 재계산**</span>과 <span style="color:gold">**연속 삭제 제한**</span>이 고압축 시 발생하는 <span style="color:gold">**엔트로피 이동과 문맥 붕괴를 제어**</span>해, 효율-성능 트레이드오프 곡선을 우호적으로 이동시킨다는 점을 보여준다.

<br/>
<br/>

# Conclusion
본 논문은 동적 어텐션 인지(Dynamic Attention-aware) 프롬프트 압축을 통해 정보 엔트로피와 누적 어텐션 신호를 통합하고, 압축 과정에서 발생하는 엔트로피 이동을 단계적으로 보정함으로써 높은 압축률에서도 성능 저하를 최소화하는 실용적 방법을 제시하였다. 다양한 벤치마크와 서로 다른 LLM 계열에서 일관된 성능 우위를 보였으며, 추가되는 압축 오버헤드를 상회하는 추론 시간·메모리 절감 효과를 확인하여 실제 배치 환경에서의 유효성을 입증하였다.

**Contribution**
- 정보 엔트로피와 누적 어텐션 점수를 결합한 통합 압축 메트릭을 제안하여 엔트로피 단독 기준의 한계를 보완하였다.
- 단일 스텝 대신 다단계 동적 절차를 도입하고, 매 단계에서 **엔트로피를 재계산하며 연속 삭제를 제한**함으로써 엔트로피 이동에 강건한 압축(robust compression)을 구현하였다.
- LongBench, GSM8K, BBH 등 다양한 과제와 Qwen2·LLaMA3 등 서로 다른 모델 계열에서 일관된 성능 향상을 보여 모델 불가지론적 특성을 입증하였다.
- 압축에 따른 추가 연산이 존재하더라도 전체 추론 시간 및 메모리 사용량 절감이 더 커 실사용 관점에서 효율성을 확보하였다.

**Limitations**
- 모든 레이어와 헤드의 어텐션 행렬을 요구하므로 대표 어텐션만 선별·결합할 수 있는 효율적 설계가 필요하다. 또한 Flash Attention 등 고효율 어텐션 방식과의 직접적 호환성이 낮아 추가 점수 계산 비용이 발생할 수 있다.
- 매우 긴 컨텍스트에서는 **동적 반복 횟수의 상한 때문에 단계별 재평가의 세밀도가 떨어져 국지적 엔트로피 변화를 놓칠 수 있으며**, **정보 밀도에 따라를 적응적으로 조절하는 방안이 필요**하다.
