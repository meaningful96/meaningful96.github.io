---
title: "[논문리뷰]LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders(COLM, 2024)"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-09-17
last_modified_at: 2025-09-17
---

*Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy*. 2024. [*LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders**](https://arxiv.org/abs/2404.05961). arXiv:2404.05961 [cs.CL] https://arxiv.org/abs/2404.05961

# Problem Statment
**[Causal Attention 제약]** Decoder-only LLM은 causal attention을 사용하여 각 토큰이 이전 토큰만 볼 수 있어, 시퀀스 전체의 문맥 정보를 반영하는 데 제한이 있었다. 이로 인해 풍부한 컨텍스트 임베딩 생성에 적합하지 않았다.

**[복잡하고 비용 높은 적응(adaptation)]** 기존의 텍스트 임베딩 학습은 대규모 weakly supervised & fully-supervised learning 파이프라인을 거쳐야 했으며, 종종 GPT-4와 같은 외부 생성 데이터를 필요로 했다. 이는 데이터 및 연산 자원이 많이 소모되는 방식이었다.

**[Causal Mask 제거의 제한적 연구]** 일부 연구는 supervised fine-tuning 중 특정 레이어에서 causal mask를 제거하거나 입력을 복제하여 future token을 참조하도록 했으나, 이는 계산 비용이 크거나 제한된 태스크에만 효과적이었다.

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.17%5Dllm2vec/llm2vec_figure1.png?raw=true">
</p>

Figure 1은 LLM2Vec의 세 가지 핵심 단계를 보여준다. 먼저, 'Enabling Bidirectional Attention(Bi)' 단계는 기존 디코더 전용 언어 모델의 인과적 어텐션 제약을 극복하기 위해 Bidirectional Attention을 활성화하는 과정이다. 다음으로, 'Masked Next Token Prediction(MNTP)' 단계는 마스킹된 다음 토큰 예측 학습을 통해 모델이 Bidirectional Attention을 활용하도록 적응시킨다. 마지막으로, 'Unsupervised Contrastive Learning' 단계에서는 비지도 대조 학습(SimCSE)과 평균 풀링(mean pooling)을 적용하여 더 나은 시퀀스 표현을 학습하게 된다.

## Enabling Bidirectional Attention
<p align="center">
<img width="300" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.09.17%5Dllm2vec/llm2vec_figure2.png?raw=true">
</p>

첫 번째 단계는 디코더 전용 LLM의 본질적인 한계인 인과적 어텐션 마스크를 극복하는 데 중점을 둔다. 인과적 어텐션 마스크(causal attention mask)는 토큰이 이전 토큰에만 주의를 기울이도록 제한하여, 전체 시퀀스에 걸친 문맥적 정보를 완전히 파악하기 어렵게 만든다. LLM2Vec은 이 어텐션 <span style=”color:gold”>**마스크를 모든 토큰이 시퀀스 내의 다른 모든 토큰에 접근할 수 있도록 하는 전체 1 행렬(all-ones matrix)로 대체**</span>한다. 이로써 모델은 양방향 LLM으로 변환된다. 그러나 이 단순한 접근 방식은 모델이 미래 토큰에 주의를 기울이도록 학습되지 않았기 때문에, 단독으로 사용하면 성능이 저하될 수 있다.

<br/>
<br/>

# Experiments

<br/>
<br/>

# Conclusion
