---
title: "[논문리뷰]Reformer: The Efficient Transformer"

categories: 
  - NR
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-11-05
last_modified_at: 2023-11-05
---
*Kitaev, Nikita, et al. “[Reformer: The Efficient 트랜스포머](https://arxiv.org/abs/2001.04451).” ArXiv:2001.04451 [Cs, Stat], 18 Feb. 2020, arxiv.org/abs/2001.04451.*

# Problem Statement

## Drawbacks of Vanilla 트랜스포머
<span style = "font-size:110%"><b>1. Attention 구조에 의한 메모리 문제</b></span>  

입력으로 길이가 $$L$$인 Sequence를 받는데, 시간 복잡도와 공간 복잡도는 $$O(L^2)$$이 된다. 트랜스포머에서 Attention 연산은 **Dot-Product Attnetion**이다. 수식으로 좀 더 구체화하면 다음과 같다.

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

- Query($$Q$$): 영향을 받는 단어 (객체)
- Key($$K$$): 영향을 주는 단어 (주체)
- Value($$V$$): Key에 대응되는 값

시간 복잡도(Time Complexity)가 $$Q$$의 크기와 $$K$$의 크기에 곱에 비례한다. 이는 데이터의 길이가 10배 길어지면 연산 자체는 100배 더 많아지는 것과 같다. 

<span style = "font-size:110%"><b>2. N-stacked Residual Connection에 의한 메모리 문제</b></span>  
Vanilla 트랜스포머의 Encoder와 Decoder의 Layer수가 너무 많다는 문제점이 있다. N개의 Layer 층을 이루기 때문에 N배 많은 메모리를 필요로 한다. 마지막 Layer에서부터 시작 Layer까지 backpropagation을 하면서 미분값을 구하고, parameter를 업데이트 히는데, 층 수가 많아지면 많아질수록 연산에 더 많은 메모리가 요구된다. 이는 N개의 Layer가 쌓이면 그만큼 Residual Connection도 늘어나므로, 연산이 증가함을 의미한다.

<span style = "font-size:110%"><b>3. Feed Forward Layer에 의한 메모리 문제</b></span>  
Feed Forward Layer(FFN)이 attention activation의 깊이보다 더 클 수 있다. 실제로 FFN이 각 Attention Layer의 출력에 모두 적용이 되어야 한다. 이 구조가 차지하는 메모리는 따라서 Sequence의 길이($$L$$)와 해당 layer의 입출력 차원의 곱에 비례한다. 문제는 보통 이 때 사용되는 입출력 차원은 대개 모델 임베딩의 차원에 비해 크다는 것이다. 

$$FFN(x) = max(0, x \cdot W_1 + b_1) \; \cdot \; W_2 + b_2$$

트랜스포머에서도 입력 시퀀스는 512개의 토큰을 maximum으로 하지만, 실제로 입력 차원수는 2048이다. 따라서 데이터의 길이가 충분히 길면 이 FFN구조가 차지하는 메모리도 무시할 수 없게 된다. 즉, Sequence의 길이에 보통 4배 정도로 Layer의 차원수를 설정하고, Sequence가 길어지면 그에 따라 계산해야하는 파라미터수가 늘어나므로, 이것의 계산 복잡도가 시퀀스에 비례해 커지는 것을 말한다.

<br/>
<br/>

# Related Work
## 1. Understanding about Memory

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/6c5b4807-708c-46cd-a07d-8f6b99815e4c">
</p>  

데이터를 load하고, 모델에 구조에 맞게 forward-propagation(순전파), back-propagation(역전파)의 과정을 거친 후 모델의 파라미터가 업데이트된다. 이 때, 그림에서와 같이 역전파 이전에는 연산의 중간 결과물인 $$b_1, b_2$$등을 저장하고 있는다. 이는 메모리 측면에서 단점으로 작용할 수 있으며, 트랜스포머에서 여실히 보여주게된다. 

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b9c7945d-ee6c-4bac-8ea1-a74c47c6251c">
</p>  

트랜스포에서 학습 시 Memory를 증가시키는 요인은 역전파하기 전까지 중간결과물($$b_n$$)을 저장해야하기 때문이다. 즉, <span style = "color:gold">**batch size**</span>가 메모리 사용량에 지대한 영향을 미친다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/e075ec63-2b2d-4c0d-8f43-aa6ef2649cb6">
</p>  

또한 트랜스포머에서 batch size이외에도 메모리 사용량을 증가시키는 요소는 여러가지가 더 있다. 바로 Layer 수에 영향을 받는 <span style = "color:gold">**모델 깊이**</span>, Hidden size에 해당하는 <span style = "color:gold">**모델 넓이**</span> 그리고 입력 시퀀스의 길이에 해당하는 <span style = "color:gold">**문장 길이**</span>이다. 따라서 메모리의 총량은 **(모델깊이 X 모델넓이 X 문장길이 X Batch)**에 의해 결정된다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/32086959-b2c6-434f-bf66-29618e82aac9">
</p>  

이를 그림으로 표현하면 위와 같다. 이처럼, 학습 시 메모리 사용량을 줄이려면 (모델깊이 X 모델넓이 X 문장길이 X Batch)중 파라미터를 일부 조정하면서 떨어트릴 수 있다. 하지만, Reformer에서 말하는 것은 단순히 파라미터 조정을 통해 메모리 사용량을 줄이는 것이 아니다. Reformer가 보여주는 것은 바로 **메모리 효율성**이다. 그러면 하이퍼파라미터를 조절하지않고 어떻게 하면 효율적으로 만들 수 있는지가 관건이다. 그 답은 그리고 앞에서 나온 것과 같이 <u><b>연산의 중간 산물</b></u>과 관련이 있다. 

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/bab8baf7-b0e4-4df3-8851-04d1899e4cb5">
</p>  



# Reference
[Blog: Reformer, The Efficient 트랜스포머](https://velog.io/@nawnoes/Reformer-%EA%B0%9C%EC%9A%94)    
[Blog: 꼼꼼하고 이해하기 쉬운 Reformer 리뷰](https://tech.scatterlab.co.kr/reformer-review/)  
[Reformer: The Efficient Transformer](https://www.youtube.com/watch?v=6ognBL6DEYM)  
