---
title: "[논문리뷰]Reformer: The Efficient Transformer"

categories: 
  - NR
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-11-05
last_modified_at: 2023-11-05
---
*Kitaev, Nikita, et al. “[Reformer: The Efficient 트랜스포머](https://arxiv.org/abs/2001.04451).” ArXiv:2001.04451 [Cs, Stat], 18 Feb. 2020, arxiv.org/abs/2001.04451.*

# Problem Statement

## Drawbacks of Vanilla 트랜스포머
<span style = "font-size:110%"><b>1. Attention 구조에 의한 메모리 문제</b></span>  

입력으로 길이가 $$L$$인 Sequence를 받는데, 시간 복잡도와 공간 복잡도는 $$O(L^2)$$이 된다. 트랜스포머에서 Attention 연산은 **Dot-Product Attnetion**이다. 수식으로 좀 더 구체화하면 다음과 같다.

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

- Query($$Q$$): 영향을 받는 단어 (객체)
- Key($$K$$): 영향을 주는 단어 (주체)
- Value($$V$$): Key에 대응되는 값

시간 복잡도(Time Complexity)가 $$Q$$의 크기와 $$K$$의 크기에 곱에 비례한다. 이는 데이터의 길이가 10배 길어지면 연산 자체는 100배 더 많아지는 것과 같다. 

<span style = "font-size:110%"><b>2. N-stacked Residual Connection에 의한 메모리 문제</b></span>  
Vanilla 트랜스포머의 Encoder와 Decoder의 Layer수가 너무 많다는 문제점이 있다. N개의 Layer 층을 이루기 때문에 N배 많은 메모리를 필요로 한다. 마지막 Layer에서부터 시작 Layer까지 backpropagation을 하면서 미분값을 구하고, parameter를 업데이트 히는데, 층 수가 많아지면 많아질수록 연산에 더 많은 메모리가 요구된다. 이는 N개의 Layer가 쌓이면 그만큼 Residual Connection도 늘어나므로, 연산이 증가함을 의미한다.

<span style = "font-size:110%"><b>3. Feed Forward Layer에 의한 메모리 문제</b></span>  
Feed Forward Layer(FFN)이 attention activation의 깊이보다 더 클 수 있다. 실제로 FFN이 각 Attention Layer의 출력에 모두 적용이 되어야 한다. 이 구조가 차지하는 메모리는 따라서 Sequence의 길이($$L$$)와 해당 layer의 입출력 차원의 곱에 비례한다. 문제는 보통 이 때 사용되는 입출력 차원은 대개 모델 임베딩의 차원에 비해 크다는 것이다. 

$$FFN(x) = max(0, x \cdot W_1 + b_1) \; \cdot \; W_2 + b_2$$

트랜스포머에서도 입력 시퀀스는 512개의 토큰을 maximum으로 하지만, 실제로 입력 차원수는 2048이다. 따라서 데이터의 길이가 충분히 길면 이 FFN구조가 차지하는 메모리도 무시할 수 없게 된다. 즉, Sequence의 길이에 보통 4배 정도로 Layer의 차원수를 설정하고, Sequence가 길어지면 그에 따라 계산해야하는 파라미터수가 늘어나므로, 이것의 계산 복잡도가 시퀀스에 비례해 커지는 것을 말한다.

<br/>
<br/>

# Related Work
## 1. Understanding about Memory

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/6c5b4807-708c-46cd-a07d-8f6b99815e4c">
</p>  

데이터를 load하고, 모델에 구조에 맞게 forward-propagation(순전파), back-propagation(역전파)의 과정을 거친 후 모델의 파라미터가 업데이트된다. 이 때, 그림에서와 같이 역전파 이전에는 연산의 중간 결과물인 $$b_1, b_2$$등을 저장하고 있는다. 이는 메모리 측면에서 단점으로 작용할 수 있으며, 트랜스포머에서 여실히 보여주게된다. 

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b9c7945d-ee6c-4bac-8ea1-a74c47c6251c">
</p>  

트랜스포에서 학습 시 Memory를 증가시키는 요인은 역전파하기 전까지 중간결과물($$b_n$$)을 저장해야하기 때문이다. 즉, <span style = "color:gold">**batch size**</span>가 메모리 사용량에 지대한 영향을 미친다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/e075ec63-2b2d-4c0d-8f43-aa6ef2649cb6">
</p>  

또한 트랜스포머에서 batch size이외에도 메모리 사용량을 증가시키는 요소는 여러가지가 더 있다. 바로 Layer 수에 영향을 받는 <span style = "color:gold">**모델 깊이**</span>, Hidden size에 해당하는 <span style = "color:gold">**모델 넓이**</span>, 그리고 입력 시퀀스의 길이에 해당하는 <span style = "color:gold">**문장 길이**</span> 이다. 따라서 메모리의 총량은 **(모델깊이 X 모델넓이 X 문장길이 X Batch)**에 의해 결정된다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/32086959-b2c6-434f-bf66-29618e82aac9">
</p>  

이를 그림으로 표현하면 위와 같다. 이처럼, 학습 시 메모리 사용량을 줄이려면 (모델깊이 X 모델넓이 X 문장길이 X Batch)중 파라미터를 일부 조정하면서 떨어트릴 수 있다. 하지만, Reformer에서 말하는 것은 단순히 파라미터 조정을 통해 메모리 사용량을 줄이는 것이 아니다. Reformer가 보여주는 것은 바로 **메모리 효율성**이다. 그러면 하이퍼파라미터를 조절하지않고 어떻게 하면 효율적으로 만들 수 있는지가 관건이다. 그 답은 그리고 앞에서 나온 것과 같이 <u><b>연산의 중간 산물</b></u>과 관련이 있다. 

<br/>
<br/>

# Method
## 1. Locality-Sensitive Hashing (LSH)

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/9fd1037a-c32f-4919-a20d-c27b9f18180b">
</p>

Hashing은 해시 function(해시 함수) algorithm을 말하며 임의의 길이의 데이터를 고정된 길이의 데이터로 매핑하는 함수를 해시 함수이라고 한다. 데이터를 미리 Hashing해두면 해시값만으로 쉽게 데이터를 찾을 수 있다. 보통 해시값은 연결된 데이터와 전혀 관련이 없을 때가 만고, 그렇기 때문에 전체 데이터 분포에서 상대적 위치를 확인하거나 한 데이터와 가장 가까운 다른 데이터를 찾는 등 데이터에 대한 비교 분석을 할 때 반드시 실제 데이터 값을 비교하는 연산이 필요하다. 

이 논문에서 Hashing을 사용함에 있어 핵심은, <span style = "color:gold"><b>가까운 거리에 위치한 데이터들은 가까운 해시값을 갖도록 구성</b></span>하는 것이다. 이러면 **비교 연산을 해시값에 대한 연산으로 근사**할 수 있다. 이렇게 가까운 값들을 가까운 해시값을 가지도록 Hashing하는 것을 Locality-Sensitive Hashing(LSH)라 한다. LSH를 보기 전 기존 트랜스포머의 Attention을 먼저 알아야 한다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/5e49bf9e-ba75-4541-bef6-5d2850a45d96">
</p>

Vanilla 트랜스포머에서는 세 가지 종류의 Attention이 존재한다. 바로 Encoder의 Self-Attention과 Decoder의 Masked Self-Attention, 그리고 Encoder의 최종 레이어의 출력과 디코더의 Masked Self-Attention을 거친 출력과 Cross Attention을 하는 Encoder-Decoder Attention이다. 또한 이 세 가지 Attention은 기본적으로 Dot-Product Attention이다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/16e8996f-72cb-4268-b11a-8abc7ca35093">
</p>

이때 `it`의 Attention 중 5개 ('The', 'animal', 'street', 'it', '.' )  를 제외하면 제대로 그 영향력이 전달되지 않은 것을 볼 수 있다. 그래서 `didn't`나 `cross`등과 같은 단어들은 연산 결과 Attention을 받지 않았기 때문에 실제로는 Sparse하게 된다. 이를 바꿔말하면, 우리는 Query(Q)에 대해 밀접한 연관을 가진 K에 대해서만 Attention을 하면 된다. 하지만 기존 트랜스포머는 이 부분을 비효율적으로 찾고, 모든 단어와 단어 사이 Attention을 계산하기 때문에 비효율 적인 것이다. Reformer에서는 LSH를 통해 이 문제를 해결하였다. 그러면 여기서 문제는 어떻게 LSH를 기계적으로 수행할 수 있을지이다.

### 1) Simple LSH

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/a2159ae7-6f2d-4ae8-98bf-adc23e55107c">
</p>

LSH는 고차원 데이터에 대해 *nearest neighbor* 알고리즘을 이용해 효율적으로 근사하는 방법이다. 두 점 $$p, \; q$$가 충분히 가깝다면 해시 함수을 거친 결과가 $$해시(p) == 해시(q)$$일 것이다. 임의의 직선 $$h1, \; h2, \; h3$$를 그어 영역에 따라 0과 1로 분류 한다. 각 포인트는 3개의 해시 값을 가지게 되고, 그 해시값이 버켓이라고 보면 된다. 이때 각 충분히 가까운 포인트는 거의 같은 버켓에 들어갔을음 볼 수있다.

<br/>

### 2) Angular LSH
Reformer에서 실제로 사용한 방법은 **Angular LSH**이다. 이 방법은 <u>방향 성분만을 활용하여 해시값을 생성</u>한다. 전체적인 과정은 다음과 같다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/e1596ed3-d49d-4e42-a38b-5be2b43f3946">
</p>

1. 전체 데이터 포인트들의 벡터를 단위 구면에 맵핑한다. 이러면 전체 데이터 포인트를 오직 **각도**만 사용해서 기술할 수 있다. ($$r=1$$인 구면 좌표계)
2. 이제 각 각도가 어느 사분면에 있는지 확인한다. 비슷한 데이터들은 같은 사분면에 있다. 따라서 사분면의 번호를 해시값으로 사용한다면, 비슷한 데이터들을 가깝게 구성할 수 있다.
3. 이제 사상한 구면을 필요한 만큼 임의로 회전시킨다. 데이터가 가까우면 가까울수록 전체 해값을 공유할 가능성이 높아지고, 충분히 많은 해시값을 사용하면 데이터를 구별하는 변별력이 생긴다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/a2f40798-f545-4320-b3af-3b6fb4407a40">
</p>

예를 들어, 2차원 임베딩 벡터가 $$X_1 = (3, 4)$$, $$X_2 = (-12,5)$$가 있다고 가정하고, 이를 반지금 1인 구에 맵핑하면 각각의 점들은 $$X_1^{'} = (\frac{3}{5}, \frac{4}{5})$$와  $$X_2^{'} = ( - \frac{12}{13}, \frac{5}{13})$$이 된다.(그림에서는 이해를 위해 축을 회전) 다음으로 임의의 벡터 $$Y = (4,3)$$이 주어졌다고 가정하면 이 벡터는 원 위에서 $$Y^{'} = (\frac{4}{5}, \frac{3}{5})$$으로 맵핑된다. 똑같이 원을 돌리면서 해시값을 표현해보면 $$Y$$의 해시값은 <b>(1, 4, 2)</b>가 된다.

이 값은 $$X_1$$의 해시갑과 같고, $$X_2$$의 해시값과는 다르므로 데이터 간의 직접 비교 연산 없이 <span style = "color:gold"><b>해시값이 일치하는지 보는 것만으로도 가까운 점들을 선별</b></span>할 수 있다. 하지만 '이런 방식으로 데이터를 저장하면 방향 값은 살아있어도 크기에 대한 값은 손실되는 것이 아닌가?' 라는 의문이 생기고 이에 대한 실험은 Ablation Study에서 기술하였다.

<br/>

### 3) LSH Attention

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/df852a8b-f14a-4561-aedb-821cf1c09ac5">
</p>  

기존의 트랜스포머에서 길이가 $$L$$인 시퀀스가 입력으로 들어오면 그에 따른 $$(Q, K, V)$$값들은 $$(batch, length, d_{model})$$의 차원수를 가진다. 그리고 $$QK^T$$는 $$(batch, length, length)$$의 shape을 가지기 때문에 결론적으로 시간 복잡도와 공간 복잡도가 모두 $$O(L^2)$$이 된다. 이로 인해 Long-Sequence가 들어오면 학습에 드는 비용이 기하급수적으로 증가한다. 하지만, LSH등을 사용하면 가까운 임베딩끼리만 Attention을 진행할 수 있기 때문에 수식이 다음과 같이 바뀌며, 이는 메모리 효율적이다.

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/33d6720b-fb6a-40fa-8ace-1bfe2ad41f59">
</p>

> But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately,
> only calculating $$softmax(\frac{q_iK^T}{\sqrt{d_k}})$$ once in memory, and then re-computing it on the backward pass when needed for gradients.
> This way of computing attention may be less efficient but it only uses memory proportional to length.

최종적으로 LSH attention의 softmax안에 들어가는 식은 다음과 같다. 여기서 $$\mathcal{P_i}$$는 query의 position이고, $$z$$함수는 partition function이다. 여기서 batch까지 고려하면 다시 식은 아래와 같이 바뀌고, 최종적인 attention식이 된다.

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/8e4dfcb5-41b6-440c-ad1c-db77b7536171">
</p>

기존의 Attention에서는 입력으로 들어온 Sequence 내 모든 토큰들이 서로 서로 attention을 진행하기 때문에 히트맵에서 모든 부분에 영향력을 나타내는 정도가, 비록 그 어텐션 값이 작을지라도 표시된다. 반면, LSH Attention의 히트맵 같은 경우 같은 bucket안에 들어있는 단어들끼리만 어텐션을 진행하므로 다음과 같이 히트맵이 변한다. 

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b0b6314f-4bb6-4bd8-a2e3-7d632b82d277">
</p>

<br/>

### 4) Summary

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/cb93866a-32de-461a-9b51-f4ba17d1d79a">
</p>

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/affde840-b1fd-4af9-85cf-8ef0753c4bd0">
</p>

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/f4db7a17-d0ab-4822-815a-e6efae4bfcdc">
</p>

## 2. Reverisble Transformer

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/3f8a6427-396b-4cd1-b3a1-977336d7eb79">
</p>

Reverseible 트랜스포머는 기존에 연구된 [The Reversible Residual Network (RevNet)](https://arxiv.org/abs/1707.04585)의 아이디어를 적용한 것이다. 구조를 요약하면 Encoder, Decoder에서 attention layer와 feed-forward layer를 하나로 묶어버린 것이다. 이렇게하면, 각 layer안에 activation을 저장할 필요가 없다. 따라서 저장 공간을 줄어든다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/76e57d14-d36a-4302-997c-70d485514aa0">
</p>

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/16573956-cdc8-4c5f-b7c0-cec66ad307f6">
</p>

RevNet은 원래 이미지 처리에 사용되는 ResNet 구조에서 메모리를 효율적으로 사용하기 위해 고안되었다. 기존의 ResNet에서 사용되는 계산, 일반적인 Residual Connection의 연산은 위와 같다. 특징적인 것은, 하나의 입력에 대해 하나의 출력이 나온다는 구조이다. 하지만 이러한 방식은 $$x$$에서 $$y$$를 계산할 수는 있어도 $$y$$에서 $$x$$를 역으로 계산해낼 수는 없다. 따라서 들어가는 입력 $$x$$와 출력 $$y$$를 ($$x_1, x_2$$), ($$y_1, y_2$$)쌍 형태로 바꾼다. 이를 도식화하면 다음과 같다.

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/8d695895-1085-421d-a480-36882ee6be96">
</p>

여기서 $$F(\cdot)$$은 Attention Layer를, $$G(\cdot)$$ Feed-Forward Layer이다. 이렇게 식을 나눠서 입출력을 pair로 쓰면 $$y_1$$과 $$y_2$$가 주어졌을 때, $$x_2 = y_2 - G(y_1)$$로 역산할 수 있고, $$x_1 = y_1 = F(x_2)$$로 역산할 수 있다. 이렇게 함으로써 임의의 시점의 출력값을 토대로 그 출력에 대한 입력값을 표현할 수 있다. 따라서, 중간 결과를 저장할 필요가 없이 Forward연산을 반복적으로 적용해 수치적 미분값을 얻을 수 있게 된다. 또한 두 단계의 Sequential한 연산을 하나의 과정으로 합친 것과 같아, activation function을 하나만 사용해도 된다. 즉, <span style = "color:gold">**Activation function에 대한 Parameter를 공유**</span>하여 메모리 효율적이다.

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/2bbb1527-2c05-4db7-823b-66f324b2bc11">
</p>

최종적으로 Reverisble Transformer의 식을 표현하면 위와 같다. Reverisble Transformer에서는 activation을 각 layer마다 저장할 필요가 없기 때문에 $$n_l$$에 관한 부분이 수식에서 없어진다. 이로서 Reversible Network 적용 후 memory의 최대 사용량은 다음과 같다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/25dc0256-5295-42b7-a7fb-fd5778f0ed6a">
</p>


## 3. Chuncking

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/86ed6c8d-6bc9-4474-9805-31fff1662996">
</p>

Reverisble Transformer를 통해서 $$n_l$$ 에 관한 부분이 사라졌더라고해도, 여전히 많은 메모리를 요구한다. FFN은 특히 벡터의 차원수로 $$d_{ff} = 4K$ 또는 이보다 높은 차원수를 요구한다. 즉, Sequence가 길어질수록 FFN을 연산하는데도 많은 메모리가 소요된다. Chuncking에서 제안하는 방식은 이런 Computational cost를 줄이기 위해 하나의 FFN을 시퀀스에 위치와는 독립적으로 만들어 쪼갠 후 그걸 각각 연산하자는 것이다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/42ad9adb-2a98-4bf1-9004-bf66fe0f0ffc">
</p>

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/3b91bfa4-a4bb-40ba-a3db-dc21aafb070a">
</p>

> computations in feed-forward layers are completely independent across positions
> in a sequence, so the computation can be split into c chunks

<p align="center">
<img width="500" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/2a74c0d0-6b9e-4e74-b7e0-e278e445975f">
</p>

이 Layer는 일반적으로 <span style = "color:god">**모든 위치에 대해 병렬로 연산을 수행하지만, 한 번에 하나의 Chunk에 대해 연산**</span>하여 메모리를 줄일 수 있다. 또한 Reverisble Transformer의 연산에더도 마찬가지로 Chunking을 할 수 있다. FFN외에도, 많은 단어를 요구하는($$d_{model}$$ word type보다 많은 단어를 요구) 모델의 경우 출력에서 log-likelihoo로 chuncking하고 한 번에 Sequence의 섹션에 대한 loss를 계산한다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/e3dc5aed-4a3f-4324-9be5-df9c946d102c">
</p>

## Final Model Structure

### 1) LSH Attention

앞서 각각의 특징을 살펴봤으므로 이제, 최종적인 모델 구조를 봐야한다. 논문에서는 각 데이터 포인트에 Locality Sensitive Hashing을 적용한다. 요점은 Hash값이 일치하는 데이터에 대해서만 Attention을 계산하는 것이다. 이를 위해 논문에서는 $$Query(Q) = Key(K)$$라는 가설을 세운다. 각 데이터 포인트를 Q로 projection하는 행렬과 K로 projection하는 행렬을 동일하게 설정한 것이다. 이 가설에서 **Query와 Key는 본직적으로 같은 값은 값으로 설정**한다.

$$Query(Q) == Key(K)$$

이러한 가설은 직관적으로 납득되지 않으나, 논문에서는 매우 큰 데이터셋에서는 이 가설을 사용해도 성능 저하가 일어나지 않음을 증명하였다. 이러한 가설을 세울 수 있는 근간은, 한 문장 내에서 중요한 단어는 몇 단어 안되고 나머지는 모두 불용어에 속하기 때문이다.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/b3acc028-9f38-4ab4-a53c-4d44b9282a6b">
</p>

LSH Attention을 적용하는 절차는 첫 번째는 Q = K이므로 각 데이터 포인트에 대한 Q = K값을 일려로 된 벡터 형태로 나타낸 후, 각 데이터 포인트에 LSH를 적용하는 것이다. 그 이후 같은 Hash값을 가진 데이터 포인트끼리 버킷(Bucket)으로 묶는다. 각 해시 버킷에 임의로 순서를 매겨서 정렬한다. 

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/19599572-c249-49b5-8ced-3948b7929956">
</p>

각 버킷에서 높은 확률로 데이터 포인터들이 불균형하게 할당될 것이다. 따라서 데이터 포인터들을 고정된 크기의 구역으로 분할한다.(Chuncking) 이후 Attention Weight을 계산하는데, 다음 조건을 만족하는 쌍들에 대해서만 계산한다. 

1. 두 데이터 포인트가 같은 버킷에 있어야 한다.
2. 두 데이터 포인트는 서로 같은 chunk에 있거나, attention 연산의 마지막 데이터 포인트는 시작 데이토 포인트가 있는 구역 바로 앞에 있어야 한다.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/02ae2bcf-dab7-463d-b660-dd7ee30c1e3c">
</p>

일반적인 트랜스포머는 각 데이터 포인트들이 스스로를 attend할 수 있는 구조이지만, 이 구조에서는 Q=K를 따르기 때문에 Self-Attention의 내적값이 다른 Attention 내적값보다 언제나 매우 크다. 따라서 조건을 만족하는 쌍이 자신으로만 구성되는 경우를 제외하면 스스로를 attend하는 것을 허용하지 않으며 이를 그림으로 표현하면 위와 같다.

같은 부분과 앞 부분에만 접근하므로 각 데이터 포인트가 최대로 attend하는 수는 각 chunking된 크기의 2배 만큼이다. 만약 이 시퀀스의 길이를 $$l$$이러고 하고, chunking 수를 $$c$$라 한다면, partition의 크기는 $$\frac{l}{c}$$이다. 또한 Attention의연산 수는 $$l \cdot (2 \frac{l}{c})^2$$에 비례한다. 만약 $$c$$가 충분히 크다면 $$l$$에 선형적으로 비례하는 구조로 간주할 수 있고, 원래 트랜스포머가 모든 쌍을 attend하기 때문에 $$l^2$$에 비례하는 복잡도를 가지는 것을 생각하면 상당히 개선된 것이다.

<br/>

### 2) FFN with Reversible Network 

Attention Layer와 Feed Forward Layer가 이루는 블록을 Residual Network로 간주할 수 있으며 따라서 Reversible Network에서와 같이 입출력을 둘로 나눌 수 있음을 보였다. 

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/0225f084-d1b1-4870-9a0d-05c447d18bca">
</p>

이 구조로 변형된 트랜스포머는 <span style = "color:gold">**N층 블록의 결과물을 모두 저장할 필요 없이 한 층에 대해서만 메모리를 사용하여 연산**</span>을 수행한다. PyTorch에서 backward연산을 forward 함수로 구현하는 형태를 생각해보면 이해가 쉽다.

<br/>

### 3) FFN with Chunking

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Deep_Learning/assets/111734605/2e33c5ff-6f91-461c-9cdb-170fb8db9a11">
</p>

FFN은 데이터 포인트의 위치와 무관하다. 각 구역으로 나뉜 부분 $$c$$에 FFN을 순차적으로 적용하면 하나의 부분에 대한 FFN 분량의 메모리만 필요하다. 논문의 공간 복잡도와 시간 복잡도를 비교한 표는 이 구조가 얼마나 효율적으로 트랜스포머구조를 간소화하는지를 보여준다. 표에서와 같이 <span style = "color:aqua">**시공간 복잡도의 이득 대부분은 LSH Attention에서 기인**</span>한다.

<br/>

# Reference
[Blog: Reformer, The Efficient 트랜스포머](https://velog.io/@nawnoes/Reformer-%EA%B0%9C%EC%9A%94)    
[Blog: 꼼꼼하고 이해하기 쉬운 Reformer 리뷰](https://tech.scatterlab.co.kr/reformer-review/)  
[Reformer: The Efficient Transformer](https://www.youtube.com/watch?v=6ognBL6DEYM)  
