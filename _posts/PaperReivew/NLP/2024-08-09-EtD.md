---
title: "[논문리뷰]Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2024-08-09
last_modified_at: 2024-08-09
---

*Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao*. 2024. **Explore then determine: A gnn-llm synergy framework for reasoning over knowledge graph**.

# Problem Statement
Limitations of Prior Studies

1. **대형 언어 모델(LLM)의 지식 부족**: LLM은 자연어 처리에서 강력한 성능을 보이지만, KGQA 작업에서는 사실적 지식의 부족과 환각 문제로 인해 어려움을 겪고 있다. 기존의 LLM 기반 추론 방법들은 질문과 관련된 정확한 지식을 추출하는 데 중요한 **지식 그래프(Knowledge Graph, KG)에서의 구성적 학습(compositional learning)을 간과**하고 있다는 점이 문제로 지적되고 있다.

2. **비효율성과 높은 비용**: 현재의 많은 KGQA 방법들은 LLM의 미세 조정이나 **빈번한 상호작용**을 필요로 하는데, 이는 특히 대규모 지식 그래프에서 다중 단계 추론을 수행할 때 시간과 자원이 많이 소요되어 비효율적이다.


<br/>
<br/>


# Method
## Model Overview

Knowledege Graph(KG)는 헤드 엔티티(= head entity), 릴레이션(= 릴레이션), 테일 엔티티(= tail entity)로 이루어진 트리플을 지식의 기본 단위로 저장하고 있다. 헤드, 릴레이션, 테일은 각각 주체(= subject), 술어(= predicate), 객체(= object)라고도 불린다. 

- Notation
  - 트리플(Triple): \{$$(e_s, r, e_o)$$\}
  - Knowledge Graph: $$\mathcal{G} = \{(e_s, r, e_o) \mid e_s, e_o \in V, r \in R \}$$
  - 엔티티 집합: $$V$$
  - 릴레이션 집합: $$R$$
  - 질문: $$q$$ 

KGQA 문제는 질문 $$q$$와 그래프 $$\mathcal{G}$$가 주어졌을 때, 답변 엔티티 $$e_a \in V$$를 KG에서 찾아내는 것이다. 이를 함수로 $$F(q, G)$$와 같이 표현할 수 있다. KGQA는 질문과 관련된 구성적 지식(Compositional Knowledge)을 KG에서 정확하게 탐색해야 하며, 질문과 KG 내 엔티티 간의 텍스트 이해 및 매칭 능력을 요구하는 어려운 작업이다. 그리고 **LLM은 여전히 멀티 홉 추론을 함에 있어서 유망한 후보를 제대로 추출하지 못한다**. 

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/0b76f8ed-31e2-4527-abce-97b5ee9a58c8" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>Figure 1. Explore-then-Determine(EtD) Framework</em>
  </figcaption>
</figure>

본 논문에서는 인간이 어려운 과제를 마주할 때 여러 가능한 대안을 식별한 후 최적의 선택을 하는 방식에서 영감을 받아, 멀티 홉 추론을 위한 Explore-then-Determine(EtD) 프레임워크를 제안한다. <span style="color:red">**LLM에 유망한 후보와 세부 지식을 제공하기 위한 탐색 모듈을 채택**</span>하고, <span style="color:red">**LLM이 최종 답변을 생성하도록 안내하는 결정 모듈을 사용**</span>한다.

Figure 1은 EtD 프레임워크를 나타내며, 그림에서와 같이 두 가지 구성 요소로 이루어져 있다. 각각 (1)의미를 인식하고 유망한 후보 엔티티를 추출하는 그래프 탐색(**Explore**); (2)LLM을 통해 후보들을 활용해 답변 결정(**Determine**)이다. 첫 번째 부분에서는 그래프에서 질문과 관련된 구성적 지식을 정확하게 파악하기 위해, LLM이 강화된 GNN 모듈을 설계하여 주어진 질문과 관련된 후보와 관련 지식을 KG에서 탐색한다. 즉, ($$\mathcal{C_q}, \mathcal{K_q}$$) = $$f_{exp}(q, \mathcal{G})$$이다. 두 번째 부분에서는 첫 번째 단계에서 탐색된 정보를 효과적으로 활용하기 위해, 지식이 강화된 **다중 선택 프롬프트**를 신중하게 설계하여, LLM이 KG의 명시적 지식과 LLM 내부의 암묵적 지식을 바탕으로 최종 답변을 결정하도록 안내한다, 즉, $$e_a = g_{det}(q, \mathcal{C_q}, \mathcal{K_q})$$이다.

## Semantic-Aware Graph Exploration
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/user-attachments/assets/2b55d178-312f-4bf2-8901-c08d76b65bbc" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>Figure 2. Semantic-Aware Graph Exploration의 진행 과정</em>
  </figcaption>
</figure>

**Semantic-Aware Graph Exploration**은 주어진 질문에 대해 지식 그래프(KG)에서 의미적으로 관련 있는 후보 엔티티와 세부 지식을 탐색하기 위해 설계된 과정이다. 이 과정은 크게 두 가지 주요 단계, **Semantic-aware pruning**과 **GNN encoding through propagation**으로 나누어진다.

### 1) Semantic-Aware Pruning  
**Semantic-aware pruning** 단계에서는 주어진 질문 $$q$$에 대해 지식 그래프 $$\mathcal{G}$$에서 의미적으로 관련이 있는 후보 엔티티를 선택하고, 불필요한 정보를 걸러내는 과정을 수행한다. 이 과정은 다음과 같은 절차로 이루어진다.

**1. 초기화**    
질문 $$q$$와 관련된 토픽 엔티티 $$e_q$$를 초기 후보 집합 $$\mathcal{C_0}^q$$로 설정한다. 토픽 엔티티 $$e_q$$의 초기 임베딩 표현(representation) $$h_{e_q}^0$$은 질문 $$q$$의 임베딩 $$h_q$$로 초기화된다. 이 질문의 임베딩 표현은 사전학습된 LLM
의 출력 임베딩을 사용하며, 논문에서는 Llama2-13B를 사용하였다.

<center>$$h_{e_q}^0 = h_q = W_L \cdot \text{LLM}(q)$$</center>

위의 수식이 질문 임베딩, 토픽 엔티티의 초기 표현값을 나타내며, $$W_L$$은 학습 가능한 가중치 행렬이다. 

**2. 후보 집합 확장 및 중요도 계산**     
다음으로, $$\ell$$ 번째 단계에서 현재 후보 집합 $$C_{\ell-1}^q$$을 사용하여 후보 엔티티 집합을 확장한다.

<center>$$C_{\ell}^q = \{e_o : (e_s, r, e_o) \in G, e_s \in C_{\ell-1}^q \}$$</center>

여기서 $$e_s$$는 현재 후보 엔티티, $$e_o$$는 새로운 후보 엔티티, 그리고 $$r$$는 두 엔티티 사이의 릴레이션이다. 각 후보 엔티티 간의 엣지에 대해 중요도 $$\alpha_{\ell}^{q \vert sr}$$를 계산한다:

<center>$$\alpha_{\ell}^{q \vert sr} = \sigma \left( W_{\ell}^{(s)} h_{s}^{\ell-1} + W_{\ell}^{(r)} h_r + W_{\ell}^{(q)} h_q + W_{\ell}^{(qr)} (h_r \odot h_q) \right)$$</center>

여기서 $$\sigma$$는 시그모이드(Sigmoid) 함수, $$(W_{\ell}$$은 학습 가능한 가중치 행렬들, $$h_r$$는 릴레이션 $$r$$의 표현, $$\odot$$는 하다마드 곱(Hadamard product)이다. 하다마드 곱은 Element-wise product이다. 중요도 기반 필터링 단계에서는 각 엣지의 중요도 $$\alpha_{\ell}^{q \vert sr}$$를 바탕으로, **중요도가 높은 상위 K개의 엣지를 남기고 나머지는 필터링**하여 새로운 후보 집합 $$\tilde{C}_{\ell}^q$$을 만든다. 이 과정은 **관련성이 낮은 엔티티와 릴레이션을 제거**하여 후보 엔티티 수의 폭발적 증가를 방지한다.

### 2) GNN encoding through propagation
Semantic-aware pruning에서 필터링된 후보 엔티티들을 바탕으로, **GNN을 사용하여 각 엔티티의 표현을 학습**하고, 질문과 관련된 의미적 정보를 그래프 구조를 통해 전파(propagation)하는 과정이다.

**1. 후보 엔티티 표현 업데이트**  
GNN을 사용하여 현재 후보 엔티티 $$\tilde{C_{\ell}}^q$$에서 이전 단계의 후보 엔티티 $$\tilde{C}_{\ell-1}^q$$로부터 정보를 전파한다.

<center>$$h_{o}^{\ell} = \delta \left( \sum_{(e_s, r, e_o) \in \tilde{N}_{e_o}^{\ell}} \alpha_{\ell}^{q|sr} W_{\ell} \left( h_{s}^{\ell-1} \odot h_r \right) \right)$$</center>

여기서 $$\delta$$는 활성화 함수, $$\tilde{N}_{e_o}^{\ell}$$는 $$e_o$$의 남아있는 이웃 엣지 집합, $$h_{s}^{\ell-1}$$은 이전 단계에서의 후보 엔티티 $$e_s$$의 표현이다. L번의 전파 과정을 거친 후, 최종 후보 집합 $$C_q = \tilde{C}_0^q \cup \tilde{C}_1^q \cup \ldots \cup \tilde{C}_L^q$$과 각 엔티티의 최종 표현 $$h_e^L$$을 얻는다.

**Semantic-Aware Graph Exploration**은 주어진 질문에 대해 지식 그래프에서 <span style="color:red">**의미적으로 관련 있는 후보 엔티티와 세부 지식을 탐색**</span>하고, 이를 통해 효율적이고 신뢰할 수 있는 질의응답을 가능하게 하는 과정이다. 이 과정은 **의미적 관련성을 바탕으로 후보 엔티티와 관계를 필터링**하는 **Semantic-aware pruning**과, **필터링된 엔티티 간의 정보를 전파하여 의미적 표현을 학습**하는 **GNN encoding through propagation**을 결합하여, 최종적으로 **가장 유망한 후보와 그와 관련된 세부 지식을 도출**하는 역할을 한다.

## Knowledge-Enhanced Answer Determination




<br/>
<br/>

# Experiments



<br/>
<br/>

# Limitations and Contributions

