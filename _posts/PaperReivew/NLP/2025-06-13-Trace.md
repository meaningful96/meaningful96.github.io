---
title: "[논문리뷰]TRACE: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-13
last_modified_at: 2025-06-13
---

*Jinyuan Fang, Zaiqiao Meng, and Craig MacDonald*. “[**TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation**](https://aclanthology.org/2024.findings-emnlp.496/).” In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 8472–8494, Miami, Florida, USA.

# Problem Statement
<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.13%5DTRACE/Trace1.png?raw=true">
</p>

이 논문은 다중 문서에서 정보를 통합해 복잡한 질문에 답해야 하는 **multi-hop question answering(MHQA)** 문제를 해결함에 있어, 기존 RAG 모델들이 다음과 같은 한계점을 지적한다.
- **irrelevant한 문서를 검색**하여 노이즈를 유입시키고, 이로 인해 추론 성능이 저하됨.
- 특히 단순히 retriever가 가져온 문서를 LLM에 연결(concatenation)하는 기존 방식은, **다중 홉 추론에 필요한 근거(evidence)**를 효과적으로 구성하지 못함.

이를 해결하기 위해 저자들은 **문서들을 Knowledge Graph(KG)로 변환**하고, 이 KG로부터 **논리적으로 연결된 추론 체인(Reasoning Chain)을 구성**하여, 보다 정제된 supporting evidence를 RAG 모델에 제공하는 방법인 **TRACE**를 제안한다.

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.13%5DTRACE/Trace2.png?raw=true">
</p>

TRACE는 multi-hop 질문에 대한 정답을 생성하기 위해, 검색된 문서로부터 Knowledge Graph(KG)를 생성하고, 이 KG로부터 논리적으로 연결된 reasoning chain을 autoregressive하게 구성하며, 최종적으로 이 reasoning chain을 기반으로 정답을 생성하는 RAG 프레임워크이다. 전체 확률 모델은 다음과 같이 정의된다:

- $$q$$: 질문
- $$D_q$$: 질문에 대해 검색된 문서 집합
- $$G_q$$: $$D_q$$로부터 생성된 KG
- $$z$$: KG에서 생성된 추론 체인
- $$a$$: 생성된 정답

## KG Generator
- **입력**: 검색된 문서들 (by Original Question, $$D_q$$)
- **출력**: Knowledge Graph
    - Head: 문서의 title
    - Tail: Body (text에서 추출된 entity나 문장)

KG Generator는 검색된 문서 집합 $$D_q$$를 입력으로 받아, 각 문서에서 ⟨head, relation, tail⟩ 형태의 knowledge triple을 추출함으로써 KG $$G_q$$를 생성한다. 이때, 각 문서의 **title을 head entity로 간주**하고, **본문에서 출현하는 tail entity들과의 관계를 추출하여 triple을 구성**한다. KG 생성은 LLM 기반 in-context prompting으로 수행되며, KG generator는 다음 확률식을 따르는 모듈이다:

$$p(G_q \vert D_q)$$

TRACE는 **lost-in-the-middle** 문제를 방지하기 위해, <span style="color:red">**모든 문서 $$d_i \in D_q$$에 대해 triple을 독립적으로 생성**</span>하고, 문서 간 triple 연결은 공통 entity를 통해 암묵적으로 연결한다. 즉, 긴 문서를 하나로 입력하는 방식 대신, 문서 단위로 triple을 추출하고 이를 합쳐 전체 KG를 구성한다:

$$G_q = \bigcup_{i=1}^{N} \text{KG Generator}(d_i)$$

문서가 Wikipedia 기반이기 때문에, title과 본문 간 관계를 추출하기 용이하며, 추출된 triple은 reasoning chain 구성의 후보로 사용된다. 이때의 prompt는 "title과 관련된 tail entity와 그 관계를 문장에서 추출하라"는 방식으로 구성된다.

## Reasoning Chain Constructor
- **입력**: 질문 $$q$$, KG $$G_q$$
- **출력**: Reasoning Chain $$z = [z_1, \dots, z_L]$$
- reasoning chain을 autoregressive하게 구성하며, 각 단계마다 다음 triple을 선택한다.
- 필요 없는 triple 생성을 방지하기 위해 adaptive chain termination 전략 도입 (선택지 A: “no need for additional triples”)


Reasoning Chain Constructor는 KG $G_q$를 기반으로 질문 $q$에 대한 reasoning chain $$z = [z_1, z_2, \dots, z_L]$$을 autoregressive하게 구성한다. 이 모듈은 $$p(z \vert q, G_q)$$의 근사로 작동하며, chain factorization은 다음과 같이 정의된다:

$$
p(z \mid q, G_q) = \prod_{i=1}^{L} p(z_i \mid q, z_{<i}, \hat{G}_i)
$$

여기서 $$\hat{G}_i$$는 i번째 step에서의 후보 triple 집합이다. 이 reasoning chain 생성을 위해 TRACE는 두 개의 핵심 서브모듈을 포함한다.

- **Triple Ranker**
- **Triple Selector**

**Triple Ranker**는 현재 질문과 이전 triple들을 합친 context를 bi-encoder 모델(E5-Mistral)을 통해 임베딩한 후, KG 내 모든 triple들과의 inner product를 계산하여 상위 K개의 triple을 후보군으로 선택한다. 

이어서 **Triple Selector**는 LLaMA3-8B-Instruct 기반 LLM을 사용하여 multiple-choice 형식의 prompt로부터 다음 triple을 선택한다. 이때 “A. no need for additional triples”라는 특수 선택지를 포함시켜 reasoning chain의 종료 시점을 LLM이 스스로 판단하도록 하며, 이를 adaptive chain termination 전략이라 한다. 이 선택 과정은 softmax(logit)를 기반으로 다음과 같은 분포를 정의한다:

$$p(z_i \vert q, z_{<i}, \hat{G}_i) = \text{Softmax}(l(c_1), \dots, l(c_K))$$

전체 reasoning chain은 beam search를 통해 top-$$R$$개의 후보 chain을 구성하며, chain의 길이는 최대 $$L$$로 제한하거나 LLM의 종료 선택에 따라 조절된다. 이러한 autoregressive chain 구성 방식은 단순한 triple 수집이 아니라 논리적으로 연결된 근거를 단계적으로 추출하는 데에 핵심적인 역할을 수행한다.

## Answer Generator
- **입력**: reasoning chain 혹은 이를 통해 선별된 문서
- **출력**: 정답

이 모듈은 구성된 reasoning chains를 활용하여 최종 답변을 생성한다. TRACE는 이 과정을 두 가지 방식으로 수행할 수 있다. 

첫 번째 방식인 **TRACE-Triple**은 reasoning chain 내의 triple들을 그대로 context로 사용하여 정답을 생성하며, triple의 순서 자체가 추론의 논리 구조를 반영하므로 별도의 재정렬 없이 LLM에 그대로 입력된다. 

두 번째 방식인 **TRACE-Doc**은 reasoning chain 내 각 triple이 생성된 원본 문서를 추적하고, 각 triple이 자신이 유래한 문서에 투표함으로써 득표 수 기준으로 문서를 정렬하여 context로 활용한다. 이 과정을 통해 noise가 포함된 전체 문서 대신 핵심 문서만을 사용하여 보다 정제된 context를 구성한다. 

두 방식 모두 in-context learning 기반 LLM을 사용하며, 다음과 같은 조건부 확률 모델을 따른다

$$p(a \mid q, z, D_q)$$ 

실험 결과에 따르면, <span style="color:red">**TRACE-Triple은 token 수가 상대적으로 적음에도 불구하고 기존 RAG 방식보다 높은 정확도를 기록**</span>하며, reasoning chain만으로도 충분한 정보를 제공할 수 있음을 입증한다.

<br/>
<br/>

# Experiments



<br/>
<br/>

# Conclusion
