---
title: "[논문리뷰]Multi-level Relevance Document Identifier Learning for Generative Retrieval"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-08-11
last_modified_at: 2025-08-11
---

*Fuwei Zhang, Xiaoyu Liu, Xinyu Jia, Yingfei Zhang, Shuai Zhang, Xiang Li, Fuzhen Zhuang, Wei Lin, and Zhao Zhang*. 2025. [Multi-level Relevance Document Identifier Learning for Generative Retrieval](https://aclanthology.org/2025.acl-long.497/). In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 10066–10080. https://doi.org/10.18653/v1/2025.acl-long.497

# Problem Statement
MERGE는 **Generative Retrieval (GR)** 환경에서 **고품질의 DocID를 생성하는 방법**을 다루고 있다. 기존 GR에서는 문서의 텍스트 표현만을 기반으로 DocID를 생성하기 때문에, 유사한 의미를 가지는 문서라도 표현 방식 차이로 인해 DocID 간의 의미적 유사성이 약해질 수 있다. 이를 해결하기 위해, 저자들은 **쿼리를 문서 간 의미 연결의 매개체**로 활용하여 다단계(relevance level) 의미 정보를 반영한 DocID 학습 방식을 제안한다. MERGE에서 말한 기존 연구의 한계는 다음과 같다.

**[문서 내용 기반 DocID 생성의 한계]** 기존 Generative retrieval 기법은 주로 문서의 텍스트 표현만을 기반으로 DocID를 생성하기 때문에, **의미적으로 유사한 문서라도 표현 방식의 차이(동의어, 서술 구조 변화 등)로 인해 생성된 DocID 간의 의미적 유사성이 약해질 수 있다**. 이로 인해 동일한 질의에 대해 유사 문서들이 서로 다른 식별자를 가지게 되어 검색 일관성이 저하된다.

**[이진 수준 Relevance 학습의 한계]** 많은 Generative retrieval 연구들은 **쿼리–문서 간 관련성을 이진(관련/비관련) 수준에서만 반영하여 학습**한다. 이러한 접근은 관련 문서들 간의 세부적인 유사도 차이를 반영하지 못하고, 결과적으로 동일 relevance 그룹 내의 문서 구분 능력이 떨어진다.

**[쿼리–문서 간 계층적 의미 정보 미활용]** 기존 방법들은 쿼리와 문서 간의 **다단계 relevance 관계를 효과적으로 학습 과정에 반영하지 않는다**. 특히, 유사 문서 간 관계를 중재하는 '쿼리'의 연결 역할이 활용되지 않아, 계층적 의미 정보(e.g., Exact–Substitute–Complement 구분)가 DocID에 반영되지 못한다.

- 계층적 의미 정보는 ESCI 데이터셋에 내제된 relevance label 구조를 의미한다.
    - E (Exact): 쿼리 조건과 속성(사이즈, 색상, 기능 등)을 **모두 만족**하는 완전 일치 상품
    - S (Substutute): 쿼리와 **대체 가능**하지만, 일부 속성이 다르거나 부가 조건을 만족하지 않는 상품
    - C (Complement): 쿼리 제품과 **함께 사용할 수 있는 보완 제품**
    - I (Irrelevant): 쿼리와 관련 없는 상품
- 즉, **계층적 의미 정보**는 단순히 ‘관련 vs 비관련’이 아니라,  **관련(Exact) → 덜 관련(Substitute) → 보완적(Complement) → 비관련(Irrelevant)** 이렇게 **다단계의 의미적 거리**를 표현하는 라벨 구조를 의미한다.
- 일반적인 QA 데이터셋 (e.g., SimpleQA, NaturalQuestions, TriviaQA, HotpotQA, 2WikimultihopQA)들은 모두 binary relevance (정답 문서 vs 오답 문서)로 label이 주어져있고, 어떤 문서가 정답에 더 가까운지에 대한 정보가 없으므로, “정답 문서에 가까운 정도”를 label로 만들어야지 relevance관련된 아이디어를 사용할 수 있다.

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.08.11%5DMERGE/figure1.png?raw=true">
</p>

MERGE는 먼저 **쿼리–문서 쌍을 입력**으로 받아, 문서를 PLM을 통해 임베딩한 뒤 **RQ-VAE**를 사용해 다단계 코드북 기반의 계층적 DocID를 생성한다. 이 과정에서 **Multi-relevance Query–Document Alignment**로 서로 다른 relevance level 문서 임베딩을 쿼리에 정렬하고, **Outer-level Relevance Contrastive Learning**으로 관련/비관련 문서를 구분하며, **Inner-level Multi-level Relevance Learning**으로 관련 문서들 간의 세부 구분을 학습한다.

학습된 RQ-VAE는 각 문서에 고유하고 의미 계층이 반영된 DocID를 부여하며, 이를 GR 학습 데이터로 변환한다. 마지막으로, Seq2Seq 기반 GR 모델이 쿼리를 입력받아 해당 문서의 DocID를 생성하도록 학습하여, 질의 시 생성된 DocID를 통해 문서를 검색한다.

## RQ-VAE Training
### Step 1. DocID Learning via Multi-level Relevance
- **입력:** PLM(BERT/T5/mT5)으로 인코딩된 문서 임베딩
- **출력:** 계층적구조를 가진 DocID ($$c_0, c_1, \cdots, c_{m-1}$$), 각 $$c_l$$은 레벨 $$l$$의 코드북에서 선택된 인덱스

먼저 문서 임베딩을 입력으로 받아 **다층 코드북 기반의 이산 DocID 시퀀스**를 생성해야 한다. PLM을 통해 문서를 임베딩하여 $$d$$ 벡터를 생성하고, 이를  RQ-VAE 모듈에 입력시킨다. 그리고 $$m$$개의 코드북에서 단계별로 고른 인덱스 $$(c_0, c_1, \cdots, c_{m-1})$$로 구성된 **계측적 DocID**를 출력한다. 이때 DocID는 coarse-to-fine의 위계를 가지며, 이후 GR에서 쿼리를 입력받아 해당 DocID를 생성·검색하는 용도로 활용된다.

RQ-VAE 절차를 세분화하면 다음과 같다. 먼저 DNN 인코더 $$E$$가 문서 임베딩 $$d$$를 잠재 표현 $$z = E(d)$$으로 변환하고, 이를 초기 잔차 $$r_0 = z$$로 둔다. 각 레벨 $$l$$마다 코드북 $$C_l =\{e_k^l\}_{k=1}^K$$ 에서 잔차 $$r_l$$에 최근접인 코드워드를 고른 뒤 $$c_l = \text{argmin_k} \vert \vert r_l - e_k^l \vert \vert$$ 로 표기하고, 잔차를 $$r_{l+1} = r_l - e_{c_l}^l$$로 갱신한다. 이 과정을 $$m$$번 반복하여 인덱스 튜플 ($$c_0, c_1, \cdots, c_{m-1}$$)을 얻는다. 코드북 충돌을 막기 위해 각 **코드북은 잠재표현에 대한 k-means 초기화를 사용**한다.

최종 양자화 표현은 $$\hat{z} = \sum{l=0}^{m-1}e_{c_l}^l$$ 이며, 디코더 $$D$$를 통해 입력 임베딩을 복원하는 손실 함수 $$\mathcal{L}_{\text{recon}}$$과, 코드북과 잔차의 상호 근접을 유도하는 commitment 손실 $$\mathcal{L}_{\text{rq}}$$을 정의해서 사용한다. 구체적인 수식은 아래와 같다.

<center>$$
\mathcal{L}_{\text{recon}} = \vert \vert d - D(\hat{z}) \vert \vert_2^2 
$$</center>

<center>$$
\mathcal{L}_{\text{rq}} = \sum_{l=0}^{m-1} \Big ( \vert \vert \text{sg}[r_l] - \mathbf{e}_{c_l}^l \vert \vert_2^2 + \alpha \vert \vert \mathbf{r}_l - \text{sg}[e_{c_l}^l] \vert \vert_2^2 \Big)
$$</center>

이 때, $$\text{sg}[\cdot]$$은 stop-gradient로 해당 항의 역전파를 차단하여 코드북과 잔차를 안정적으로 맞물리게 한다. 최종적으로 RA-VAE loss는 다음과 같다.

<center>$$
\mathcal{L}_{\text{RQ-VAE}} = \mathcal{L}_{\text{recon}} + \mathcal{L}_{\text{rq}}
$$</center>
