---
title: "[논문리뷰]ChainRAG: Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-09
last_modified_at: 2025-06-09
---

*Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu**. “[**Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering**](https://arxiv.org/abs/2502.14245).” In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, ACL 2025.

- **Key word**
  - Sub-question decomposition
  - Sentence based KG Construction
  - Retrieve sentences and entities for a given sub-question which is missed crucial entity for reasoning.

# Problem Statement

이 논문은 Retrieval-Augmented Generation(RAG) 시스템의 핵심 병목이 LLM의 불완전한 reasoning 능력과 문서 검색기의 제한된 문맥 이해에 있다고 보고, 이 두 모듈의 상호작용을 개선하기 위한 방안을 제시한다. 기존의 RAG는 일반적으로 하나의 질문에 대해 문서를 검색하고 이를 기반으로 LLM이 정답을 생성하는 구조이나, 이때 문서 검색은 단순 키워드 기반이거나 shallow한 semantic matching에 그치며, LLM은 복잡한 multi-hop reasoning을 수행할 수 있는 chain-of-thought(COT) 추론 능력을 갖고 있더라도 이를 활용하지 못하고 단발성 응답만을 생성한다는 한계가 있다. 특히, 복잡한 질문의 경우 검색된 문서들이 reasoning 과정에서 필요로 하는 중간 정보와 연결되지 않아, LLM이 hallucination을 일으키거나 부정확한 답변을 생성하는 경향이 발생한다. 

이에 따라 본 논문은 reasoning 과정 자체를 explicit하게 외화하여 retriever가 그 reasoning 경로를 따라 적절한 문서를 검색할 수 있도록 유도하는 새로운 구조가 필요하다고 주장한다.

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.09%5DChainRAG/chainrag1.png?raw=true">
</p>

ChainRAG는 Retrieval-Augmented Generation(RAG) 구조에 Chain-of-Thought(COT)를 도입하여, retriever와 generator 간의 연계를 강화한 프레임워크이다. Teacher LLM으로부터 추출한 reasoning chain을 기반으로 retriever는 각 reasoning step을 개별 쿼리로 활용해 문서를 검색하고, generator는 이 chain과 문서들을 함께 입력받아 정답을 생성한다. 이를 통해 ChainRAG는 reasoning 경로를 명시적으로 활용함으로써 정답의 일관성과 근거 충실도를 높이며, 기존 RAG의 비구조적 추론 및 문서-정답 불일치 문제를 효과적으로 해결한다.

<br/>

## Sentence Graph with Entity Indexing
- **입력**: 문서 내 문장 집합
- **출력**: sentence 간 관계를 표현한 graph 구조 (sentence-level graph), entity index 포함

ChainRAG는 먼저 문서 내 문장들을 노드로 하고, 문장 간의 의미적 또는 구조적 관계를 edge로 연결한 sentence graph를 구성한다. 각 문장에서는 **named entity recognition과 linking을 통해 등장하는 개체들을 추출**하고, 이 entity 정보를 문장에 index 형태로 저장한다. 문장 간 edge는 총 세 가지 기준으로 연결된다.

- **Entity Co-occurrence (EC)**: 동일한 개체가 두 문장에 함께 등장할 경우 두 문장을 연결한다.
- **Semantic Similarity (SS)**: 문장 임베딩 간 의미적 유사도가 높은 경우 두 문장을 연결한다.
- **Structural Adjacency (SA)**: 문서 내에서 물리적으로 인접하거나 같은 문단에 포함된 문장들끼리 연결한다.

이렇게 구성된 sentence graph는 이후 단계에서 sub-question이 특정 entity를 중심으로 문장을 확장 검색하는 데 핵심적인 역할을 하며, multi-hop reasoning을 위한 정보 탐색 구조를 제공한다.

<br/>

## Sentence and Entity Retrieval
- **입력**: 현재 sub-question, sentence graph, 각 문장에 대한 entity index 정보
- **출력**: 현재 sub-question과 관련된 문장 집합 (retrieved supporting sentences)

이 단계에서는 sub-question이 지칭하는 핵심 entity를 중심으로 관련 문장을 탐색한다. 먼저 sub-question에 등장하는 entity를 식별하고, sentence graph에서 해당 entity가 등장하는 문장을 anchor point로 선택한다. 그 다음, anchor 문장과 직접 연결된 문장들(예: 같은 entity를 포함하거나, 의미적으로 유사하거나, 물리적으로 가까운 문장들)을 확장하여 문맥적으로 밀접한 문장 집합을 구성한다. 이러한 방식은 단순히 질문-문서 간 유사도에 기반한 검색이 아니라, **entity 중심의 그래프 탐색**을 통해 정보 흐름을 따라가는 retrieval을 실현하며, 특히 multi-hop 질의에서 중간 개념을 포함한 문장들을 효과적으로 수집할 수 있도록 돕는다.

<br/>

## Sub-question Rewriting
- **입력**: 현재 sub-question, 해당 sub-question에 대한 retrieved 문장 집합
- **출력**: 다음 step의 sub-question

retrieval 결과를 바탕으로, ChainRAG는 현재 sub-question에 이어지는 <span style="color:red">**다음 reasoning step에 해당하는 sub-question을 재작성**</span>한다. 이는 teacher LLM이 생성한 chain-of-thought을 기반으로 학습된 rewriting module이 수행하며, 현재까지의 질문 흐름과 새롭게 얻은 문맥 정보를 통합하여 다음 질문을 만들어낸다. 

이 단계는 단순히 질문을 재구성하는 것이 아니라, **reasoning chain의 구조를 따라 문서를 점진적으로 확장해 나가는 역할**을 한다. 생성된 새로운 sub-question은 다음 단계의 entity retrieval에 사용되며, 추론 경로가 유기적으로 이어질 수 있도록 한다. 이 모듈은 학습 시 gold chain-of-thought을 supervision으로 사용하여 teacher의 추론 흐름을 학습한다.

<br/>

## Answer and Context Integration
- **입력**: 모든 sub-question들과 각 단계에서 수집된 문장들 (문맥 정보), 최종 reasoning chain
- **출력**: 최종 정답 텍스트

모든 retrieval 과정과 sub-question rewriting이 완료된 후, ChainRAG는 **각 step의 sub-question과 해당 문장들을 통합**하여 최종 정답을 생성한다. 이때 generator는 각 단계의 정보(예: 질문 흐름, 문장 정보)를 모두 고려하여 정답을 생성하는데, 문서 요약이 아니라 논리적 reasoning 경로를 따라가며 정답을 도출하는 방식이다.

학습 시에는 teacher가 생성한 gold reasoning chain과 정답을 supervision으로 활용하여, chain-aware한 정답 생성을 가능하게 한다. 이 통합 구조는 단편적인 문장 정보만 반영하는 기존 RAG와 달리, **추론 경로에 기반한 문맥 일관성**과 **정답의 신뢰도**를 동시에 확보할 수 있게 해준다.

<br/>
<br/>

# Experiments
## Main Results
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.09%5DChainRAG/chainrag2_main.png?raw=true">
</p>

ChainRAG는 세 개의 멀티홉 QA 데이터셋(MuSiQue, 2Wiki, HotpotQA)에서 기존의 NaiveRAG 및 고도화된 RAG 방식들보다 전반적으로 높은 성능을 기록하였다. 특히 GPT4o-mini, Qwen2.5-72B, GLM-4-Plus 세 모델에서 모두 일관된 향상을 보였으며, Qwen2.5-72B 기준으로 ChainRAG(CxtInt)는 평균 F1 59.92%로 HippoRAG 대비 9.6%p 향상되었다. NaiveRAG 대비로는 MuSiQue에서 약 60% 수준의 상대적 F1 증가가 관찰되었다. 또한, sub-question 기반 응답(AnsInt)과 문맥 기반 응답(CxtInt) 전략은 각각 reasoning 능력과 긴 문맥 처리 능력에 따라 LLM별로 상호보완적으로 작용하였다. 이는 ChainRAG의 구성 방식이 다양한 환경에서의 확장성과 안정성을 확보함을 시사한다.

<br/>

## Ablation Study
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.09%5DChainRAG/chainrag4.png?raw=true">
</p>

먼저, <span style="color:red">**sub-question rewriting**</span> 모듈의 제거는 모든 데이터셋에서 성능 저하를 유발하였으며, 특히 MuSiQue에서는 F1 기준 약 12~14%p의 급격한 감소가 나타났다. 이는 “lost-in-retrieval” 문제를 방치할 경우 검색 정확도와 최종 정답 생성 모두에 심각한 영향을 미친다는 점을 실증적으로 보여준다.

다음으로 sentence graph를 구성하는 세 종류의 edge—Entity Co-occurrence (EC), Semantic Similarity (SS), Structural Adjacency (SA)—중 어느 하나라도 제거할 경우 전반적으로 성능이 하락하였다. 세 가지 edge 중 어떤 것이 가장 중요한지는 데이터셋별로 차이를 보였는데, MuSiQue에서는 SS edge 제거가 가장 큰 성능 저하를 유발하였고, 2Wiki에서는 EC edge, HotpotQA에서는 SA edge가 상대적으로 중요했다. 이는 각 데이터셋의 문장 구조나 정보 밀도 차이에 따라 edge의 중요도가 다르게 나타난다는 점을 시사한다.

마지막으로 sentence graph 자체를 제거하고 NaiveRAG 방식처럼 chunk 단위로 문서를 구성했을 경우 성능은 전반적으로 하락하였다. 이는 세부 문장 단위의 정교한 인덱싱이 retrieval과 reasoning 성능 향상에 기여하고 있음을 의미한다.

## Analysis. LLM Calls
<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.09%5DChainRAG/chainrag3.png?raw=true">
</p>

Figure 6에서는 다양한 RAG 방식들의 LLM 호출 횟수를 기준으로 효율성을 비교하였다. ChainRAG는 LongRAG 대비 평균 17.3%의 호출 횟수 감소를 달성하면서, 성능은 오히려 높은 수준을 유지하였다. 이는 ChainRAG가 entity-aware sentence graph와 progressive retrieval 방식을 활용함으로써 불필요한 LLM 호출을 줄이면서도 필요한 정보를 효과적으로 획득할 수 있음을 의미한다.

한편, HippoRAG는 LLM을 사용한 knowledge graph 구축 방식으로 인해 가장 많은 호출 횟수를 기록하였으며, 데이터셋이 길어질수록 그 차이는 더 커졌다. 반면, Iter-RetGen은 가장 적은 호출 횟수를 기록했으나, ChainRAG에 비해 전반적인 QA 성능은 낮았다. 이러한 결과는 ChainRAG가 정확도와 효율성 간의 균형을 가장 잘 달성하고 있다는 것을 실험적으로 입증하는 근거가 된다.

<br/>
<br/>

# Conclusion
본 논문은 멀티홉 질의응답에서 Retrieval-Augmented Generation(RAG) 방식이 겪는 핵심 문제인 “lost-in-retrieval” 현상을 정밀하게 분석하고, 이를 해결하기 위한 새로운 프레임워크인 ChainRAG를 제안하였다. 기존 RAG 시스템에서는 sub-question이 핵심 entity 없이 생성되어 적절한 문장을 검색하지 못하고, 이로 인해 reasoning chain이 붕괴되는 문제가 발생한다. 이를 해결하기 위해 ChainRAG는 sub-question rewriting과 sentence-level graph 기반 progressive retrieval을 결합함으로써 누락된 entity를 보완하고, 문맥 간 구조적 연결성을 강화하였다. 실험 결과, ChainRAG는 기존 SOTA 비지도 RAG 기법들을 안정적으로 능가하였으며, 다양한 LLM과 멀티홉 QA 데이터셋에서도 일관된 성능 향상과 효율성을 입증하였다. 특히, sub-question rewriting이 가장 큰 성능 향상에 기여하였고, sentence graph의 세부 설계 역시 성능 유지에 중요한 역할을 함을 ablation을 통해 보여주었다.

다만, 제안된 방법에는 몇 가지 한계도 존재한다. 첫째, ChainRAG는 iterative하게 sub-question을 처리하고 문장을 확장 검색하기 때문에, NaiveRAG 대비 계산량과 시간적 자원이 더 많이 소요된다. 둘째, entity recognition과 rewriting 품질이 전반적인 성능에 직접적인 영향을 미치기 때문에, 해당 모듈의 오류가 전체 추론 성능 저하로 이어질 수 있다. 마지막으로, 현재 구조는 범용성을 보였으나, 특수 도메인(의료, 금융 등)에 적용할 경우 sentence graph 구성 방식의 추가 튜닝이 필요할 수 있다. 그럼에도 불구하고 ChainRAG는 multi-hop QA의 주요 병목을 정조준하여 근본적으로 개선함으로써, RAG 시스템의 향후 발전 가능성에 중요한 기여를 하였다.
