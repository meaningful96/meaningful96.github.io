---
title: "[논문리뷰]BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-08
last_modified_at: 2025-06-08
---

*Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, and Xiaofeng He*. “[**BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering.**](https://arxiv.org/abs/2505.11811)” In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, ACL 2025.

<br/>

# Problem Statement
<span style = "font-size: 110%">**Closed-book Reasoning의 한계**</span>  
- **지식 한계**: LLM이 학습 시점까지 내재적으로 습득한 지식에만 의존하기 때문에, 최신 정보나 훈련 데이터에 포함되지 않은 복잡한 멀티홉 질문에 대해 정답률이 낮고, 핵심 근거가 부족하여 환각(hallucination)이 쉽게 발생한다.
- **추론 경로의 다양성 부족**: 여러 reasoning operator(예: CoT, sub-step 등)를 활용하더라도 외부에서 새로운 정보를 얻지 못하기 때문에, 정답 도출에 필요한 모든 근거를 아우를 수 없다.

<br/>

<span style='font-size:110%'>**Retrieval-augmented Reasoning의 한계**</span>
- **검색 및 연산 효율성 저하**: 단일 검색(single-step retrieval)로는 멀티홉 질문에 필요한 모든 근거를 한 번에 확보하기 어렵고, 반복 검색(iterative retrieval) 방식도 질문의 복잡도와 무관하게 일괄적으로 적용되어 불필요하게 많은 연산 및 외부 호출이 발생하여 계산 비용이 증가한다.
- **질문 유형 미고려**: 기존 연구들은 질문의 유형(추론, 비교, 시간, 기타)에 따라 요구되는 reasoning 패턴이 다름에도 불구하고, 대부분의 프레임워크가 고정된 연산 방식을 일괄 적용하거나 질문 유형에 따른 연산자 선택이 이루어지지 않아, 간단한 질문에도 과도한 연산이 소모되고 복잡한 질문에는 최적화된 reasoning 경로를 제공하지 못한다.

이러한 이유로 **i)복잡한 질의에는 비효율적이고, ii)불필요하게 많은 계산 자원이 소모되며, iii)질문 유형별 최적화가 부족해 성능 개선이 제한됨**을 논문에서 지적하고 있다. 특히, 질문 유형별로 요구되는 reasoning 방식이 다름에도 불구하고, 기존 MHQA framework는 하나의 고정된 연산 방식에 의존하거나 외부 지식 활용이 제한적이라는 점이 핵심 한계로 부각된다.

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.08%5DBELLE/belle_1.png">
</p>
BELLE는 질문 유형에 따라 다양한 reasoning operator를 조합해 실행하는 Bi-Level Multi-Agent Reasoning Framework이다. 전체 구조는 세 가지 단계로 구성된다. 첫째, Question Type Classifier는 주어진 Multi-hop 질문을 Inference, Comparison, Temporal, Null 네 가지 유형으로 분류하여 downstream reasoning 전략의 입력으로 제공한다. 둘째, Bi-Level Multi-Agent Debate System은 두 계층의 토론 구조로 구성된다. 1단계는 Affirmative/Negative Debater가 대립적인 주장을 통해 연산자 선택을 유도하고, 2단계는 Fast Debater가 현재 토론의 적절성을, Slow Debater가 전체 토론 이력을 반영하여 일관성과 안정성을 평가한다. Judge는 두 계층의 결과를 종합하여 연산자 조합 계획을 산출한다. 셋째, Multi-hop QA Executor는 이 계획을 실행하며 각 operator(CoT, Sub-step, Single-step 등)를 순차적으로 적용해 reasoning 결과를 생성하고, 최종 정답을 도출한다. 전체 구조는 기존 MAD 구조에서 발전된 형태로, 정보 누수 없이 이력 기반 reasoning 조율이 가능한 점이 핵심이다.

## Question Type Classifier
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.08%5DBELLE/belle_2.png">
</p>

- **입력**: 자연어 질문
- **출력**: 질문 유형 라벨 (Infrence, Comparison, Temporal, Null) 중 하나
- LLM을 이용한 텍스트 분류 태스크로, In-Context Learning (ICL) 방식의 프롬프트를 활용한다. 이로써 이후 단계에서 해당 질문 유형에 맞는 reasoning operator를 조합하도록 한다.

첫 번째 모듈인 **Question Type Classifier**는 주어진 Multi-hop 질문을 Inference, Comparison, Temporal, Null의 네 가지 유형으로 분류한다. 이 과정은 LLM 기반의 In-Context Learning을 활용하여 수행되며, 입력 질문과 함께 여러 개의 QA 예제를 포함한 템플릿을 입력으로 받아, <span style="color:red">**질문 유형을 예측**</span>하는 방식으로 동작한다. 해당 모듈은 향후 실행될 operator 조합을 결정하는 데 중요한 사전 지식으로 작용한다.

## Bi-level Multi-Agent Debate
- **입력**: 질문 유형, 이전 debate history
- **출력**: 최종적으로 실행할 operator 조합에 대한 계획 (Execution Plan)
- **구조 요약**:
    - 1단계: Affirmative/Negative debater가 "찬성/반대" 논쟁을 반복하며 operator 선택에 대해 논의한다.
    - 2단계: Fast debater와 Slow debater가 각각 현재 논의가 합리적인지 평가(fast), 과거 모든 논의와 일관성을 유지하며 판단(slow)한다.
    - Judge: 위 논의 결과를 종합해 operator 조합을 확정한다.
- Debate 구조를 2단계로 확장해 operator 선택의 신뢰도와 다양성을 모두 확보함.

두 번째 모듈인 **Bi-Level Multi-Agent Debate(MAD)**는 에이전트 간의 구조화된 토론을 통해 operator 조합을 결정하는 핵심 구성 요소이다. 이 모듈은 두 계층으로 나뉘며, 1단계에서는 Affirmative와 Negative Debater가 토론을 주도하고, 2단계에서는 Fast Debater와 Slow Debater가 각각 현재 논의의 타당성 평가와 역사 정보 기반 조율을 담당한다. 특히, **Fast Debater는 현재 round에서 선택된 연산자의 타당성을 평가하고, Slow Debater는 과거의 토론 맥락을 바탕으로 일관된 계획이 수립되도록 조율**한다. 이러한 이중 구조는 <span style="color:red">**단일 round 또는 발화에 과도하게 의존하는 기존 MAD 시스템의 한계를 극복**</span>하고자 하는 의도를 반영한다. 최종적으로 Judge 에이전트는 각 에이전트의 발언을 종합하여 operator 조합 실행 계획을 산출한다.

## Multi-hop QA Executor
- **입력**: 선정된 operator 실행 계획, Original question 및 필요 시 외부 지식
- **출력**: answer
- Debate 과정에서 결정된 operator 조합(예: sub-step → single-step retrieval 등)에 따라 LLM 및 retrieval을 순차적으로 실행하여 step별 sub-answer를 만들고, 이를 종합해 최종 답을 도출한다

세 번째 모듈인 **Multi-hop QA Executor**는 이전 모듈에서 생성된 operator 계획을 바탕으로 실제 reasoning 과정을 수행하는 역할을 한다. 이 모듈은 각 sub-question이나 sub-step을 LLM을 통해 실행하고, 그 결과를 결합하여 최종적인 정답을 생성한다. 일관성을 유지하기 위해 계획 수립에 사용된 것과 동일한 LLM을 reasoning 실행에도 활용한다. 결과적으로 BELLE는 질문의 유형 분류부터 operator 선택, reasoning 실행까지 모두 구조화된 방식으로 통합하여 수행하는 프레임워크이다.

<br/>
<br/>

# Experiments
## Main Results

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.08%5DBELLE/belle_3.png">
</p>

BELLE는 Multi-hop QA 벤치마크인 Multi-hop RAG, HotpotQA, 2WikiQA, MuSiQue 네 가지 데이터셋에서 일관되게 우수한 성능을 기록하였으며, 특히 reasoning 복잡도가 높은 Inference 및 Null 유형 질문에서 두드러진 성능 향상을 보여준다. Table 1에 따르면, BELLE는 F1 기준으로 Multi-hop RAG에서 70.4%, HotpotQA에서 66.5%, 2WikiQA에서 75.7%를 기록하며 기존 SOTA 모델인 BeamAggR보다 각각 3.2%, 3.6%, 4.1% 더 높은 성능을 보였다. 특히 MuSiQue에서는 hop 수가 늘어날수록 성능 차이가 확대되어, 4-hop 기준으로 7.6%p 향상된 29.2%를 기록하였다.

## Analysis 1. Performance Comparison based on Question Types
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.08%5DBELLE/belle_4.png?raw=true">
</p>

Figure 4에서는 질문 유형별 성능을 분석하였다. 그 결과, 비교적 단순한 Temporal 및 Comparison 유형에서는 CoT 또는 단일 연산자 기반 접근도 일정 수준의 성능을 유지하지만, 복잡한 reasoning이 필요한 Inference 및 Null 유형에서는 BELLE가 현저한 성능 우위를 나타냈다. 예를 들어, Inference 유형에서 BELLE는 BeamAggR보다 약 4~6% 더 높은 F1 점수를 보였고, Null 유형에서는 다양한 operator를 조합할 수 있는 유연성이 큰 이점으로 작용하여 가장 큰 성능 격차를 보였다.

## Analysis 2. Performance Comparison based on the Number of Tokens
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.08%5DBELLE/belle_5.png">
</p>

마지막으로 Figure 6과 Table 3은 BELLE가 기존 Retrieval 기반 방식에 비해 훨씬 더 적은 토큰 수로 높은 성능을 달성한다는 점을 강조한다. 예를 들어, 평균적으로 BeamAggR은 약 48,000 tokens을 사용하는 반면, BELLE는 약 20,000 tokens 이하로 비슷하거나 더 나은 성능을 보인다. 이는 BELLE가 적절한 operator 조합을 빠르게 결정하고, 불필요한 반복 추론을 줄이는 데 성공했음을 보여주는 증거이다. 나아가 에이전트 수 증가가 성능 향상에 기여하지만, 일정 수 이상에서는 오히려 token 소비가 급증하므로, 2~3개 수준의 bi-level 에이전트 구성이 가장 효율적인 trade-off임이 확인되었다.

<br/>
<br/>

# Conclusion
본 논문은 다양한 유형의 Multi-hop 질문에 따라 적절한 reasoning 연산자를 동적으로 조합하여 추론 경로를 계획하고 실행할 수 있는 Bi-Level Multi-Agent Reasoning Framework인 BELLE을 제안하였다. 기존 방식들이 단일 연산자나 고정된 전략에 의존했던 것과 달리, BELLE은 질문 유형 분류 → 이중 에이전트 토론 → 실행 계획 생성이라는 일관된 구조를 통해, 질문의 복잡도에 따라 효율적이고 정교한 reasoning을 가능하게 한다. 특히, Slow/Fast Debater로 구성된 2단계 토론 구조는 이전 라운드의 논의 이력을 반영하며 operator 조합의 안정성과 일관성을 강화하였다. 실험 결과, BELLE은 다양한 Multi-hop QA 벤치마크에서 기존 SOTA 대비 높은 정확도와 낮은 연산 비용을 동시에 달성하였다.

그러나 BELLE에도 한계는 존재한다. 먼저, 연산자 조합을 위한 에이전트 간 상호작용 과정에서 복수의 LLM 호출이 필요하므로, 시스템이 커질수록 비용과 시간 오버헤드가 증가할 수 있다. 또한 현재는 사전에 정의된 네 가지 질문 유형만을 분류 대상으로 삼고 있기 때문에, 실세계에서 마주할 수 있는 복합적 또는 새로운 유형의 질문에 대해서는 대응력이 제한될 수 있다. 이러한 한계를 극복하기 위해 향후에는 질문 유형 공간의 확장과 에이전트 간 협업 구조의 최적화를 통해 보다 범용적이고 효율적인 reasoning 프레임워크로 발전시킬 필요가 있다.
