
---
title: "[논문리뷰]KG-RAG: Bridging the Gap Between Knowledge and Creativity"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2024-06-26
last_modified_at: 2024-06-26
---

*Sanmartin, D*. (2024, May 20). **KG-RAG: Bridging the gap between knowledge and creativity**. arXiv.org. [https://arxiv.org/abs/2405.12035](https://arxiv.org/abs/2405.12035)

# Problem Statement

## 1. Hallucination
LLMs(대형 언어 모델)는 종종 사실과 일치하지 않는 정보를 생성하는 경향이 있다. 이는 "환각(Hallucination)"이라고 불리며, 모델이 존재하지 않는 사실을 만들어내거나 잘못된 정보를 제공하는 문제이다. 특히 LLMs에서 잘못된 정보를 마치 사실인 것 처럼 생성해내는 경우가 종종 발생하며, 이는 LLMs의 신뢰성을 떨어트린다. 

## 2. Catastrophic forgetting
재앙적 망각(Catastrophic forgetting)은 LLMs가 새로운 정보로 학습될 때 이전에 학습된 지식을 잊어버리는 문제를 말한다. 이는 모델이 지속적으로 새로운 데이터를 학습할 때 기존의 지식을 유지하기 어렵게 만든다. 

## 3. Processing Long-Context
LLMs는 긴 문맥을 처리하는 데 어려움을 겪는다. 긴 대화나 문서에서 중요한 정보를 놓치거나 잃어버리는 경우가 발생한다. 이는 긴 문서나 대화에서 연관된 정보를 유지하고 처리하는 데 한계를 보인다.

<br/>
<br/>

# Related Work



<br/>
<br/>

# Method



<br/>
<br/>

# Experiments



<br/>
<br/>


# Contribution

<br/>
<br/>

# Reference
