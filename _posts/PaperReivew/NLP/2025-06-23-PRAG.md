---
title: "[논문리뷰]Parametric Retrieval Augmented Generation"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-23
last_modified_at: 2025-06-23
---

*Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu*. “[**Parametric Retrieval Augmented Generation**](https://arxiv.org/abs/2501.15915).” In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2025).

# Problem Statement
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_1.png?raw=true">
</p>

<span style="font-size:110%">**Long Context Inefficiency**</span>  
문서를 context로 삽입하는 기존 RAG 방식은 문서 수가 증가할수록 입력 길이가 길어지며, 이에 따라 inference time과 memory usage가 기하급수적으로 증가한다. 특히 복잡한 QA 또는 generation task에서 context 길이가 attention cost에 직접 영향을 주어 성능 저하를 유발한다.

<span style="font-size:110%">**Shallow Knowledge Integration**</span>  
in-context 방식은 문서를 단지 입력으로 넣는 방식이므로, 해당 정보는 attention module의 key-value pair에만 영향을 주고, 모델의 내부 파라미터(즉, FFN 등)에 반영되지 않는다. 따라서 LLM이 보유한 internal knowledge처럼 활용되기 어려우며, 진정한 의미에서의 "내면화"가 불가능하다.

<span style="font-size:110%">**Insufficient Reasoning Capability**</span>  
multi-hop QA나 complex reasoning task에서는 단순한 context 추가만으로는 reasoning chain을 효과적으로 형성하기 어렵다. 문서 간 관계, 정보 연결, chain-of-thought 형성은 context-level 처리로는 한계가 있으며, 모델이 이를 내부적으로 이어서 reasoning하기엔 정보 통합 구조가 부족하다.

이 각각을 해결하기 위해 P-RAG에서는 세 가지 아이디어를 제안한다.
1. 문서를 입력에 넣지 않고 파라미터로 주입하는 “**Retrieve-Update-Generate**”
2. 문서의 지식을 LoRA 파라미터로 변환해 LLM 내부에 직접 통합하는 “**Parametric Injection via LoRA**”
3. 문서를 QA 쌍과 재작성 형태로 확장해 파라미터 학습에 활용하는 “**Document Augmentation and Parametric Encoding**”







![image](https://github.com/user-attachments/assets/35c4ff84-6816-40fc-a4d5-b2f18044280e)


<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_2.png?raw=true">
</p>

## Offline Document Parameterization Process
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_3.png?raw=true">
</p>

### Document Augmentation
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_4.png?raw=true">
</p>

<br/>

### Parametric Document Encoding
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_5.png?raw=true">
</p>

## Online Inference
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_6.png?raw=true">
</p>

<br/>
<br/>

# Experiments
## Main Results
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_7.png?raw=true">
</p>

## Ablation Study 1. LoRA Parameter Initialization
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_8.png?raw=true">
</p>

## Ablation Study 2. Document Augmentation
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_9.png?raw=true">
</p>

## Runtime Analysis
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_10.png?raw=true">
</p>


<br/>
<br/>

# Conclusion
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_12.png?raw=true">
</p>
