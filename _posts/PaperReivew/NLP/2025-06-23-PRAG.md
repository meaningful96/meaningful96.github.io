---
title: "[논문리뷰]Parametric Retrieval Augmented Generation"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-23
last_modified_at: 2025-06-23
---

*Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu*. “[**Parametric Retrieval Augmented Generation**](https://arxiv.org/abs/2501.15915).” In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2025).

# Problem Statement
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_1.png?raw=true">
</p>

<span style="font-size:110%">**Long Context Inefficiency**</span>  
문서를 context로 삽입하는 기존 RAG 방식은 문서 수가 증가할수록 입력 길이가 길어지며, 이에 따라 inference time과 memory usage가 기하급수적으로 증가한다. 특히 복잡한 QA 또는 generation task에서 context 길이가 attention cost에 직접 영향을 주어 성능 저하를 유발한다.

<span style="font-size:110%">**Shallow Knowledge Integration**</span>  
in-context 방식은 문서를 단지 입력으로 넣는 방식이므로, 해당 정보는 attention module의 key-value pair에만 영향을 주고, 모델의 내부 파라미터(즉, FFN 등)에 반영되지 않는다. 따라서 LLM이 보유한 internal knowledge처럼 활용되기 어려우며, 진정한 의미에서의 "내면화"가 불가능하다.

<span style="font-size:110%">**Insufficient Reasoning Capability**</span>  
multi-hop QA나 complex reasoning task에서는 단순한 context 추가만으로는 reasoning chain을 효과적으로 형성하기 어렵다. 문서 간 관계, 정보 연결, chain-of-thought 형성은 context-level 처리로는 한계가 있으며, 모델이 이를 내부적으로 이어서 reasoning하기엔 정보 통합 구조가 부족하다.

이 각각을 해결하기 위해 P-RAG에서는 세 가지 아이디어를 제안한다.
1. 문서를 입력에 넣지 않고 파라미터로 주입하는 “**Retrieve-Update-Generate**”
2. 문서의 지식을 LoRA 파라미터로 변환해 LLM 내부에 직접 통합하는 “**Parametric Injection via LoRA**”
3. 문서를 QA 쌍과 재작성 형태로 확장해 파라미터 학습에 활용하는 “**Document Augmentation and Parametric Encoding**”


<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_2.png?raw=true">
</p>

Parametric RAG(P-RAG)는 모든 문서를 사전에 파라미터 형식으로 저장하고, 사용자 질의에 대해 외부에서 관련 문서를 검색한 후 해당 문서의 파라미터를 LLM 내부에 일시적으로 삽입하는 방식이다. 사용자 질의가 주어지면 먼저 외부 코퍼스에서 관련 문서들을 검색하고, 각 문서에 대응하는 사전 학습된 파라메트릭 표현들을 불러온다. 검색된 문서들의 파라미터는 병합 과정을 거쳐 하나의 통합된 파라미터 표현으로 만들어지며, 이 병합된 파라미터가 LLM의 내부 파라미터 공간에 일시적으로 주입되어 업데이트된 모델이 질의에 대한 응답을 생성한다. 기존 in-context RAG가 문서를 프롬프트에 직접 삽입하여 지식을 활용하는 것과 달리, P-RAG는 문서를 LLM의 파라미터 수준에 통합함으로써 입력 길이를 늘리지 않으면서도 외부 지식을 모델 내부에 효과적으로 반영할 수 있다.

## Offline Document Parameterization Process
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_3.png?raw=true">
</p>

Offline Document Parameterization Process는 Parametric RAG의 핵심적인 사전 처리 단계로, 외부 문서들을 LLM의 파라미터 공간에 직접 통합할 수 있는 형태로 변환하는 과정이다. 이 과정은 기존 RAG 방법들이 추론 시점에 문서를 입력 컨텍스트에 추가하는 방식과 근본적으로 다른 접근법을 제시한다.

전체 프로세스는 두 가지 주요 단계로 구성된다. 첫 번째 단계인 Document Augmentation에서는 원본 문서 하나를 다양한 언어적 표현으로 재작성하고, 동시에 해당 문서의 내용을 기반으로 한 질문-답변 쌍들을 생성한다. 이러한 증강 과정의 목적은 모델이 문서의 핵심 정보를 단순한 토큰 단위의 암기가 아닌 의미적 이해를 통해 학습할 수 있도록 돕는 것이다.

두 번째 단계인 Parametric Document Encoding에서는 앞서 생성된 증강 데이터를 활용하여 **각 문서에 대응하는 독립적인 LoRA 파라미터를 학습**한다. 이 과정에서 LLM의 기본 가중치는 고정된 채로 유지되며, 오직 저랭크 행렬 형태의 LoRA 파라미터만이 학습된다. 이렇게 생성된 파라메트릭 표현은 해당 문서의 지식을 압축적으로 인코딩하고 있으며, 추론 시점에 LLM의 feed-forward network에 직접 통합될 수 있다.

이러한 오프라인 파라미터화 과정을 통해 각 문서는 고유한 파라미터 표현을 갖게 되며, 이는 <span style="color:red">**기존 방식처럼 긴 텍스트를 입력에 추가하는 대신 모델의 내부 지식 공간에 직접 삽입**</span>할 수 있는 형태가 된다. 결과적으로 이 방법은 추론 시 컨텍스트 길이 증가 없이도 외부 지식을 효과적으로 활용할 수 있게 한다.

### Document Augmentation
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_4.png?raw=true">
</p>

Document Augmentation은 Parametric RAG의 첫 번째 핵심 단계로, 원본 문서를 보다 풍부하고 다양한 형태로 변환하여 모델이 효과적으로 학습할 수 있도록 준비하는 과정이다. 첫 번째 작업은 **문서 재작성(Document Rewriting)**이다. 원본 문서 di를 동일한 사실적 내용을 유지하면서도 다양한 문체, 표현 방식, 조직 구조로 여러 번 재작성한다. 이를 통해 모델은 동일한 정보를 다양한 언어적 패턴으로 접할 수 있게 되어, 특정 표현에만 의존하지 않고 내용의 본질을 파악할 수 있다.

두 번째 작업은 **질문-답변 쌍 생성(QA Pair Generation)**이다. 원본 문서에서 추출할 수 있는 사실 정보를 바탕으로 여러 개의 질문과 그에 대응하는 답변을 생성한다. 이는 모델이 단순히 문서의 토큰을 순차적으로 예측하는 것을 넘어서, 문서 내 지식을 실제 질의응답 형태로 적용하는 능력을 학습하도록 돕는다.

최종적으로 재작성된 문서들과 생성된 QA 쌍들을 조합하여 증강된 문서 $$D_i$$를 생성한다. 이렇게 생성된 Di는 원본 문서보다 훨씬 다양하고 풍부한 학습 신호를 제공하며, 후속 단계인 Parametric Document Encoding에서 모델이 문서의 핵심 지식을 파라미터에 효과적으로 인코딩할 수 있는 기반을 마련한다.

<br/>

### Parametric Document Encoding
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_5.png?raw=true">
</p>

Parametric Document Encoding은 Document Augmentation 단계에서 생성된 증강 데이터를 활용하여 각 문서를 LLM의 파라미터 공간에 직접 통합할 수 있는 형태로 변환하는 과정이다. 이 단계는 외부 지식을 모델의 내부 파라미터에 직접 인코딩하는 Parametric RAG의 핵심 메커니즘을 구현한다.

먼저 LoRA 기반 파라미터 초기화가 수행된다. 각 문서 $$d_i$$에 대해 LLM의 Feed-Forward Network(FFN) 파라미터 행렬 W에 대응하는 저랭크 행렬 $$A$$와 $$B$$를 초기화한다. 이때 원본 가중치 행렬 $$W$$는 고정된 상태로 유지되며, 새로 도입된 저랭크 행렬 $$A, B$$만이 학습 가능한 파라미터가 된다. 업데이트된 가중치는 $$W' = W + ΔW = W + AB^T$$ 형태로 표현되며, 여기서 $$Δθ = \{A, B\}$$가 학습 대상이 된다.

학습 과정에서는 증강된 데이터 $$D_i$$의 각 트리플릿(재작성된 문서, 질문, 답변)을 연결하여 토큰 시퀀스 $$x = \[d_i^k \oplus q_i^j \oplus a_i^j\]$$를 구성한다. 이후 표준 언어 모델링 목적 함수(for Next token prediction)를 사용하여 LoRA 파라미터를 최적화한다. 이 과정을 통해 모델은 문서의 내용뿐만 아니라 질문-답변 형태의 지식 적용 능력까지 파라미터에 내재화하게 된다.

학습이 완료되면 생성된 **파라미터 표현 $$Δ\theta$$는 경량 문서별 지식 표현**으로 기능한다. 추론 시에는 기존 방식처럼 긴 문서 텍스트를 입력 컨텍스트에 추가하는 대신, 해당 문서에 대응하는 LoRA 파라미터만을 로드하면 된다. 이러한 파라미터 로딩 비용은 단일 토큰 디코딩 비용의 약 1%에 불과하여 매우 효율적이며, 결과적으로 컨텍스트 길이 증가 없이도 외부 지식을 모델에 직접 통합할 수 있게 된다.


## Online Inference
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_6.png?raw=true">
</p>

<br/>
<br/>

# Experiments
## Main Results
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_7.png?raw=true">
</p>

## Ablation Study 1. LoRA Parameter Initialization
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_8.png?raw=true">
</p>

## Ablation Study 2. Document Augmentation
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_9.png?raw=true">
</p>

## Runtime Analysis
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_10.png?raw=true">
</p>


<br/>
<br/>

# Conclusion
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.17%5DPRAG/PRAG_12.png?raw=true">
</p>
