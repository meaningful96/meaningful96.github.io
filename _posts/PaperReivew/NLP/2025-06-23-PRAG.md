---
title: "[논문리뷰]Parametric Retrieval Augmented Generation(SIGIR, 2025)"

categories: 
  - NR
  
toc: true
toc_sticky: true

date: 2025-06-23
last_modified_at: 2025-06-23
---

*Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu*. “[**Parametric Retrieval Augmented Generation**](https://arxiv.org/abs/2501.15915).” In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2025).

# Problem Statement
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_1.png?raw=true">
</p>

<span style="font-size:110%">**Long Context Inefficiency**</span>  
문서를 context로 삽입하는 기존 RAG 방식은 문서 수가 증가할수록 입력 길이가 길어지며, 이에 따라 inference time과 memory usage가 기하급수적으로 증가한다. 특히 복잡한 QA 또는 generation task에서 context 길이가 attention cost에 직접 영향을 주어 성능 저하를 유발한다.

<span style="font-size:110%">**Shallow Knowledge Integration**</span>  
in-context 방식은 문서를 단지 입력으로 넣는 방식이므로, 해당 정보는 attention module의 key-value pair에만 영향을 주고, 모델의 내부 파라미터(즉, FFN 등)에 반영되지 않는다. 따라서 LLM이 보유한 internal knowledge처럼 활용되기 어려우며, 진정한 의미에서의 "내면화"가 불가능하다.

<span style="font-size:110%">**Insufficient Reasoning Capability**</span>  
multi-hop QA나 complex reasoning task에서는 단순한 context 추가만으로는 reasoning chain을 효과적으로 형성하기 어렵다. 문서 간 관계, 정보 연결, chain-of-thought 형성은 context-level 처리로는 한계가 있으며, 모델이 이를 내부적으로 이어서 reasoning하기엔 정보 통합 구조가 부족하다.

이 각각을 해결하기 위해 P-RAG에서는 세 가지 아이디어를 제안한다.
1. 문서를 입력에 넣지 않고 파라미터로 주입하는 “**Retrieve-Update-Generate**”
2. 문서의 지식을 LoRA 파라미터로 변환해 LLM 내부에 직접 통합하는 “**Parametric Injection via LoRA**”
3. 문서를 QA 쌍과 재작성 형태로 확장해 파라미터 학습에 활용하는 “**Document Augmentation and Parametric Encoding**”

<br/>
<br/>

# Methodology
## Overview
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_2.png?raw=true">
</p>

Parametric RAG(P-RAG)는 모든 문서를 사전에 파라미터 형식으로 저장하고, 사용자 질의에 대해 외부에서 관련 문서를 검색한 후 해당 문서의 파라미터를 LLM 내부에 일시적으로 삽입하는 방식이다. 사용자 질의가 주어지면 먼저 외부 코퍼스에서 관련 문서들을 검색하고, 각 문서에 대응하는 사전 학습된 파라메트릭 표현들을 불러온다. 검색된 문서들의 파라미터는 병합 과정을 거쳐 하나의 통합된 파라미터 표현으로 만들어지며, 이 병합된 파라미터가 LLM의 내부 파라미터 공간에 일시적으로 주입되어 업데이트된 모델이 질의에 대한 응답을 생성한다. 기존 in-context RAG가 문서를 프롬프트에 직접 삽입하여 지식을 활용하는 것과 달리, P-RAG는 문서를 LLM의 파라미터 수준에 통합함으로써 입력 길이를 늘리지 않으면서도 외부 지식을 모델 내부에 효과적으로 반영할 수 있다.

## Offline Document Parameterization Process
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_3.png?raw=true">
</p>

Offline Document Parameterization Process는 Parametric RAG의 핵심적인 사전 처리 단계로, 외부 문서들을 LLM의 파라미터 공간에 직접 통합할 수 있는 형태로 변환하는 과정이다. 이 과정은 기존 RAG 방법들이 추론 시점에 문서를 입력 컨텍스트에 추가하는 방식과 근본적으로 다른 접근법을 제시한다.

전체 프로세스는 두 가지 주요 단계로 구성된다. 첫 번째 단계인 Document Augmentation에서는 원본 문서 하나를 다양한 언어적 표현으로 재작성하고, 동시에 해당 문서의 내용을 기반으로 한 질문-답변 쌍들을 생성한다. 이러한 증강 과정의 목적은 모델이 문서의 핵심 정보를 단순한 토큰 단위의 암기가 아닌 의미적 이해를 통해 학습할 수 있도록 돕는 것이다.

두 번째 단계인 Parametric Document Encoding에서는 앞서 생성된 증강 데이터를 활용하여 **각 문서에 대응하는 독립적인 LoRA 파라미터를 학습**한다. 이 과정에서 LLM의 기본 가중치는 고정된 채로 유지되며, 오직 저랭크 행렬 형태의 LoRA 파라미터만이 학습된다. 이렇게 생성된 파라메트릭 표현은 해당 문서의 지식을 압축적으로 인코딩하고 있으며, 추론 시점에 LLM의 feed-forward network에 직접 통합될 수 있다.

이러한 오프라인 파라미터화 과정을 통해 각 문서는 고유한 파라미터 표현을 갖게 되며, 이는 <span style="color:gold">**기존 방식처럼 긴 텍스트를 입력에 추가하는 대신 모델의 내부 지식 공간에 직접 삽입**</span>할 수 있는 형태가 된다. 결과적으로 이 방법은 추론 시 컨텍스트 길이 증가 없이도 외부 지식을 효과적으로 활용할 수 있게 한다.

### Document Augmentation
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_4.png?raw=true">
</p>

Document Augmentation은 Parametric RAG의 첫 번째 핵심 단계로, 원본 문서를 보다 풍부하고 다양한 형태로 변환하여 모델이 효과적으로 학습할 수 있도록 준비하는 과정이다. 첫 번째 작업은 **문서 재작성(Document Rewriting)**이다. 원본 문서 di를 동일한 사실적 내용을 유지하면서도 다양한 문체, 표현 방식, 조직 구조로 여러 번 재작성한다. 이를 통해 모델은 동일한 정보를 다양한 언어적 패턴으로 접할 수 있게 되어, 특정 표현에만 의존하지 않고 내용의 본질을 파악할 수 있다.

두 번째 작업은 **질문-답변 쌍 생성(QA Pair Generation)**이다. 원본 문서에서 추출할 수 있는 사실 정보를 바탕으로 여러 개의 질문과 그에 대응하는 답변을 생성한다. 이는 모델이 단순히 문서의 토큰을 순차적으로 예측하는 것을 넘어서, 문서 내 지식을 실제 질의응답 형태로 적용하는 능력을 학습하도록 돕는다.

최종적으로 재작성된 문서들과 생성된 QA 쌍들을 조합하여 증강된 문서 $$D_i$$를 생성한다. 이렇게 생성된 Di는 원본 문서보다 훨씬 다양하고 풍부한 학습 신호를 제공하며, 후속 단계인 Parametric Document Encoding에서 모델이 문서의 핵심 지식을 파라미터에 효과적으로 인코딩할 수 있는 기반을 마련한다.

<br/>

### Parametric Document Encoding
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_5.png?raw=true">
</p>

Parametric Document Encoding은 Document Augmentation 단계에서 생성된 증강 데이터를 활용하여 각 문서를 LLM의 파라미터 공간에 직접 통합할 수 있는 형태로 변환하는 과정이다. 이 단계는 외부 지식을 모델의 내부 파라미터에 직접 인코딩하는 Parametric RAG의 핵심 메커니즘을 구현한다.

먼저 LoRA 기반 파라미터 초기화가 수행된다. 각 문서 $$d_i$$에 대해 LLM의 Feed-Forward Network(FFN) 파라미터 행렬 W에 대응하는 저랭크 행렬 $$A$$와 $$B$$를 초기화한다. 이때 원본 가중치 행렬 $$W$$는 고정된 상태로 유지되며, 새로 도입된 저랭크 행렬 $$A, B$$만이 학습 가능한 파라미터가 된다. 업데이트된 가중치는 $$W' = W + ΔW = W + AB^T$$ 형태로 표현되며, 여기서 $$Δθ = \{A, B\}$$가 학습 대상이 된다.

학습 과정에서는 증강된 데이터 $$D_i$$의 각 트리플릿(재작성된 문서, 질문, 답변)을 연결하여 토큰 시퀀스 $$x = [d_i^k \oplus q_i^j \oplus a_i^j]$$를 구성한다. 이후 표준 언어 모델링 목적 함수(for Next token prediction)를 사용하여 LoRA 파라미터를 최적화한다. 이 과정을 통해 모델은 문서의 내용뿐만 아니라 질문-답변 형태의 지식 적용 능력까지 파라미터에 내재화하게 된다.

학습이 완료되면 생성된 **파라미터 표현 $$Δ\theta$$는 경량 문서별 지식 표현**으로 기능한다. 추론 시에는 기존 방식처럼 긴 문서 텍스트를 입력 컨텍스트에 추가하는 대신, 해당 문서에 대응하는 LoRA 파라미터만을 로드하면 된다. 이러한 파라미터 로딩 비용은 단일 토큰 디코딩 비용의 약 1%에 불과하여 매우 효율적이며, 결과적으로 컨텍스트 길이 증가 없이도 외부 지식을 모델에 직접 통합할 수 있게 된다.


## Online Inference
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_6.png?raw=true">
</p>

Online Inference는 사전에 파라미터화된 문서들을 활용하여 실제 질의응답을 수행하는 단계로, Retrieve-Update-Generate(RUG) 워크플로우를 통해 진행된다. 이 과정은 기존 RAG 방법들과 달리 문서 텍스트를 입력에 추가하지 않고도 외부 지식을 활용할 수 있는 혁신적인 접근법을 제시한다.
첫 번째 단계인 Retrieve에서는 BM25를 사용하여 사용자 질의와 관련성이 높은 상위 k개의 문서를 검색한다. 검색된 각 문서는 오프라인 단계에서 미리 계산된 파라미터 표현을 갖고 있으며, 검색 결과로 얻은 문서 ID를 기준으로 외부 저장소에서 해당하는 **LoRA 파라미터 $$Δ \theta_i = \{A_i, B_i\}$$를 로드**한다. 이 과정에서 문서 텍스트 자체는 불러오지 않고 오직 파라미터만을 메모리에 로드한다.

두 번째 단계인 Update에서는 검색된 모든 문서의 LoRA 파라미터를 하나로 통합한다. **LoRA 파라미터 병합** 과정에서 $$ΔW_{\text{merge}} = \alpha · \sum(j=1 \text{to} k) A_j B_j^T$$ 공식을 사용하여 여러 문서의 저랭크 행렬들을 가중합으로 결합한다. 이후 **FFN 가중치 업데이트**를 통해 원본 Feed-Forward Network의 가중치 행렬에 병합된 업데이트를 적용하여 $$W' = W + ΔW_{\text{merge}}$$ 형태로 모델을 일시적으로 수정한다.

마지막 단계인 Generate에서는 업데이트된 LLM이 외부 문서를 입력으로 받지 않고 <span style="color:gold">**오직 사용자 질문만을 입력받아 답변을 생성**</span>한다. 이때 모델의 파라미터에는 이미 관련 문서들의 지식이 통합되어 있어, 긴 컨텍스트 없이도 외부 지식을 활용한 정확한 답변을 생성할 수 있다. 이러한 접근법은 추론 시 계산 비용과 메모리 사용량을 크게 줄이면서도 효과적인 지식 활용을 가능하게 한다.

<br/>
<br/>

# Experiments
## Main Results
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_7.png?raw=true">
</p>

- **P-RAG**: 검색된 문서들의 LoRA 매개변수만 LLM에 주입하고, 문서는 컨텍스트에 추가하지 않음
- **DA-RAG**: Document Augmentation으로 증강된 문서들을 기존 in-context RAG 방식으로 프롬프트에 추가
- **Combine Both**: P-RAG(매개변수 주입) + Standard RAG(컨텍스트 추가)를 동시에 적용

Parametric RAG의 실험 결과는 네 가지 벤치마크 데이터셋에서 기존 RAG 방법들과 비교하여 우수한 성능을 보여준다. 2WikiMultihopQA와 HotpotQA 같은 Multi-hop 추론 데이터셋에서 특히 뛰어난 성능을 보이는데, 이는 파라미터에 직접 통합된 지식이 복잡한 다단계 추론 과정에서 더 효과적으로 활용되기 때문이다.

PopQA와 ComplexWebQuestions(CWQ)는 본래 Knowledge Graph 기반의 사실적 질문응답 데이터셋이지만, 이 연구에서는 Wikipedia articles를 외부 지식 소스로 활용하여 Parametric RAG를 적용했다. 각 Wikipedia 문서에 대해 Document Augmentation을 거쳐 LoRA 매개변수를 생성하고, 이를 통해 구조화된 지식 그래프 정보를 텍스트 기반으로 학습할 수 있도록 했다.

실험에서 비교한 주요 방법들은 다음과 같다. **P-RAG(제안 방법)**는 검색된 문서들의 LoRA 매개변수만을 LLM에 주입하고 문서 텍스트는 컨텍스트에 추가하지 않는다. DA-RAG는 Document Augmentation으로 증강된 문서들을 기존 in-context RAG 방식으로 프롬프트에 추가하여, 증강 데이터 자체의 효과를 검증한다. Combine Both는 P-RAG의 매개변수 주입과 Standard RAG의 컨텍스트 추가를 동시에 적용한 하이브리드 접근법이다. 특히 주목할 점은 모델 크기가 클수록 P-RAG의 성능 향상이 더욱 두드러진다는 것이다. LLaMA-8B에서 가장 큰 성능 차이를 보이는데, 이는 더 큰 모델이 파라미터에 내재화된 지식을 더 효과적으로 활용할 수 있기 때문이다. 또한 Combine Both가 전반적으로 최고 성능을 달성한 것은 파라미터 기반 지식 주입과 컨텍스트 기반 지식 주입이 상호 보완적으로 작용함을 시사한다.

## Ablation Study 1. LoRA Parameter Initialization
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_8.png?raw=true">
</p>

Table2는 LoRA 파라미터의 초기화 방법에 따른 성능 차이를 보여준다.
- **Random Initialization(P-RAG Rand.)**: 모든 실험의 기본값으로, LoRA 파라미터를 랜덤하게 초기화.
- **Warm-Up Initialization(P-RAG Warm.)**: QA pair 600개로 미리 프리트레이닝 후 초기화.

LoRA Parameter Initialization에 대한 Ablation Study는 파라미터 초기화 전략이 성능에 미치는 영향을 분석한 실험이다. 두 가지 초기화 방법을 비교했다. **Random Initialization(P-RAG Rand.)**은 LoRA 파라미터를 완전히 랜덤하게 초기화하는 기본 방법이고, **Warm-Up Initialization(P-RAG Warm.)**은 600개의 QA 쌍으로 사전 훈련한 후 초기값으로 사용하는 방법이다. 실험 결과 모든 모델과 데이터셋에서 **Warm-up 방식이 일관되게 우수한 성능**을 보인다. 이는 LoRA 파라미터가 베이스 LLM과 효과적으로 정렬되고 태스크 관련 정보를 사전에 내재화할 수 있음을 의미한다. 특히 특정 도메인이나 태스크에 적용할 때 소량의 QA 쌍으로 사전 훈련을 수행하면 성능을 크게 개선할 수 있다는 실용적 시사점을 제공한다.

## Ablation Study 2. Document Augmentation
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_9.png?raw=true">
</p>

- **w/o QA**: QA generation 없이 원본 문서 + rewriting만 사용하여 LoRA 훈련
- **w/o Rewrite**: Rewriting 없이 원본 문서 + QA generation만 사용하여 LoRA 훈련
- **w/o Both**: QA generation과 rewriting 둘 다 없이 원본 문서만으로 LoRA 훈련
- **with Both**: QA generation + rewriting 모두 사용하여 증강된 데이터로 LoRA 훈련

Document Augmentation에 대한 Ablation Study는 Document Augmentation의 두 핵심 구성 요소인 Document Rewriting과 QA Generation이 각각 성능에 미치는 영향을 분석한 실험이다.

실험은 네 가지 설정으로 진행되었다. w/o Both는 증강 없이 원본 문서만으로 LoRA를 훈련하는 방법이고, w/o QA는 QA 생성 없이 원본 문서와 재작성된 문서만 사용하는 방법이다. w/o Rewrite는 재작성 없이 원본 문서와 QA 쌍만 사용하고, with Both는 모든 증강 기법을 적용한 완전한 P-RAG 방법이다.

실험 결과 w/o Both에서 성능이 가장 크게 저하되어, 단순히 원본 문서를 next-token prediction으로 학습하는 것만으로는 지식이 효과적으로 내재화되지 않음을 보여준다. <span style="color:gold">**QA Generation이 Rewriting보다 더 큰 성능 향상**</span>을 가져오는데, 이는 모델이 사실 정보를 질의응답 형태로 적용하는 능력을 학습하는 것이 더 중요함을 의미한다. 최종적으로 with Both가 모든 데이터셋에서 최고 성능을 달성하여, Rewriting과 QA Generation이 상호 보완적으로 작용함을 확인할 수 있다. Rewriting은 다양한 언어적 표현을 통한 일반화를, QA Generation은 지식의 실제 적용 능력을 각각 향상시키는 역할을 한다.

## Runtime Analysis
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_10.png?raw=true">
</p>

Runtime Analysis는 Parametric RAG의 계산 효율성을 실증적으로 검증한 실험으로, 추론 시점의 실제 소요 시간을 측정하여 기존 RAG 방법들과 비교했다. 실험 결과 P-RAG는 Standard RAG 대비 29~36%의 시간 단축을 달성했다. 2WQA에서는 3.03초에서 2.34초로, CWQ에서는 2.82초에서 2.07초로 단축되어 각각 1.29배, 1.36배의 속도 향상을 보인다. 이는 긴 문서 텍스트를 입력 컨텍스트에 추가하지 않고 **LoRA 파라미터만 로드하는 방식의 효율성**을 보여준다.

특히 FLARE와 DRAGIN 같은 Multi-round RAG 방법들은 질의당 최대 5배 이상의 지연시간을 보인다. 이들은 불확실한 토큰을 만날 때마다 추가 검색을 수행하기 때문이다. 반면 P-RAG는 단일 LoRA merge/load 연산만 추가로 필요하여 계산 오버헤드가 최소화된다. 주목할 점은 P-RAG와 Combine Both 방법에서 "+0.32"로 표시된 LoRA 병합 및 로딩 시간이다. 논문에서는 이 시간이 현재 구현의 메모리 로딩 최적화 부족으로 인한 것이며, 엔지니어링 개선을 통해 더욱 단축될 수 있다고 언급한다. 결과적으로 P-RAG는 성능 향상과 동시에 추론 효율성까지 확보한 혁신적인 접근법임을 확인할 수 있다.


<br/>
<br/>

# Conclusion
<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/blob/main/Paper_Review/%5B2025.06.23%5DPRAG/PRAG_12.png?raw=true">
</p>

**Contribution**
- 컨텍스트 길이 한계와 온라인 추론 비용 문제를 동시에 해결하였다.
- 기존 in-context RAG와 상호 보완적으로 결합이 가능한 확장성 있는 지식 증강 프레임워크를 제공하였다.
- RAG 연구가 “지식의 저장 혹은 주입 위치”를 보다 유연하게 설계할 수 있음을 실험적으로, 이론적으로 제시하였다.

**Limitations**
- 파라메트릭 representation 생성과 저장에 높은 연산 및 메모리 비용이 발생한다.  즉, 오프라인에서 각 문서별로 LoRA 파라미터를 학습하고 저장해야 하므로, 원본 텍스트 대비 메모리 요구량이 커지며, 대규모 corpus에는 확장성이 제한적일 수 있다.
- Document Augmentation 과정에서 rewriting된 문서와 QA Pair의 정확한 매칭이 보장되지 않는다. 즉, rewriting된 문서가 해당 QA를 풀 수 있는지를 검증하거나 정렬하는 과정은 없다.

