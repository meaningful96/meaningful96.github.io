---
title: "[논문리뷰]RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"

categories: 
  - NR

  
toc: true
toc_sticky: true

date: 2024-06-25
last_modified_at: 2024-06-25
---

*Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S.*, & Kiela, D. (2020, May 22). **Retrieval-Augmented Generation for Knowledge-Intensive NLP tasks**. arXiv.org. https://arxiv.org/abs/2005.11401

# Problem Statement
## 1. 정보의 한계성
기존의 언어 모델은 주로 training set에서 학습한 정보에 근거하여 답변을 생성한다. 이는 모델이 <span style="color">학습한 데이터 범위 내의 정보만 제공할 수 있다는 한계를 가진다. 즉, 모델이 학습하지 않은 최신 데이터나 특수한 Knowledge에 대해서는 답변을 제공하기 어렵다

## 2. Hallucination 
Hallucination이란 문제가 질문(Query)에 대한 오답을 마치 정답처럼 생성해내는 것을 말한다. LLM에서는 특히 이 Hallucination 문제가 매우 중요한 한계로 작용하고 있다. 모델이 학습한 데이터셋의 크기가 작거나 vocabulary 사이즈가 너무 작은 경우, 혹은 overfitting이 되었거나 데이터의 qulity가 낮은 경우 발생한다.

Retrieval-Augmented Generation(RAG)는 이러한 문제점을 해결하기 위해 <span style="color:gold">검색 기반의 접근</span>을 하며 <span style="color:gold">생성 모델과의 통합</span>을 통해 해결하고자 하였다.

<br/>
<br/>

# Related Work
## 1. Open Domain Question Answering(ODQA)
Open Domain Question Answering은 보통 Open Domain QA 혹은 ODQA로 불린다. 이 task는 매우 넓은 범위의 주제에 대해 질문(query)에 답할 수 있는 모델을 설계하는 것을 목표로 한다. 이러한 질문은 특정 도메인에 국한되지 않으며, 시스템은 인터넷이나 데이터베이스와 같은 넓은 범위의 지식에서 답변을 검색한다.

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/e9ff7c5f-e5c2-49f3-9afa-580bb7781d9b" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>


쉽게 말해, 입력으로 들어온 query에 대해 Wikidata와 같은 외부 데이터베이스에서 관련된 문서(passage)를 search하고, query와 관련된 case를 여러 개 가지고와 모델이 이를 통해 가장 정합한 case를 고르고 답변을 추론하는 것이다. (즉, 모델이 Input sequence와 함께 외부 데이터의 문서를 같이 이용해서 추론을 한다.) 또한 답변(answer)는 "바나나, 사과, 수박"과 같이 연속된 여러 개의 토큰으로 출력이 가능하다. 외부 데이터의 문서를 검색하기 때문에 <span style="color:gold">답변이 외부 데이터베이스나 질문에 존재가 가능</span>하다. 

- Example
  1. 질문(Query): "브라질의 수도는 어디인가요?"
    - 답변(Answer): "브라질리아"
    - 이 때 모델은 브라질의 지리적, 정치적 정보를 검색하여 "브라질리아"를 답변한다.
  2. 질문(Query): "양조의 과정은 어떤 것이 있나요?"
    - 답변(Answer): "제맥아, 제분, 담금, 끓임, 발효, 숙성, 여과, 포장"
    - Wikidata에서 "양조가 이루어지는 데에는 제맥아, 제분, 담금, 끓임, 발효, 숙성, 여과, 포장 등의몇 가지 과정을 거친다"를 검색하여 답변을 추론한다.

## 2. Knowledge Intensive Task(KIT)
Knowledge Intensive Task는 특정 지식이 요구되는 작업을 지칭한다. 이런 작업들은 단순한 정보 검색을 넘어서, 복잡한 추론, 문맥 이해, 전문 지식을 필요로 한다. 특히 사실 확인(fact checking)문제가 대표적인 예이다. ODQA와 달리 KIT는 더 복잡한 추론과 전문 지식을 필요로 하며, 답변을 생성하기 위해 여러 출처의 정보를 통합하고 유추하는 과정이 필요할 수 있다. 따라서 <span style="color:gold">질문(query)이나 검색된 외부 데이터(passage)에 정답 토큰이 직접적으로 존재하지 않을 수 있다</span>.

- Example (Fact Checking)
  1. 질문(Query): "한국의 수도는 강원도 태백이다." 
    - 답변(Answer): "거짓"
    - Wikidata에 검색된 passage: 대한민국의 수도는 서울특별시이며 한강이 도시를 관통한다.

이처럼 질문과 검색된 외부데이터(passage)어디에도 거짓이라는 토큰이 존재하지 않음에도 불구하고 모델이 "거짓"을 답변으로 추론할 수 있다. 

## 3. ODQA와 KIT를 푸는 모델들의 종류

Data 형식
- Question($$q$$): 질문(Query)에 해당하며 외부 지식없이 쉽게 답할 수 없다.
- Answer($$a$$): 답변, Passage 내에 연속된 span으로 존재한다고 보장이 불가능하다.
- Knowledge Base($$KB$$): 외부 지식 데이터 베이스, 수백만 ~ 수억 개의 문서로 구성되며 대표적으로 Wikidata가 이에 해당한다.
- Passage($$p$$): 질문과 관련된 문서, KB에서 선택된 문서이다.

모델은 크게 세 가지로 나뉘어 진다. Retreiver, Reader와 Generator가 있으며, Reader와 Generator는 방법론에 따라 달리 선택된다.

### 1) Retriever
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/ecbc564a-3a89-495f-a461-8b32ef9c8448" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

Retriever는 **질문과 관련된 passage를 KB에서 찾는 모델**이다. BERT와 같은 Encoder 구조를 가진다. 위의 그림처럼 Bi-encoder의 형태일 수 있다. 위에서는 정답을 추론하기 위해 score를 (i)Passage 인코더와 (ii)Question 인코더의 출력 벡터의 내적으로 삼았다. 이 때, Passage 인코더에서 출력된 벡터가 일종의 weight로 동작하게 된다. score값이 높을 수록 검색된 문서가 정답 추론에 중요한 역할을 한다는 것을 의미한다.

<br/>

### 2) Reader
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/9b7072eb-fcea-489c-99d2-78f2a1829e6e" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

Reader는 **passage에서 정답 후보가 될 수 있는 Span을 찾는 모델**이다. 먼저 BERT가 질문과 passage text를 함께 입력으로 받아 hidden representation을 출력한다. 이후, Reader가 시작 (CLS)과 끝 (SEP) 토큰의 Hidden representation을 concatenation한 벡터를 입력으로 받고 최종 score를 산출한다. 한 query에 대해 top-k개의 passage마다 score를 각각 산출하고 Marginalize하여 최종 정답을 선택한다.

<br/>

### 3) Generator
Generator는 <span style="color:gold">passage와 짛질문을 입력으로 하여 정답을 생성하는 몯모델</span>이다. Reader와 달리 Marginalize하는 과정이 없으며, Generation을 하기 위해 BART나 T5와 같이 Encoder-Decoder 구조를 가진다.

## 4. 외부 지식(passage)이 필요한 이유.
만약 Open Domain QA를 푼다고 가정할 때 외부 지식이 없다고 가정하면, 학습된 데이터로만 정답을 추론하게 된다. 이 때 질문 $$q$$는 외부 지식 없이 쉽게 답할 수 없는 질문이며, 정답 $$a$$는 passage 내에 연속된 span으로 존쟇재한다고 보장이 불가능하다. 이러한 상황에서 외부 지식을 사용하지 않을 경우 정답에 대한 확률 값은 **질문이 주어졌을 때 정답의 Likelihood**가 되며 이를 수식으로 쓰면 $$max P(a \vert q)$$이다. 즉, 질문에 대한 정답의 확률값을 모두 구하고 이 확률이 최대가 되는 값을 정답으로 삼는 것이다.

Related Work Section에서 예시로 "양조의 과정은 어떤 것이 있나?"라는 질문을 했을 때 정답을 "제맥아, 제분, 담금, 끓임, 발효, 숙성, 여과, 포장"라고 답했고, 이는 외부 지식(passage)에 연속된 형태로 존재하던 토큰이다. 즉, Open Domain QA에서 이 외부 지식이 없으면 위와 같은 정답을 유추해내지 못한다. 다시 말해 <u>ODQA에서 QA만으로 task 수행이 불가능</u>하다. (이 수식을 베이즈 정리를 적용한다 하더라도 정답에 대한 질문의 확률 분포를 모른다.)

반면 passage를 사용하게 되면 $$max P(a \vert q)$$의 식은 $$max P(a, p \vert q)$$로 바뀌게 된다. 이는 베이즈 정리(Bayes' theory)에 의해 $$max P(a, p \vert q) = max P(a \vert p, q)P(p \vert q)$$로 바뀌게 된다. 식의 의미를 생각해보면 다음과 같다.

(i) $$P(a \vert p, q)$$는 질문과 passage가 입력으로 들어왔을 때 정답의 likelihood가 최대화되도록 학습.  
(ii) $$P(p \vert q)$$는 해당 질문에 대해 가장 적합한 passage의 likelihood를 최대화하도록 학습  

다시 말해 외부 지식이 있으면 ODQA의 정답에 대한 liklihood는 (i)Passage의 확률분포와 (ii)Answer에 대한 Passage의 liklihood로 분해되어 쉽게 풀 수 있다.

## 5. 모델의 발전 양상
<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/a4a72737-e686-42d8-b9a8-52ed487d532e" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>


위에처럼 기존에는 Retriever-Reader를 중심으로 Open Domain QA문제를 푸는 것을 목표로 했다. RAG가 가지는 중요한 의미는 ODQA task를 KIT와 결합하여 확장했다는 것이다. RAG이전의 모델들은 간단하게 정리하면 다음과 같다.

- **DrQA**: Retriever에 대한 training이 불가하여 passage 개선이 불가능하다.
- **ORQA**: Retriever를 학습하는 것이 가능하다. 하지만, passage를 Latent variable에 도입하는 모델링($$P(a, p \vert q) = P(a \vert p, q)P(p \vert q)$$)이 불가능하다.
- **REALM**: Retriever 업데이트 시 필요한 정보는 "정답 추론 시 passage가 도움이 되는 정도"이다. REALM을 통해 Passage를 Latent Variable로 도입한 모델링이 가능하다.

<figure style="text-align: center; margin: auto;">
  <img width="1000" alt="1" src="https://github.com/meaningful96/Blogging/assets/111734605/2a17f6c3-a686-44d9-81a1-8c274030f956" style="display: block; margin: auto;">
  <figcaption style="font-size:70%; text-align: center; width: 100%; margin-top: 0px;">
    <em>ref) 고려대학교 산업경영공학부 DSBA Lab</em>
  </figcaption>
</figure>

이와 관련된 [PDF](https://github.com/meaningful96/Blogging/blob/main/Paper_Review/RAG/Before_RAG.pdf)이다.  
<span style="font-size: 80%">※ 이 자료는 고려대학교 DSBA Lab의 자료이다.</span>
  
<br/>
<br/>

# Method


<br/>
<br/>

# Experiments

<br/>
<br/>

# Contribution

<br/>
<br/>

# Reference
Paper: [RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)      
Video: [고려대학교 산업경영공학부 DSBA](https://www.youtube.com/watch?v=gtOdvAQk6YU)

