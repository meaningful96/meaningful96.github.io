---
title: "[논문리뷰]GPT-1, GPT-2, GPT-3 Model Architecture 비교"

categories: 
  - PaperReview
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-03-02
last_modified_at: 2023-03-02
---

# GPT-1

GPT-1: Improving Langauge Understanding by Generative Pre-Training

## Related Work
### 1) Semi-Supervised Learning for NLP

Semi-Supervised Learning이란 적은 Labeled data와 추가로 활용할 수 있는 대용량의 Unlabled data로 학습시키는 방법이다. '준지도 학습'이라고도 불리며 <u>labeled data에 한하여Supervised learning적용</u>하고, <u>대용량 unlabled data에는 unsupervised learning을 적용</u>해 추가적인 성능향상을 목표로 하는 방법론이다. 이 방법이 가능할 것이라는 믿음은 label을 맞추는 모델에서 벗어나 데이터 자체의 본질적인 특성이 모델링 된다면 소량의 labled data를 통한 약간의 **가이드 라인**으로 일반화 성능을 끌어올릴 수 있다는 것이다.

<p align="center">
<img width="500" alt="1" src="https://user-images.githubusercontent.com/111734605/229306428-b7e2deeb-e9ad-448e-9b77-5a527073629d.png">
</p>

Semi-Supervised learning의 목적함수는 supervised loss $$L_s$$와 unsupervised loss $$L_u$$의 합을 최소하하는 것으로 표현할 수 있다. 다시 말해, <span style = "color: aqua">**Supervised와 Unsupervised를 한번에 학습**</span>한다. 이것이 2-Stage로 이루어지는 self-supervised learning과 transfer learning와의 차이점이다.

<span style = "font-size:120%">$$Loss = L_s + Lu$$</span>

<br/>

### 2) Unsupervised Pre-Training

신경망을 제대로 학습시키기 위해서는 많은 양의 labeled data가 필요하다. 하지만, 실제로 이런 labeling된 데이터는 턱없이 부족하므로 Unlabeled data를 사용하는 경우가 많다.
이 경우 label이 없는 학습 데이터를 이용해 신경망의 각 layer를 앞서 설명한 방식으로 사전 학습(pre-training)을 시킬 수 있다.

비록 Pre-training 방식이 linguistic information을 포착하는데 도움을 준다고해도, LSTM 모델들의 사용은 짧은 범위 안(적은 Token 수)에서만 예측 가능하다. LSTM이나 Seq2Seq를 기반으로 하는 자연어 처리 모델들은 Long-Term Dependency가 발생한다. 대조적으로, Transformer 기반의 pre-training 모델은 보다 넓은 범위의 linguistic information을 포착한다.

<br/>

### 3) Auxiliary Training objectives

Auxiliary feature 또는 Auxiliary unsupervised training objective를 추가해주는 것은 Semi-supervised learning의 한 형태이다. POS tagging, chunking, named entity recognition과 language modeling 같은 NLP task를 풀 때 이러한 방법을 이용해 성능을 향상시킨 연구가 많이 진행되었다.

## Model Architecture

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229307436-761c9e37-7c5d-4c57-9d91-8e168e381dbf.png">
</p>

논문에서는 unlabeled text data의 활용 문제를 개선하기 위한 **Semi-supervised model, GPT**를 제안한다. GPT는 기본적으로 Transformer 구조로 이루어져 있어, text sequence의 long-term dependency를 다룰 수 있다. 또한 GPT는 두 가지 학습 단계를 통해 최소한의 구조 변화로 Target Task에 Transfer 가능한 언어 모델이다.

- GPT의 두 가지 학습 단계
  1. Unsupervised Pre-Training
  2. Supervised Fine-Tuning


### 1) Stage 1. Unsupervised Pre-Training

첫 번째 학습 단계 Unspervised pre-training은 unlabeled token $$u = {u_1, \cdots, u_n}$$를 통해 일반적인 언어모델의 목접함수 Likelihood $$L_1(u)$$를 최대화 하는 과정이다.

<p align="center">
<img width="300" alt="1" src="https://user-images.githubusercontent.com/111734605/229307787-cc907d38-aed8-432f-b433-4ac7b58703dd.png">
</p>

<p align="center">
<img width="500" alt="1" src="https://user-images.githubusercontent.com/111734605/229307672-48268715-ffb4-4fad-9176-07e7db7af676.png">
</p>


<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229304685-53dab5aa-479f-46c9-b613-b6a38140616c.png">
</p>

<br/>
<br/>

# GPT-2



<br/>
<br/>

# GPT-3



<br/>
<br/>

# Reference
[GPT-1]("https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf")  
[GPT-2]("https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf")  
[GPT-3]("https://arxiv.org/pdf/2005.14165.pdf")    
[고려대학교 산업경영공학부 DSBA 연구실 youtube]("https://www.youtube.com/@dsba2979")

bert는 Masked Langauge Model이니 방향성이 Bidirectional
GTP2는 Auto Regressive이다. 한 토큰이 만들어지고 그 만들어진 토큰이 다음 토큰을 만드는데 인풋으로 쓰인다.
