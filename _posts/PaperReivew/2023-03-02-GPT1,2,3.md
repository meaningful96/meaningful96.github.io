---
title: "[논문리뷰]GPT-1, GPT-2, GPT-3 Model Architecture 비교"

categories: 
  - PaperReview
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-03-02
last_modified_at: 2023-03-02
---

# GPT-1

GPT-1: Improving Langauge Understanding by Generative Pre-Training

## Model Architecture

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229304685-53dab5aa-479f-46c9-b613-b6a38140616c.png">
</p>



# GPT-2
# GPT-3

# Reference
[GPT-1]("https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf")  
[GPT-2]("https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf")  
[GPT-3]("https://arxiv.org/pdf/2005.14165.pdf")    
[고려대학교 산업경영공학부 DSBA 연구실 youtube]("https://www.youtube.com/@dsba2979")

bert는 Masked Langauge Model이니 방향성이 Bidirectional
GTP2는 Auto Regressive이다. 한 토큰이 만들어지고 그 만들어진 토큰이 다음 토큰을 만드는데 인풋으로 쓰인다.
