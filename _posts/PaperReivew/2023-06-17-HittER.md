![image](https://github.com/meaningful96/meaningful96.github.io/assets/111734605/5fe57954-ac65-4e59-9deb-9f8a37906db1)---
title: "[논문리뷰]HittER: Hierarchical transformers for knowledge graph embeddings"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-06-17
last_modified_at: 2023-06-17
---

Chen, S., Liu, X., Gao, J., Jiao, J., Zhang, R., & Ji, Y. (2021, October 6). HittER: Hierarchical Transformers for Knowledge Graph Embeddings. ArXiv.org. https://doi.org/10.48550/arXiv.2008.12813

# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/421f0cd0-9456-4493-8e28-b73d26a27e0d">
</p>

<span style = "font-size:115%"><b>1. Knowledge Graph는 여전히 불완전(Incomplete)하고 Noisy하다.</b></span>

Knowledge Graph는 Heterogeneous graph의 일종이다. 그리고 그래프 형식으로 정보를 저장할 때 사람이 직접 <span style = "color:gold"><b>수동으로(manually)</b></span> 넣어줘야하고, 이는 데이터 유실로 이어질 수 있다.
```
The first major problem is the incompleteness and noise in the knowledge graph.
This issue arises from the fact that much of the information in the graph is
manually inputted by humans so that it leads to potential loss of data.
```

<span style = "font-size:115%"><b>2. 그래프 임베딩 모델(Graph Embedding Method, KGE)의 한계</b></span>

그래프 임베딩은 기본적으로 하나의 벡터 공간에 그래의 구조 정보를 표현하는 것에 초점을 맞춘다. 예를 들어, head와 relation의 합을 tail이라 정의할 수 있다(TranE 모델). 그래프 임베딩 방식은 
<b>그래프의 구조 정보</b>만을 활용한다. 한 노드를 중심으로 이웃 노드들의 정보를 취합(aggregation)한다. 다시 말해, 저차원의 벡터 공간에서 지리적인, 구조적인 특성을 이용하는 것이 KGE이다. 

이렇게 할 경우 취합된 모든 정보를 벡터 공간에서 하나의 단일 벡터에 저장해야 하기떄문에 정보의 유실이 생길 수 있다(Seq2Seq 모델의 단점과 비슷: context vector에 정보를 압축). 정리하면 그래프 임베딩 방식(KGE, Knowledge Graph Embedding)의 경우 <span style = "color:gold"><b>1. 오직 그래프의 구조 정보만을 활용</b></span>하며 <span style = "color:gold"><b>2. 하나의 단일 벡터에 정보를 압축해서 정보의 손실이 발생</b></span>하는 한계점들이 존재한다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/e051b97c-47b4-4a64-b97a-a95d43be05dd">
</p>

위의 Sub-graph로 Link prediction을 진행한다고 가정해보자. Incomplete Triple의 head가 Sunnyvale이고 relation이 country일 때 tail을 찾는다고 가정해보자. 그래프 임베딩의 경우 Sunnyvale에 저장된 구조적인 정보만을 활용하여 추론을 진행한다. 다시 말해, Sunnyvale 엔티티를 기준으로 이웃의 정보를 모으는 것이다. 반면 이웃 노드들의 정보와 더불어 context 정보를 활용하면 state와 California에 대한 정보도 활용할 수 있다. 즉, 이웃 노드들의 정보를 활용하기위해 완전한 트리플을 사용하고, 여기서 California라는 context정보를 읽어내어 이용하여 좀 더 수월하고 정확학 추론이 가능하다.

Knowledge Graph에는 구조적인 정보뿐만 아니라 Context information도 존재한다. 이를 활용하기 위해 Graph Neural Network(GNN)이나 Attention-based 방식도 연구가 진행되었다. 하지만 Graph의 경우 Layer수를 늘리면 **Oversmoothing현상**이 발생하기 때문에 제약 조건이 존재한다. HittER 모델의 시작은 "<span style = "color:gold"><b>어떻게 하면 이웃 노드의 정보(KGE)와 Context정보(Attention-based)를 모두 이용하면서  Deep한 모델을 만들수 있을까?</b></span>" 라는 질물에서 출발한다. 

<br/>
<br/>

# Method

## 1. Overview

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/c692b2ac-5971-418d-aea6-31cc8aa56e58">
</p>

HittER는 "**Deep Hierachical Transformer**"모델로 엔티티와 릴레이션의 representation을 이웃들로부터 정보를 취합해 동시에 학습하는 모델이다. Attention을 하는 Transformer를 이용해 기본적으로 그래프의 context정보를 학습할 수 있으며 동시에 이웃 엔티티들의 정보또한 학습할 수 있게 만든 모델이다. HittER는 두 가지의 Transformer가 계층적으로 쌓여진 형태이다. 

## 2 Model Architecture
### 2.1 Simple Context-Independent Transformer

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/d1f1e06c-b809-4844-a250-0af2eb65dfc0">
</p>
먼저 간단하게 Notation을 살펴보면, 저자는 head, relation, tail을 각각 subject, predictate, object라고 표현을 했다. 또한 Incomplete Triple에서는 내가 알고 있는 엔티티를 **Source entity**라고 지칭하며 Knowledge Graph Completion을 통해 찾아야 하는 엔티티를 **Target entity**라고 지칭한다.

- Triple: <$$head, relation, tail$$> = <$$source, predicate, object$$>

Simple Context-Independent Transformer는 Bottom block의 맨 앞단 Transformer 하나 만을 이용해서도 Link prediction을 할 수 있다. 즉, Bottom block의 점선으로 된 박스 부분만을 사용해 Link prediction을 진행한다. 이 Transformer의 입력은 \[CLS\]토큰의 임베딩과 source entity의 임베딩($$e_{src}$$), predicate의 임베딩($$r_p$$)이 된다. <u>모델은 Link preiction에 필요한 scoring function으로 Transformer의 encoder방식(Multilayer + Bidirectional)을 사용하여 간단한 방법으로 수</u>행할 수 있다.

\[CLS\] 토큰 임베딩과 $$e_{src}$$, $$r_p$$를 각각 랜덤하게 초기화 한 후 세 개의 임베딩을 BERT와 마찬가지로 Transformer Encoder의 입력으로 집어넣는다. 그러면 Triple이 <span style = "color:gold">**Plausible**</span>한지 아닌지를 판단해주는 결과값인 $$M_{e_{src}}$$가 출력된다. 같은 방식으로 이웃들 중 모든 후보 엔티티에 대하여 plausible score인 $$M_{e_{src}}$$를 구한 후 softmax 함수를 이용해 nomalization한다. 학습과정을 정리하자면 다음과 같다.

1. True triple의 target entity와 $$M_{e_{src}}$$의 내적(Dot-product)를 구한다.
2. 같은 방식으로 다른 모든 후보 엔티티들의 score를 계산하고 SoftMax 함수를 이용해 normalize한다.
3. $$\mathcal{L_{LP}} = -log\,p(e_{tgt} \vert M_{e_{src}})$$를 사용하여 모델을 훈련한다.

<br/>

### 2.2 Bottom Block: Entity Transformer

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/8d0fd6ac-a1fd-430f-a387-fd0b94674549">
</p>

위의 Simple link prediction의 경우 BERT와 다를바가 없다. 다시 말해, Graph의 Context정보를 이용하지 않는다. 또한 스텝마다 하나의 트리플의 임베딩만 학습하므로 구조 정보또한 제대로 반영이 되지 않는다. 따라서 **Source entity의 이웃 정보들을 활용**해야한다. Bottom Block은 <span stlye = "color:aqua"><b>가능한 모든 엔티티-릴레이션 쌍으로부터 유용한 feature들을 모두 뽑아</b></span>낸다.

먼저 Incomplete triple의 $$e_{src}$$와 $$r_p$$쌍을 첫 번째 transformer의 입력으로 넣는다. 다음으로 소스 엔티티의 이웃 엔티티들과 relation의 임베딩을 다음 transformer의 입력으로 넣는다. 이렇게 함으로써, <span style = "color:gold"><b>엔티티-릴레이션 쌍을 직접 top block으로 전파시키지 않고 두 개의 input을 하나로 변환함으로써 running time을 줄일 수 있다</b></span>.

<br/>

### 2.3 Top Block: Context Transformer

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/626e43a7-dd2a-4ddf-b2f5-28284d2de970">
</p>

Top block에서는 bottom block의 출력값과 \[GCLS\]라는 special token의 임베딩 정보를 취합한다. Top block의 입력은 총 세 가지이다. 
1) \[GCLS\] Special Token embedding     : $$E_{\[GCLS\]}$$
2) Intermedjate source entity embedding : $$M_{e_{src}}$$
3) Other neighborhood entity embedding  : $$M_{e_1}, M_{e_2}, \cdots$$

Loss function또한 바뀐다. 기본적으로 Bottom block과 마찬가지로 NLL의 형태를 띄지만 디테일이 다르다.
<b>Loss</b>: $$\mathcal{L_{LP}} = -log \, p(e_{tgt} \vert T_{\[GCLS\]})$$

<br/>

### 2.4 Balanced Contextualization: Masked Entity Prediction

위의 방식대로 context information을 주는 것은 종종 여러가지 문제를 유발한다.

1) Source entity가 link prediction을 하기에 충분한 정보를 가지고 있는 경우 추가적으로 들어가는 contextual information은 노이즈가 된다. 다시 말해, 이미 많은 정보를 내포한 Source entity는 다른 정보를 필요로 하지 않는다는 것이다.
2) 많은 context 정보가 Source entity로부터 온 정보를 downgrade시키거나 쓸데없는 상관관계 정보를 포함하기에 overfitting이 발생할 수 있다.

따라서 <span style = "color:gold"><b>Contextual information과 Source entity information이 균형</b></span>을 이루어야 한다. 논문에서는 두 문제를 해결하는 방법을 각각 제시한다.

<span style = "font-size:110%"><b>Solution of Problem 1</b></span>  
<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/16366e18-d1c3-4a05-81fc-e3847949f6b9">
</p>

첫 번째 문제를 해결하기 위해 모델을 훈련하는 동안 특정 확률로 입력 Source entity를 <u>1)\[MASK\] 토큰으로 바꾸거나, 2)랜덤하게 선택된 엔티티로 바꾸거나, 3)바꾸지 않고 그대로 둔다.</u> 이 특정 확률(Certain Probability)는 dataset마다 특화된 Hyperparameter이다. 이 과정을 통해 모델은 **contextual representation**을 학습할 수 있다.

<span style = "font-size:110%"><b>Solution of Problem 2</b></span>  
<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/a64e8a16-2fc9-4d83-a824-f8ffd9981053">
</p>

두 번째 문제를 해결하기 위해서는 모델이 혼동을 일으키는 엔티티를 발견하도록 훈련시켜야 한다. 따라서 Source entity에 상응하는 출력 임베딩, $$T_{e_{src}}$$에 하나의 <u>classification layer를 두어 Correct source entity인지 예측</u>하도록 한다.

<br/>
<br/>

# Experiment & Result



<br/>
<br/>

# Contriubution



<br/>
<br/>

# Related Work
