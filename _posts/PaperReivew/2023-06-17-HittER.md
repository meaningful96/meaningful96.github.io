---
title: "[논문리뷰]HittER: Hierarchical transformers for knowledge graph embeddings"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-06-17
last_modified_at: 2023-06-17
---

Chen, S., Liu, X., Gao, J., Jiao, J., Zhang, R., & Ji, Y. (2021, October 6). HittER: Hierarchical Transformers for Knowledge Graph Embeddings. ArXiv.org. https://doi.org/10.48550/arXiv.2008.12813

# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/421f0cd0-9456-4493-8e28-b73d26a27e0d">
</p>

<span style = "font-size:115%"><b>1. Knowledge Graph는 여전히 불완전(Incomplete)하고 Noisy하다.</b></span>

Knowledge Graph는 Heterogeneous graph의 일종이다. 그리고 그래프 형식으로 정보를 저장할 때 사람이 직접 <span style = "color:gold"><b>수동으로(manually)</b></span> 넣어줘야하고, 이는 데이터 유실로 이어질 수 있다.
```
The first major problem is the incompleteness and noise in the knowledge graph.
This issue arises from the fact that much of the information in the graph is
manually inputted by humans so that it leads to potential loss of data.
```

<span style = "font-size:115%"><b>2. 그래프 임베딩 모델(Graph Embedding Method, KGE)의 한계</b></span>

그래프 임베딩은 기본적으로 하나의 벡터 공간에 그래의 구조 정보를 표현하는 것에 초점을 맞춘다. 예를 들어, head와 relation의 합을 tail이라 정의할 수 있다(TranE 모델). 그래프 임베딩 방식은 
<b>그래프의 구조 정보</b>만을 활용한다. 한 노드를 중심으로 이웃 노드들의 정보를 취합(aggregation)한다. 다시 말해, 저차원의 벡터 공간에서 지리적인, 구조적인 특성을 이용하는 것이 KGE이다. 

이렇게 할 경우 취합된 모든 정보를 벡터 공간에서 하나의 단일 벡터에 저장해야 하기떄문에 정보의 유실이 생길 수 있다(Seq2Seq 모델의 단점과 비슷: context vector에 정보를 압축). 정리하면 그래프 임베딩 방식(KGE, Knowledge Graph Embedding)의 경우 <span style = "color:gold"><b>1. 오직 그래프의 구조 정보만을 활용</b></span>하며 <span style = "color:gold"><b>2. 하나의 단일 벡터에 정보를 압축해서 정보의 손실이 발생</b></span>하는 한계점들이 존재한다.

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/e051b97c-47b4-4a64-b97a-a95d43be05dd">
</p>

위의 Sub-graph로 Link prediction을 진행한다고 가정해보자. Incomplete Triple의 head가 Sunnyvale이고 relation이 country일 때 tail을 찾는다고 가정해보자. 그래프 임베딩의 경우 Sunnyvale에 저장된 구조적인 정보만을 활용하여 추론을 진행한다. 다시 말해, Sunnyvale 엔티티를 기준으로 이웃의 정보를 모으는 것이다. 반면 이웃 노드들의 정보와 더불어 context 정보를 활용하면 state와 California에 대한 정보도 활용할 수 있다. 즉, 이웃 노드들의 정보를 활용하기위해 완전한 트리플을 사용하고, 여기서 California라는 context정보를 읽어내어 이용하여 좀 더 수월하고 정확학 추론이 가능하다.

Knowledge Graph에는 구조적인 정보뿐만 아니라 Context information도 존재한다. 이를 활용하기 위해 Graph Neural Network(GNN)이나 Attention-based 방식도 연구가 진행되었다. 하지만 Graph의 경우 Layer수를 늘리면 **Oversmoothing현상**이 발생하기 때문에 제약 조건이 존재한다. HittER 모델의 시작은 "<span style = "color:gold"><b>어떻게 하면 이웃 노드의 정보(KGE)와 Context정보(Attention-based)를 모두 이용하면서  Deep한 모델을 만들수 있을까?</b></span>" 라는 질물에서 출발한다. 

<br/>
<br/>

# Method

## 1. Overview

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/c692b2ac-5971-418d-aea6-31cc8aa56e58">
</p>

HittER는 "**Deep Hierachical Transformer**"모델로 엔티티와 릴레이션의 representation을 이웃들로부터 정보를 취합해 동시에 학습하는 모델이다. Attention을 하는 Transformer를 이용해 기본적으로 그래프의 context정보를 학습할 수 있으며 동시에 이웃 엔티티들의 정보또한 학습할 수 있게 만든 모델이다. HittER는 두 가지의 Transformer가 계층적으로 쌓여진 형태이다. 

## 2 Model Architecture
### 2.0 Notation

<p align="center">
<img width="600" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/f90a6024-a422-4a17-9fad-47df4d73a635">
</p>

### 2.1 Simple Context-Independent Transformer

<p align="center">
<img width="1000" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/d1f1e06c-b809-4844-a250-0af2eb65dfc0">
</p>

먼저 Bottom block의 맨 앞단 Transformer 하나 만을 이용해서도 Link prediction을 할 수 있다. 이 Transformer의 입력은 [CLS]토큰의 임베딩과 source entity의 임베딩, predicate의 임베딩이 된다. 

<br/>

<br/>
<br/>

# Experiment & Result



<br/>
<br/>

# Contriubution



<br/>
<br/>

# Related Work
