---
title: "[논문리뷰]End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion"

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

published: true

date: 2023-03-06
last_modified_at: 2023-03-06
---

Shang, C. (2018, November 11). End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion. *arXiv present: 1811.04441*  
[Paper]("https://arxiv.org/abs/1811.04441")

# Problem Statement

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/228893474-6f84c0fa-6024-46c6-91fb-ee2fcb72068a.png">
</p>

Knowledge Graph는 현 시점에서 <u>많은 수의 엔티티(Entity, Node)와 릴레이션(Relation, Edge)를 가지고 있다. 또한 그 정보 역시 다양한 Heterogeneous Graph</u>이다. 하지만 기존의 존재하던 Knowledge Base Model들은 모두 Large-Graph에 부적합하다. Graph Embedding모델중에서는 PinSage 모델을 제외하고는 기존 모델들은 모두 Large Scale Graph에 부적합하다. 따라서 새로운  Graph Embedding 모델의 필요하다.

1. Knowledge Graph는 이미 수백만의 Triple을 포함한다.
  - 실제 데이터가 계속해서 추가되기 때문에 그 수가 기하급수적으로 늘어난다.
  - 따라서 KG Completion Task를 푸는 것이 점점 더 중요해진다.
<br/>
2. 기존의 임베딩 모델들은 Large Scale Graph에 부적합하며, ConvE역시 마찬가지이다.
  - ConvE는 Triple의 임베딩 연산이 TransE와는 다르게 translation property가 존재하지 않는다. 
  - TransE의 임베딩 연산은 <span style = "color:aqua">$$e_s + e_r = e_o$$</span>이다. 즉, Subject(head)와 relation의 임베딩의 합이 Object(tail)임베딩과 같다.
  - ConvE는 임베딩 공간에서 KG의 연결성을 설명하는데 부적합하다.  

<br/> 
<br/> 


# Relation Work

- Knowledge Graph Embedding
- TransE, TransR, TransD, TransH
- DistMult, ComplEx
- convKB, convE
- GCN

# Method
## 1. Overview
<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/229112137-e06272cc-b40b-4ff4-b475-e8e0f961fa1c.png">
</p>

모델의 아키텍쳐는 크게 두 부분으로 나누어지며, Encoder-Decoder 모델이다. Encoder는 <span style = "color:aqua">**WGCN**</span>으로 기존의 GCN에 Weight(가중치)를 추가하여 수정한 구조이다. 그리고 디코더는 <span style = "color:aqua">**SACN**</span>이라고 불리며 이는 Structure-Aware Convolution Network이다.

이 모델의 전체적인 구조를 단 한 줄로 설명하자면 <span style = "color:gold">**ConvE의 prediction performance에 TransE의 Translation Property를 병합**</span>시킨 모델이다.

## 2. Encoder: WGCN

### 1) GCN vs WGCN

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/229114946-de11cdb5-9fd9-4541-b6bd-017effeafb38.png">
</p>

Encoder는 GCN모델의 성능을 향상시켜만든 모델이다. **Weighted GCN**의 약자이며, 이름 그대로 가중치에 대한 정보를 GCN에 추가한 것이다. 그림을 보면 빨간색 노드가 중심노드가 된다. GCN은 Graph Embedding 모델의 한 종류이다. 즉, 하나의 노드를 기준으로 이웃 노드들의 대한 정보를 Aggregation한다.

마찬가지로 WGCN도 이웃 노드들의 정보를 Aggregation한다. 기존의 GCN과 다른점은 WGCN은 <span style = "color:gold">**중심 노드를 기준으로 이웃 정보를 취합할 때 Relation Type마다 이웃 노드들에 가중치(Weight)를 부여**</span>한다. 이로서 어떤 이웃노드들이 중심노드에 더욱 더 큰 영향력을 행사하는지 파악할 수 있다. 가중치는 Learning parameter이다.

그림에서 예시를 들면, 빨간색 노드가 중심노드이고, 중심 노드에대해 이웃들의 relation은 총 3가지 종류가 있으며 각각이 <span style = "color:blue">Blue</span>, <span style = "color:green">Green</span>, <span style = "color:orange">Orange</span> 노드로 표현되어 있다. 


- WGCN determines how much weights to give to each subgraph when combining the GCN embeddings for a node.

<br/>

### 2) Mechanism of WGCN

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/229121357-e318a6b9-199e-4154-a03e-7898687ca682.png">
</p>

WGCN은 여거래의 Layer가 쌓여져있는 형태이다. 먼저 하나의 노드에서 Aggregation되는 과정을 살펴보면, $$l^{th}$$ 레이어의 입력은 이전 레이어에서 나온 크기가 $$F^l$$인 벡터가 된다. 그렇게 들어온 입력이 WGCN을 거치면 $$F^{l+1}$$ 번째의 성분이 입력 벡터에 추가된다. 다시 말해, 레이어를 하나씩 거칠때마다 출력 벡터의 성분 개수가 하나씩 늘어난다.

$$v_i$$노드에 대한 임베딩을 수식으로 나타내면 다음과 같다. $$\alpha_t^l$$은 새롭게 추가된 parameter인 **가중치(weight)** 이다. $$t$$는 edge type의 개수로 총 $$T$$개가 있다. GCN과 마찬가지로 기본적인 Operation은 <u>1)Weighted Sum을 하고 2)Nonlinearity를 먹이는 모양</u>이다. 식의 의미를 해석하자면, **$$i$$노드를 중심으로 모든 이웃 노드의 정보를 가중치를 부여해 취합하고, Nonlinearity를 먹여 신경망의 표현 능력을 향상**시킬 수 있는 출력값을 만들어 내는것이다.

<p align="center">
<img width="400" alt="1" src="https://user-images.githubusercontent.com/111734605/229144812-d0184cdc-a24b-4a97-baaa-2226f83af125.png">
</p>

이 식을 보면 한 가지 문제점이 발생한다. 바로 중심 노드가 이웃 노드들의 정보만 취합하고, 자기 자신에 대한 정보를 취합하지 못했다는 것이다. 따라서, 중심 노드의 정보를 담을
수 있는 term을 추가해 줘야 하고 이를 <span style = "color:aqua">Self-Loop</span>라고 한다.

<p align="center">
<img width="1000" alt="1" src="https://user-images.githubusercontent.com/111734605/229153105-427d88fc-d409-4788-84df-e836eb33d312.png">
</p>

최종적으로 위의 식처럼 이웃 노드들의 정보를 가진 term과 $$v_i$$자신에 대한 정보를 가진 term으로 표현된다. $$A_t$$는 노드들의 연결 정보가 담겨져 있는 인접행렬(Adjacency matrix)이다. 이때, Self-Loop에 대한 정보까지 담아 둔 행렬을 $$A_l$$라 하고, 정리하면 오른쪽 위의 식처럼된다. 이를 $$h_i^l$$에 대입해서 정리한 후, 하나의 노드에 대해서가 아닌 레이어 전체 연산으로 바꾸어 행렬식으로 나타내면 최종 임베딩 식이 나온다.

수식을 정리하는 과정을 더 디테일하게 살펴보면 다음과 같다.

<p align="center">
<img width="600" alt="1" src="https://user-images.githubusercontent.com/111734605/229178421-e0044236-539a-4692-a7f7-f54858cf0d10.png">
</p>


<br/> 
<br/> 

# Experiment & Result

as

<br/> 
<br/> 

# Contribution

as

<br/> 
<br/> 

