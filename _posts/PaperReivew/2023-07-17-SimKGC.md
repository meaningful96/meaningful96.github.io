---
title: "[논문리뷰]SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models "

categories: 
  - PaperReview
  
tags:
  - [KG Completion]
  
toc: true
toc_sticky: true

date: 2023-07-10
last_modified_at: 2023-07-10
---


Wang, L., Zhao, W., Wei, Z., & Jingming, L. (2022). SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*. https://doi.org/10.18653/v1/2022.acl-long.295

# Problem Statement

<span style="font-size:110%"><b>1. Limitation of Graph Embedding</b></span>  
Graph Embedding은 엔티티와 릴레이션을 Text description같은 추가 정보를 사용하지않고 **Triple의 구조 정보**를 저차원 벡터로 mapping하여 그래프를 학습하게된다. Graph Embedding 방식을 채택한 모델들은 TransE, TransR, RotatE등이 있다. Graph Embedding의 문제점은 바로 Graph에 내제된 정보중 오직 <span style="color:gold">**구조 정보(Structural Information)만을 사용해 학습하고, 텍스트 정보(Texture Information)는 사용하지 못한다**<span>는 점이다. 이로인해 그래프의 특징을 온전하게 반영하지 못한다.

<br/>

<span style="font-size:110%"><b>1. Limitation of Text-Based Method</b></span>    
Text-based Method는 Graph Embedding과 달리 사용가능한 text를 entity representation learning을 위해 통합하는 방식이다. 즉, Text 정보를 이용하여 학습을 진행한다. 직관적으로 추가적인 input정보를 활용할 수 있기 때문에 Graph Embedding방식보다는 좋을 것이라 예측되지만, **실제로는 훨신 긴 Inference time이 소요되고 심지어 성능이 더 뒤쳐지는 결과**를 보였다.

<p align="center">
<img width="400" alt="1" src="https://github.com/meaningful96/DSKUS_Project/assets/111734605/360089fa-b7f3-4a1f-8e19-4a6b7b06401a">
</p>    
<center><span style="font-size:80%">Knowledge Graph에 있는 text description</span></center>

<br/>

저자는 이 Text-based method의 성능 저하 이유를 <span style="color:gold"><b>Inefficiency in contrastive learning</b></span>으로 본다. 즉, 기존의 Contrastive learning은 KGC를 하는데 pre-trainied 모델에 적용하기에 부적합하다는 것이다. 

<br/>
<br/>

# Related Work
<span style="font-size:110%"><b>1. Knowledge Graph Completion</b></span>   
Triple을 (head, relation, tail)로 정의하고, 불완전한 Triple (head, relation, ?) 또는 (?, relation,tail)이 주어졌을때 missing entity에 해당하는 ?를 찾는 task이다.

<span style="font-size:110%"><b>1. Pre-trained Language Model</b></span>  
[[논문리뷰]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://meaningful96.github.io/paperreview/bert/)  
[[논문리뷰]GPT-1:Improving Langauge Understanding by Generative Pre-Training](https://meaningful96.github.io/paperreview/GPT1/)

<span style="font-size:110%"><b>1. Contrastive Learning</b></span>  

Contrastive Learning은 데이터 인스턴스 간의 유사하고 비유사한 특징을 학습하는 자기 감독 학습의 한 형태이다. 이는 CV, NLP, 그리고 Knowledge Graph 등 다양한 도메인에서 적용되었다. KGC의 맥락에서 Contrastive Learning은 인공적으로 손상된 것들(Negative Sample)과 비교하여 Knowledge Graph에서 실제 Triple (h, r, t)를 구별하는 작업에 초점을 둔다. 

- **Positive Sample**: Knowledge Graph에서 존재하는 실제 Triple (h, r, t). 예를 들어, 동물에 관한 Knowledge Graph에서는 (고양이, is_a, 포유류)와 같은 Triple이 있을 수 있다.
- **Negative Sample**: Knowledge Graph에서 존재하지 않는 인공적 또는 가짜 Triple. Negative Sample은 일반적으로 Positive Triple를 손상시키는 방식으로 생성되며, 이는 head 또는 tail을 임의의 엔티티로 교체함으로써 이루어진다. 예를 들어, Negative Sample은 (고양이, is_a, 파충류)일 수 있다.

Contrastive Learning 모델은 이러한 <span style = "color:gold">**Positive Sample과 Negative Sample을 구별하는 방법을 학습**</span>한다. 훈련 동안 모델은 Positive Sample과 Negative Sample의 score를 조합한 Loss를 제시받는다. 이는 <u>Positive Triple에 높은 점수를, Negative Triple에 낮은 점수를 부여</u>하려는 목표를 가지고 있다. 시간이 지남에 따라, 모델은 실제와 가짜 Triple를 효과적으로 구별할 수 있도록 Knowledge Graph의 엔티티와 관계의 임베딩을 학습해야한다.

Knowledge Graph Completion Task에 대한 Contrastive Learning의 목적은 모델의 Knowledge Graph 내 복잡하고 다중 관계 데이터에 대한 이해를 향상시키는 것이다. 이는 모델이 missing link를 추론하고 새로운 ling를 예측하는 능력을 향상시킬 수 있다.

[[Deep Learning]Contrastive Learning(대조 학습)이란?](https://meaningful96.github.io/deeplearning/contrastivelarning/)

<br/>
<br/>

# Method

## 1. Notation

Knowledge Graph를 $$\mathcal{G}$$, True triple를 ($$h,r,t$$)라고 정의할 때, 이 논문에서는 추가적으로 **Inverse Triple** ($$t, r^{-1}, h$$)을 정의한다. 이 때, $$r^{-1}$$은 $$r$$의 역방향 릴레이션이다. 이렇게 정의를 함으로써 결국 tail에 대한 link prediction을 진행하면된다는 이점이 생긴다.

## 2. Model Architecture

<p align="center">
<img width="800" alt="1" src="https://github.com/meaningful96/DataStructure_and_Algorithm/assets/111734605/847b4274-acce-430f-a0b1-250d319f8332">
</p>  



<br/>
<br/>

# Experiment & Result

<br/>
<br/>

# Contribution

# Reference
