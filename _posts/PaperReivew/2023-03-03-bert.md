---
title: "[논문리뷰]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "

categories: 
  - PaperReview
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-03-02
last_modified_at: 2023-03-02
---

<b>BERT(Bidirectional Encoder Representations from Transformer)</b>는 이름 그대로 Transformer의 Encoder를 활용한 Language model이다. BERT는 ELMo, GPT-1과 비슷한 시기 발표된 논문으
로, 동일하게 양질의 <u>pre-trained language representation를 얻는 것 과 down-stream task로의 손쉬운 fine-tuning에 중점</u>을 두고 있다.



# Related Work



<br/>
<br/>

# Method



<br/>
<br/>

# Experiment & Result
