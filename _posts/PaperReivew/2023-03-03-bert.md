---
title: "[논문리뷰]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "

categories: 
  - PaperReview
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-03-02
last_modified_at: 2023-03-02
---

<b>BERT(Bidirectional Encoder Representations from Transformer)</b>는 이름 그대로 Transformer의 Encoder를 활용한 Language model이다. BERT는 ELMo, GPT-1과 비슷한 시기 발표된 논문으로, 동일하게 양질의 <u>pre-trained language representation를 얻는 것 과 down-stream task로의 손쉬운 fine-tuning에 중점</u>을 두고 있다.

# Related Work
## 1. Pre-Training (Feature-Based VS Fine-Tuning)

Pre-Training에는 두 가지 방식이 있다. 1) feature-based pre-training과 2)fine-tuning pre-training 방식이다.

### 1) Feature-based Pre-Training
Feature-Base의 핵심은 <u>어떠한 특정 task를 해결하기 위한 아키텍쳐를 task specific하게 구성하고 거기에 pre-trained representaions(= Embedding layers)를 부가적인 feature로 사용</u>한다. 대표적인 아키텍쳐로 ELMo가 있다.

### 2) Fine-Tuning Pre-Training

Fine-Tuning approch를 채택한 가장 대표적인 모델들이 GPT와 BERT로, 어떤 <span style = "color:gold">특정 Task에 대한 파라미터를 최소화하여 범용적인 Pre-Trained 모델을 사용한다. 그리고 특정 Task를 수행할 경우 그에 맞게 fine-tuning을 적용</span>한다.

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229369777-5ddc6eaf-25b3-4aae-86ca-dcba7d7ab6bc.png">
</p>

GPT와 ELMo는 모두 pre-training 모델이지만 앞서 말했듯 각각 fine-tuning 방식과 feature-based 방식을 사용한다. 하지만 두 모델의 공통점이 있는데 이는 pre-training의 목적함수로 <span style ="color:aqua">**단방향 언어모델(Unidirctional Language Model)**</span>을 채택한다는 것이다. **GPT**는 Transformer의 디코더를 사용한 구조이다. 또한 Transformer의 디코더와 GPT모두 Masked Self-Attention을 사용하기 때문에 <u>이전 토큰만을 고려하여 언어모델을 구성</u>한다.

<center><span style = "font-size:120%"> $$GPT: \; \; L \; = \; \displaystyle\sum_i logP(u_i \vert u_{i-k}, \cdots, u_{i-1}; \theta) $$ </span></center>

**ELMo**는 biLSTM을 사용하지만 forward, backward와 같은 일련의 방향이 존재하는 언어모델이다. 또한 Shallow Concatenate를 통해 언어 모델을 구성한다. 따라서 깊은 양방향 특징을 학습하기에는 최선의 방법이 아닌다.

<center><span style = "font-size:120%"> $$ELMo: \; \; L \; = \; \displaystyle\sum_i logP(u_i \vert u_{i-k}, \cdots, u_{i-1}; \overrightarrow{\theta}) \; + \; logP(u_i \vert u_{i+1}, \cdots, i_N; \overleftarrow{\theta}) $$ </span></center>

이러한 이유로 단방향의 언어모델은 pre-trained representation의 성능을 저해하는 요소가 된다. 특히 fine-tuning approach의 경우, 사전 학습 과정에서 단방향의 언어모델이 다양한 언어 구조의 선택 가능성을 제한할 수 있다. 예를 들면, 이전의 토큰만을 사용하는 GPT의 Transformer를 QA task의 fine-tuning에 사용한다고 할 때, 질문과 답변 사이의 유기적인 관계를 고려하는 능력이 떨어질 것이다.

<br/>
<br/>

# Method

## 1. Overview

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229371141-a54f2cbc-928e-40cf-99c6-b2f826c25411.png">
</p>

<br/>
<br/>

# Experiment & Result
