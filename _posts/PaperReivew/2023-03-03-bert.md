---
title: "[논문리뷰]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding "

categories: 
  - PaperReview
  
tags:
  - [NLP]
  
toc: true
toc_sticky: true

date: 2023-03-02
last_modified_at: 2023-03-02
---

<b>BERT(Bidirectional Encoder Representations from Transformer)</b>는 이름 그대로 Transformer의 Encoder를 활용한 Language model이다. BERT는 ELMo, GPT-1과 비슷한 시기 발표된 논문으로, 동일하게 양질의 <u>pre-trained language representation를 얻는 것 과 down-stream task로의 손쉬운 fine-tuning에 중점</u>을 두고 있다.

# Related Work
## 1. Pre-Training (Feature-Based VS Fine-Tuning)

Pre-Training에는 두 가지 방식이 있다. 1) feature-based pre-training과 2)fine-tuning pre-training 방식이다.

### 1) Feature-based Pre-Training
Feature-Base의 핵심은 <u>어떠한 특정 task를 해결하기 위한 아키텍쳐를 task specific하게 구성하고 거기에 pre-trained representaions(= Embedding layers)를 부가적인 feature로 사용</u>한다. 대표적인 아키텍쳐로 ELMo가 있다.

### 2) Fine-Tuning Pre-Training

Fine-Tuning approch를 채택한 가장 대표적인 모델들이 GPT와 BERT로, 어떤 <span style = "color:gold">특정 Task에 대한 파라미터를 최소화하여 범용적인 Pre-Trained 모델을 사용한다. 그리고 특정 Task를 수행할 경우 그에 맞게 fine-tuning을 적용</span>한다.

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229369777-5ddc6eaf-25b3-4aae-86ca-dcba7d7ab6bc.png">
</p>

GPT와 ELMo는 모두 pre-training 모델이지만 앞서 말했듯 각각 fine-tuning 방식과 feature-based 방식을 사용한다. 하지만 두 모델의 공통점이 있는데 이는 pre-training의 목적함수로 <span style ="color:aqua">**단방향 언어모델(Unidirctional Language Model)**</span>을 채택한다는 것이다. **GPT**는 Transformer의 디코더를 사용한 구조이다. 또한 Transformer의 디코더와 GPT모두 Masked Self-Attention을 사용하기 때문에 <u>이전 토큰만을 고려하여 언어모델을 구성</u>한다.

<center><span style = "font-size:120%"> $$GPT: \; \; L \; = \; \displaystyle\sum_i logP(u_i \vert u_{i-k}, \cdots, u_{i-1}; \theta) $$ </span></center>

**ELMo**는 biLSTM을 사용하지만 forward, backward와 같은 일련의 방향이 존재하는 언어모델이다. 또한 Shallow Concatenate를 통해 언어 모델을 구성한다. 따라서 깊은 양방향 특징을 학습하기에는 최선의 방법이 아닌다.

<center><span style = "font-size:120%"> $$ELMo: \; \; L \; = \; \displaystyle\sum_i logP(u_i \vert u_{i-k}, \cdots, u_{i-1}; \overrightarrow{\theta}) \; + \; logP(u_i \vert u_{i+1}, \cdots, i_N; \overleftarrow{\theta}) $$ </span></center>

이러한 이유로 단방향의 언어모델은 pre-trained representation의 성능을 저해하는 요소가 된다. 특히 fine-tuning approach의 경우, 사전 학습 과정에서 단방향의 언어모델이 다양한 언어 구조의 선택 가능성을 제한할 수 있다. 예를 들면, 이전의 토큰만을 사용하는 GPT의 Transformer를 QA task의 fine-tuning에 사용한다고 할 때, 질문과 답변 사이의 유기적인 관계를 고려하는 능력이 떨어질 것이다.

<br/>
<br/>

# Method

## 1. Overview

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229371141-a54f2cbc-928e-40cf-99c6-b2f826c25411.png">
</p>

BERT(Bidirectional Encoder Representations from Transformer)의 기본적인 구조는 Transformer의 Encoder를 multi-layer로 쌓아 올린 것이다.(본 눈문에서 L = 12). 더하여 BERT는 기존의 pre-trained language model이 가진 문제점을 보완하기 위한 두가지 구조적인 특징 <b>Masked language model(MLM), next sentence prediction(NSP)</b>이 존재한다. BERT가 높은 성능을 얻을 수 있었던 것은, <span style ="color:aqua">**레이블이 없는 방대한 데이터로 사전 훈련된 모델을 만든 후, 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용**</span>하기 때문이다. 기존 사례에서도 해당 기법을 사용하면 상당히 좋은 성능이 발휘되는 것이 입증되었다.

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229372120-fc3bc600-1731-46b4-878e-141ab9c0fe4a.png">
</p>

실제 task에서 사용하는 모델은 초기에 동일한 파라미터로 시작하지만, 최종적으로는 서로 다른 fine-tuned 된 모델을 보유하게 된다. BERT 는 pre-trained 된 모델과 fine-tuned 된 모델 사이의 구조적 차이가 거의 없게 된다. 그러나 미세하게 차이가 생기는 모델 여러 개를 생성하게 된다. BERT의 구별된 특징은 다양한 작업에 걸친 <span style ="color:gold">통일된(unified) 구조</span>라는 것이다. <u>pre-training 구조와 마지막 down-stream 구조 사이의 최소한의 차이</u>만 있다.

BERT의 모델 구조는 트랜스포머의 인코더만을 다중으로 쌓은 <b>다중 레이어 양방향 트랜스포머 인코더</b>이다. 즉, 기존의 Transformer와 달리 앞과 뒤 양방향에서 Attention을 사용하는 모델이라는 뜻이다.

- BERT-Base: L=12, H=768, A=12, Total Parameters=110M
- BERT-Large: L=24, H=1024, A=16, Total Parameters=340M

## 2. Input/Output Representation

Down Stream 과제 중에 단일 문장이 아닌 문장의 쌍(ex: QA = Question Answering <질문, 답변>)에 대한 학습이 필요한 경우가 있다. 이러한 경우 문장 쌍을 토큰을 통해 순서와 쌍을 만들어줄 수 있다. 주의할 것은 본 논문에서 문장에 대한 정의는 우리가 알 고 있는 문장과는 다르다. 실제 언어의 문장 대신에 유사한 텍스트의 임위 범위가 될 수 있다.

- Sentence(문장)
  - An aritrary span of contiguous text, rather than an actual linguistic sentence
  - 문장 대신에 유사한 텍스트의 임의 범위

<br>

- Sequence(시퀀스)
  - The input toke sequences to BERT, which may be a single sentence or two sentences packed together
  - BERT에 대한 입력 토큰 시퀀스, 한 개의 문장 또는 두 개의 문장을 합친(Packing)것

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229372679-17ab7d49-52c4-4f14-8d45-4b2609a787b8.png">
</p>

문장의 쌍은 하나의 시퀀스로 함께 묶인다. 그리고 이 쌍을 두 가지 방법으로 문장을 구별한다. 먼저, <span style ="color:aqua">**토큰 (\[SEP\], \[SEP\])을 통해 기존의 문장들을 분리**</span>를 보여준다. 두 번째로 우리는 <span style = "color:aqua">**모든 토큰에 이것이 문장 A인지 B인지 표시하는 학습된 임베딩을 추가**</span>한다. BERT에서 사용하는 tokenizer는 **WordPiece 임베딩** 방식이다.

아래 사진을 보면, 각각의 임베딩 layer를 통해 나타나는 embedding을 E로 표시하고 <b>각 단어의 tokenizer embedding, Segment Embedding(문장 쌍이라면 어디 속하는지), 그리고 Position Embedding</b>을 해준다.

- WordPiece Embedding : <u>실질적인 입력</u>이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기로 30,522개.
- Position Embedding : <u>위치 정보</u>를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.
- Segment Embedding : <u>두 개의 문장을 구분</u>하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.

<p align="center">
<img width="800" alt="1" src="https://user-images.githubusercontent.com/111734605/229372819-fbcac86d-58aa-4492-ac8b-d43624297e27.png">
</p>

이러한 임베딩 과정을 거쳐서 BERT 구조에 넣어주고 연산이 되는 것이다.

## 3. Model Architecture
### 1) Pre-Training BERT

BERT의 기본적인 구조는 <b>Multi Layer Bidirectional Transformer(Encoder)</b>로서, **deep bidirectional model**이다. deep bidirectional model은 GPT와 같은 단방향 구조보다 강력하며, ELMo등의 shallow bidirectional concatenate 구조보다 깊게 학습이 가능하다. 하지만, 조건부 확률로 구성되는 기본적인 언어모델은 left-to-right, right-to-left로 학습될 수 밖에 없다. <u>Bidirectional 한 조건부 확률을 사용하여 언어모델을 계산하면 각각의 단어가 스스로를 간접적으로 볼 수 있어 너무 쉬운 예측이 되기 때문</u>이다.

다시 말해, Bidirectional하게 학습을 하게 되면 간접적으로 예측하려는 단어를 참고하게 되므로, 제대로 된 학습이 어려우진다.

<p align = "center">
<img width = "800" alt = "1" src = "https://user-images.githubusercontent.com/111734605/229435421-b2389794-6f84-416c-aea2-eb109285f0fb.png">
</p>

따라서 논문에서는 **Deep Bidirectional Representation을 학습**하기 위해, 일정 비율의 단어를 Masking하는  <span style = "color:gold">**Masked Langauge Modeling(MLM)**</span>을 사용한다. MLM은 <span style = "color:aqua">**다음 단어가 무엇이 오는지 예측하는 학습이 아닌 문장 내에서 무작위로 입력 토큰의 일정 비율을 마스킹하고, 그 다음 마스킹된 토큰들을 예측**</span>한다. 즉, 문장 전체를 reconstruction하지 않고, 일정 비율로 Masking된 임의의 토큰들만을 예측하며 학습한다. 마스크 토큰에 해당하는 마지막 hidden vector는 표준 Language model에서와 같이 **어휘를 통해 출력 소프트맥스로 주어지고 단어를 예측**하게 된다.

이 방법은 방향을 갖는 조건부확률로서 언어모델을 구성하지 않는다. 따라서 보다 원할한 **bidirectional pre-training**을 가능하게 해준다. 논문에서는 **WordPiece 토큰의 15%를 무작위로 각 시퀀스에서 마스킹**한다.(시퀀스는 앞서 말했듯 하나 혹은 두 문장의 토큰을 패킹한 전체를 말한다.)

그러나 MLM은 한 가지 단점이 있다. 양방향으로의 학습이 가능해지지만, **fine-tuning중에 \[MASK\] 토큰이 나타나지 않기 때문에(빈칸 단어 예측은 그냥 빈칸 단어로 주어지기 때문)**
pre-training과 fine-tuning사이의 불일치(mismatch)를 만들어낸다. 따라서 논문에서는 Masking과정에서 Generalization을 위한 트릭을 사용한다. Masking하도록 정해진 토큰에 대하여 <span style = "color:aqua">**확률적인 variation**</span>을 주는 것이다. 


<br/>
<br/>

# Experiment & Result


# Reference
[논문리뷰 BERT]("https://supkoon.tistory.com/24")  
[논문리뷰 BERT]("https://velog.io/@xuio/NLP-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-BERT-Pre-training-of-Deep-Bidirectional-Transformers-forLanguage-Understanding-%EC%83%81%ED%8E%B8")  

